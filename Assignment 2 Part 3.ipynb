{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Andrew Whirisky - 17200679 and Neil Jones - 17202155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neil/.pyenv/versions/3.6.0/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution2D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4)                 516       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 34,692\n",
      "Trainable params: 34,692\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 700000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neil/.pyenv/versions/3.6.0/lib/python3.6/site-packages/rl/memory.py:29: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    141/700000: episode: 1, duration: 1.912s, episode steps: 141, steps per second: 74, episode reward: -466.933, mean reward: -3.312 [-100.000, 3.484], mean action: 1.709 [0.000, 3.000], mean observation: 0.117 [-1.496, 2.291], loss: 1.214043, mean_absolute_error: 1.790543, mean_q: -0.881507\n",
      "    246/700000: episode: 2, duration: 0.628s, episode steps: 105, steps per second: 167, episode reward: -149.989, mean reward: -1.428 [-100.000, 2.863], mean action: 1.562 [0.000, 3.000], mean observation: 0.229 [-0.547, 1.083], loss: 23.277130, mean_absolute_error: 3.757147, mean_q: -2.688236\n",
      "    409/700000: episode: 3, duration: 0.988s, episode steps: 163, steps per second: 165, episode reward: -82.222, mean reward: -0.504 [-100.000, 12.710], mean action: 1.957 [0.000, 3.000], mean observation: 0.020 [-0.807, 1.000], loss: 25.722189, mean_absolute_error: 4.110884, mean_q: -2.902601\n",
      "    779/700000: episode: 4, duration: 1.970s, episode steps: 370, steps per second: 188, episode reward: -536.878, mean reward: -1.451 [-100.000, 5.307], mean action: 1.757 [0.000, 3.000], mean observation: 0.133 [-5.264, 2.809], loss: 15.899183, mean_absolute_error: 4.348563, mean_q: -3.399010\n",
      "    986/700000: episode: 5, duration: 1.100s, episode steps: 207, steps per second: 188, episode reward: -198.673, mean reward: -0.960 [-100.000, 22.392], mean action: 1.812 [0.000, 3.000], mean observation: 0.098 [-4.826, 1.071], loss: 10.416095, mean_absolute_error: 5.859677, mean_q: -4.797402\n",
      "   1940/700000: episode: 6, duration: 5.842s, episode steps: 954, steps per second: 163, episode reward: 85.006, mean reward: 0.089 [-23.753, 100.000], mean action: 1.249 [0.000, 3.000], mean observation: 0.160 [-0.714, 1.000], loss: 6.978538, mean_absolute_error: 9.429192, mean_q: -7.672587\n",
      "   2894/700000: episode: 7, duration: 6.716s, episode steps: 954, steps per second: 142, episode reward: 68.067, mean reward: 0.071 [-19.911, 100.000], mean action: 1.688 [0.000, 3.000], mean observation: 0.175 [-0.326, 1.000], loss: 6.926798, mean_absolute_error: 12.554310, mean_q: -8.579312\n",
      "   3894/700000: episode: 8, duration: 6.410s, episode steps: 1000, steps per second: 156, episode reward: -114.846, mean reward: -0.115 [-3.876, 4.902], mean action: 1.834 [0.000, 3.000], mean observation: 0.107 [-0.289, 1.006], loss: 7.164652, mean_absolute_error: 11.597529, mean_q: -5.082838\n",
      "   4731/700000: episode: 9, duration: 4.791s, episode steps: 837, steps per second: 175, episode reward: 103.521, mean reward: 0.124 [-14.997, 100.000], mean action: 1.405 [0.000, 3.000], mean observation: 0.129 [-0.872, 1.000], loss: 5.920860, mean_absolute_error: 9.369236, mean_q: -0.131010\n",
      "   4932/700000: episode: 10, duration: 1.026s, episode steps: 201, steps per second: 196, episode reward: -18.668, mean reward: -0.093 [-100.000, 13.462], mean action: 1.701 [0.000, 3.000], mean observation: 0.012 [-0.928, 1.000], loss: 4.947566, mean_absolute_error: 9.477855, mean_q: 1.745625\n",
      "   5932/700000: episode: 11, duration: 6.083s, episode steps: 1000, steps per second: 164, episode reward: -203.268, mean reward: -0.203 [-4.565, 3.720], mean action: 1.733 [0.000, 3.000], mean observation: 0.061 [-0.803, 0.972], loss: 7.619989, mean_absolute_error: 8.802676, mean_q: 4.115848\n",
      "   6932/700000: episode: 12, duration: 6.334s, episode steps: 1000, steps per second: 158, episode reward: -139.879, mean reward: -0.140 [-4.657, 3.976], mean action: 1.709 [0.000, 3.000], mean observation: 0.066 [-0.449, 0.993], loss: 4.347079, mean_absolute_error: 8.695046, mean_q: 6.506717\n",
      "   7051/700000: episode: 13, duration: 0.600s, episode steps: 119, steps per second: 198, episode reward: 3.003, mean reward: 0.025 [-100.000, 16.740], mean action: 1.866 [0.000, 3.000], mean observation: 0.008 [-0.758, 1.391], loss: 5.522458, mean_absolute_error: 9.079884, mean_q: 8.803976\n",
      "   8051/700000: episode: 14, duration: 6.748s, episode steps: 1000, steps per second: 148, episode reward: -67.386, mean reward: -0.067 [-5.161, 4.425], mean action: 1.739 [0.000, 3.000], mean observation: 0.022 [-0.547, 0.925], loss: 5.308932, mean_absolute_error: 9.939664, mean_q: 10.078012\n",
      "   8233/700000: episode: 15, duration: 0.913s, episode steps: 182, steps per second: 199, episode reward: -20.326, mean reward: -0.112 [-100.000, 12.272], mean action: 1.516 [0.000, 3.000], mean observation: 0.036 [-0.994, 1.600], loss: 2.917181, mean_absolute_error: 10.746006, mean_q: 11.837657\n",
      "   8392/700000: episode: 16, duration: 0.790s, episode steps: 159, steps per second: 201, episode reward: -75.287, mean reward: -0.474 [-100.000, 11.366], mean action: 1.686 [0.000, 3.000], mean observation: 0.036 [-3.351, 1.039], loss: 2.587680, mean_absolute_error: 11.197743, mean_q: 11.410718\n",
      "   8682/700000: episode: 17, duration: 1.465s, episode steps: 290, steps per second: 198, episode reward: -113.942, mean reward: -0.393 [-100.000, 6.672], mean action: 1.531 [0.000, 3.000], mean observation: -0.014 [-0.972, 1.000], loss: 4.238706, mean_absolute_error: 11.732778, mean_q: 13.051674\n",
      "   9682/700000: episode: 18, duration: 6.112s, episode steps: 1000, steps per second: 164, episode reward: -61.099, mean reward: -0.061 [-5.236, 4.589], mean action: 1.854 [0.000, 3.000], mean observation: 0.050 [-0.771, 0.935], loss: 3.803405, mean_absolute_error: 12.454750, mean_q: 13.748010\n",
      "  10682/700000: episode: 19, duration: 5.993s, episode steps: 1000, steps per second: 167, episode reward: -73.715, mean reward: -0.074 [-3.989, 5.629], mean action: 1.769 [0.000, 3.000], mean observation: 0.069 [-0.653, 0.933], loss: 3.006693, mean_absolute_error: 12.923183, mean_q: 14.654772\n",
      "  11682/700000: episode: 20, duration: 6.615s, episode steps: 1000, steps per second: 151, episode reward: -107.717, mean reward: -0.108 [-4.453, 4.601], mean action: 1.699 [0.000, 3.000], mean observation: 0.094 [-0.242, 0.935], loss: 2.737437, mean_absolute_error: 13.432313, mean_q: 15.272865\n",
      "  12682/700000: episode: 21, duration: 6.872s, episode steps: 1000, steps per second: 146, episode reward: -120.429, mean reward: -0.120 [-4.879, 4.481], mean action: 1.787 [0.000, 3.000], mean observation: 0.059 [-0.473, 0.932], loss: 2.333558, mean_absolute_error: 13.437446, mean_q: 15.488829\n",
      "  12750/700000: episode: 22, duration: 0.357s, episode steps: 68, steps per second: 191, episode reward: -108.741, mean reward: -1.599 [-100.000, 10.095], mean action: 0.897 [0.000, 3.000], mean observation: 0.005 [-1.325, 3.579], loss: 7.595121, mean_absolute_error: 13.635893, mean_q: 16.377234\n",
      "  13750/700000: episode: 23, duration: 6.718s, episode steps: 1000, steps per second: 149, episode reward: -111.074, mean reward: -0.111 [-4.131, 5.045], mean action: 1.639 [0.000, 3.000], mean observation: 0.065 [-0.390, 0.987], loss: 2.892431, mean_absolute_error: 13.420481, mean_q: 16.077478\n",
      "  14750/700000: episode: 24, duration: 6.643s, episode steps: 1000, steps per second: 151, episode reward: -84.546, mean reward: -0.085 [-5.994, 5.747], mean action: 1.686 [0.000, 3.000], mean observation: 0.016 [-0.527, 0.925], loss: 2.714061, mean_absolute_error: 13.593492, mean_q: 16.012638\n",
      "  14969/700000: episode: 25, duration: 1.102s, episode steps: 219, steps per second: 199, episode reward: 1.702, mean reward: 0.008 [-100.000, 43.598], mean action: 1.703 [0.000, 3.000], mean observation: 0.066 [-1.171, 1.014], loss: 1.615853, mean_absolute_error: 13.468081, mean_q: 16.193686\n",
      "  15268/700000: episode: 26, duration: 1.545s, episode steps: 299, steps per second: 194, episode reward: -33.413, mean reward: -0.112 [-100.000, 17.368], mean action: 1.799 [0.000, 3.000], mean observation: 0.043 [-1.169, 1.000], loss: 2.172657, mean_absolute_error: 14.431082, mean_q: 17.032595\n",
      "  16264/700000: episode: 27, duration: 5.789s, episode steps: 996, steps per second: 172, episode reward: -296.063, mean reward: -0.297 [-100.000, 17.244], mean action: 1.400 [0.000, 3.000], mean observation: 0.116 [-0.826, 1.000], loss: 3.904883, mean_absolute_error: 14.498517, mean_q: 17.239183\n",
      "  17264/700000: episode: 28, duration: 6.350s, episode steps: 1000, steps per second: 157, episode reward: -59.603, mean reward: -0.060 [-3.978, 4.442], mean action: 1.804 [0.000, 3.000], mean observation: 0.037 [-0.513, 0.925], loss: 3.027962, mean_absolute_error: 14.908887, mean_q: 18.146242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  18264/700000: episode: 29, duration: 6.525s, episode steps: 1000, steps per second: 153, episode reward: -33.342, mean reward: -0.033 [-11.549, 13.776], mean action: 1.867 [0.000, 3.000], mean observation: 0.019 [-0.719, 1.000], loss: 4.353009, mean_absolute_error: 14.994679, mean_q: 18.186375\n",
      "  19264/700000: episode: 30, duration: 6.135s, episode steps: 1000, steps per second: 163, episode reward: -43.251, mean reward: -0.043 [-19.353, 16.785], mean action: 1.804 [0.000, 3.000], mean observation: 0.044 [-0.740, 1.000], loss: 2.792253, mean_absolute_error: 15.184410, mean_q: 18.416075\n",
      "  19493/700000: episode: 31, duration: 1.138s, episode steps: 229, steps per second: 201, episode reward: -60.403, mean reward: -0.264 [-100.000, 53.166], mean action: 1.345 [0.000, 3.000], mean observation: 0.138 [-0.970, 1.000], loss: 2.191468, mean_absolute_error: 15.405270, mean_q: 18.473156\n",
      "  20493/700000: episode: 32, duration: 6.727s, episode steps: 1000, steps per second: 149, episode reward: -56.325, mean reward: -0.056 [-4.598, 5.590], mean action: 1.787 [0.000, 3.000], mean observation: 0.036 [-0.413, 0.946], loss: 3.481512, mean_absolute_error: 15.776557, mean_q: 19.275801\n",
      "  21493/700000: episode: 33, duration: 6.735s, episode steps: 1000, steps per second: 148, episode reward: -46.319, mean reward: -0.046 [-11.413, 11.117], mean action: 1.714 [0.000, 3.000], mean observation: 0.030 [-0.546, 1.000], loss: 4.661376, mean_absolute_error: 15.317333, mean_q: 18.803431\n",
      "  22345/700000: episode: 34, duration: 5.044s, episode steps: 852, steps per second: 169, episode reward: 157.220, mean reward: 0.185 [-24.322, 100.000], mean action: 1.683 [0.000, 3.000], mean observation: 0.078 [-0.590, 1.000], loss: 4.181880, mean_absolute_error: 15.557754, mean_q: 18.821354\n",
      "  22533/700000: episode: 35, duration: 0.954s, episode steps: 188, steps per second: 197, episode reward: -122.818, mean reward: -0.653 [-100.000, 16.024], mean action: 1.787 [0.000, 3.000], mean observation: 0.060 [-3.145, 1.000], loss: 2.857293, mean_absolute_error: 15.372873, mean_q: 18.783201\n",
      "  23533/700000: episode: 36, duration: 5.692s, episode steps: 1000, steps per second: 176, episode reward: 52.834, mean reward: 0.053 [-23.875, 24.645], mean action: 1.033 [0.000, 3.000], mean observation: 0.108 [-1.362, 1.000], loss: 4.375026, mean_absolute_error: 16.207972, mean_q: 19.691269\n",
      "  23706/700000: episode: 37, duration: 0.869s, episode steps: 173, steps per second: 199, episode reward: -69.647, mean reward: -0.403 [-100.000, 15.330], mean action: 1.723 [0.000, 3.000], mean observation: 0.067 [-0.912, 1.094], loss: 7.554224, mean_absolute_error: 16.307018, mean_q: 19.356098\n",
      "  23958/700000: episode: 38, duration: 1.296s, episode steps: 252, steps per second: 194, episode reward: -80.699, mean reward: -0.320 [-100.000, 20.912], mean action: 1.821 [0.000, 3.000], mean observation: 0.065 [-1.924, 1.000], loss: 3.536025, mean_absolute_error: 16.166758, mean_q: 19.874531\n",
      "  24919/700000: episode: 39, duration: 5.679s, episode steps: 961, steps per second: 169, episode reward: 160.576, mean reward: 0.167 [-20.284, 100.000], mean action: 1.202 [0.000, 3.000], mean observation: 0.107 [-0.786, 1.000], loss: 5.393499, mean_absolute_error: 17.043505, mean_q: 20.788490\n",
      "  25537/700000: episode: 40, duration: 3.428s, episode steps: 618, steps per second: 180, episode reward: 159.124, mean reward: 0.257 [-20.944, 100.000], mean action: 1.400 [0.000, 3.000], mean observation: 0.141 [-1.242, 1.000], loss: 4.257760, mean_absolute_error: 17.719433, mean_q: 21.866703\n",
      "  25998/700000: episode: 41, duration: 2.431s, episode steps: 461, steps per second: 190, episode reward: -66.244, mean reward: -0.144 [-100.000, 14.631], mean action: 1.716 [0.000, 3.000], mean observation: 0.001 [-1.265, 1.007], loss: 5.145409, mean_absolute_error: 17.950041, mean_q: 22.356871\n",
      "  26216/700000: episode: 42, duration: 1.104s, episode steps: 218, steps per second: 197, episode reward: -133.415, mean reward: -0.612 [-100.000, 10.045], mean action: 1.697 [0.000, 3.000], mean observation: 0.126 [-0.739, 1.000], loss: 2.587172, mean_absolute_error: 17.854174, mean_q: 22.599503\n",
      "  26389/700000: episode: 43, duration: 0.865s, episode steps: 173, steps per second: 200, episode reward: -23.842, mean reward: -0.138 [-100.000, 22.948], mean action: 1.769 [0.000, 3.000], mean observation: -0.008 [-2.146, 1.000], loss: 3.870452, mean_absolute_error: 18.753162, mean_q: 23.570072\n",
      "  26526/700000: episode: 44, duration: 0.684s, episode steps: 137, steps per second: 200, episode reward: -48.288, mean reward: -0.352 [-100.000, 13.775], mean action: 1.657 [0.000, 3.000], mean observation: 0.118 [-0.679, 1.093], loss: 5.121801, mean_absolute_error: 19.173342, mean_q: 24.514381\n",
      "  26760/700000: episode: 45, duration: 1.200s, episode steps: 234, steps per second: 195, episode reward: -70.811, mean reward: -0.303 [-100.000, 9.727], mean action: 1.803 [0.000, 3.000], mean observation: 0.075 [-1.809, 1.000], loss: 7.142863, mean_absolute_error: 19.185369, mean_q: 24.109024\n",
      "  26949/700000: episode: 46, duration: 0.948s, episode steps: 189, steps per second: 199, episode reward: -71.725, mean reward: -0.379 [-100.000, 18.067], mean action: 1.810 [0.000, 3.000], mean observation: 0.127 [-1.377, 1.018], loss: 6.675792, mean_absolute_error: 19.113165, mean_q: 24.459480\n",
      "  27143/700000: episode: 47, duration: 1.018s, episode steps: 194, steps per second: 191, episode reward: -84.137, mean reward: -0.434 [-100.000, 11.710], mean action: 1.716 [0.000, 3.000], mean observation: 0.070 [-1.574, 1.000], loss: 7.274672, mean_absolute_error: 19.173916, mean_q: 24.710220\n",
      "  27316/700000: episode: 48, duration: 0.865s, episode steps: 173, steps per second: 200, episode reward: -57.390, mean reward: -0.332 [-100.000, 17.863], mean action: 1.827 [0.000, 3.000], mean observation: 0.041 [-1.451, 1.000], loss: 6.038655, mean_absolute_error: 19.038963, mean_q: 24.451685\n",
      "  27450/700000: episode: 49, duration: 0.673s, episode steps: 134, steps per second: 199, episode reward: -215.544, mean reward: -1.609 [-100.000, 39.798], mean action: 1.985 [0.000, 3.000], mean observation: -0.004 [-0.959, 2.711], loss: 7.377641, mean_absolute_error: 19.509624, mean_q: 25.187180\n",
      "  27691/700000: episode: 50, duration: 1.218s, episode steps: 241, steps per second: 198, episode reward: -78.991, mean reward: -0.328 [-100.000, 9.729], mean action: 1.714 [0.000, 3.000], mean observation: 0.107 [-1.755, 1.015], loss: 5.987119, mean_absolute_error: 20.791687, mean_q: 26.979551\n",
      "  27885/700000: episode: 51, duration: 0.969s, episode steps: 194, steps per second: 200, episode reward: -24.125, mean reward: -0.124 [-100.000, 13.145], mean action: 1.701 [0.000, 3.000], mean observation: 0.080 [-0.834, 1.000], loss: 11.376771, mean_absolute_error: 20.566429, mean_q: 26.534447\n",
      "  28099/700000: episode: 52, duration: 1.062s, episode steps: 214, steps per second: 201, episode reward: -73.855, mean reward: -0.345 [-100.000, 9.759], mean action: 1.537 [0.000, 3.000], mean observation: 0.082 [-1.842, 1.000], loss: 4.007520, mean_absolute_error: 21.065809, mean_q: 27.266674\n",
      "  28708/700000: episode: 53, duration: 3.522s, episode steps: 609, steps per second: 173, episode reward: -359.863, mean reward: -0.591 [-100.000, 19.209], mean action: 1.831 [0.000, 3.000], mean observation: -0.031 [-2.094, 1.000], loss: 6.764644, mean_absolute_error: 20.730068, mean_q: 26.810188\n",
      "  28921/700000: episode: 54, duration: 1.075s, episode steps: 213, steps per second: 198, episode reward: -70.231, mean reward: -0.330 [-100.000, 11.070], mean action: 1.634 [0.000, 3.000], mean observation: 0.137 [-0.636, 1.029], loss: 8.523708, mean_absolute_error: 20.907860, mean_q: 27.124060\n",
      "  29491/700000: episode: 55, duration: 3.172s, episode steps: 570, steps per second: 180, episode reward: 155.778, mean reward: 0.273 [-22.572, 100.000], mean action: 1.398 [0.000, 3.000], mean observation: 0.157 [-0.922, 1.000], loss: 8.078101, mean_absolute_error: 21.362053, mean_q: 27.459751\n",
      "  29795/700000: episode: 56, duration: 1.571s, episode steps: 304, steps per second: 193, episode reward: -39.133, mean reward: -0.129 [-100.000, 13.899], mean action: 1.740 [0.000, 3.000], mean observation: 0.036 [-1.179, 1.000], loss: 5.641982, mean_absolute_error: 21.770546, mean_q: 28.126635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  29930/700000: episode: 57, duration: 0.672s, episode steps: 135, steps per second: 201, episode reward: -82.428, mean reward: -0.611 [-100.000, 61.747], mean action: 1.696 [0.000, 3.000], mean observation: 0.100 [-0.838, 2.220], loss: 28.370371, mean_absolute_error: 22.039919, mean_q: 28.479109\n",
      "  30930/700000: episode: 58, duration: 6.254s, episode steps: 1000, steps per second: 160, episode reward: -44.875, mean reward: -0.045 [-21.686, 21.989], mean action: 1.582 [0.000, 3.000], mean observation: 0.117 [-0.380, 1.000], loss: 7.174170, mean_absolute_error: 21.986164, mean_q: 28.677841\n",
      "  31930/700000: episode: 59, duration: 6.497s, episode steps: 1000, steps per second: 154, episode reward: -32.256, mean reward: -0.032 [-21.495, 15.899], mean action: 1.710 [0.000, 3.000], mean observation: 0.031 [-0.744, 1.000], loss: 7.869471, mean_absolute_error: 22.421822, mean_q: 29.293188\n",
      "  32732/700000: episode: 60, duration: 4.361s, episode steps: 802, steps per second: 184, episode reward: 146.254, mean reward: 0.182 [-22.462, 100.000], mean action: 1.219 [0.000, 3.000], mean observation: 0.143 [-0.736, 1.248], loss: 7.298267, mean_absolute_error: 22.519787, mean_q: 29.750366\n",
      "  33732/700000: episode: 61, duration: 6.817s, episode steps: 1000, steps per second: 147, episode reward: -5.187, mean reward: -0.005 [-21.245, 18.327], mean action: 1.674 [0.000, 3.000], mean observation: 0.081 [-0.626, 1.000], loss: 7.587918, mean_absolute_error: 22.856714, mean_q: 30.199137\n",
      "  33917/700000: episode: 62, duration: 0.932s, episode steps: 185, steps per second: 198, episode reward: -168.426, mean reward: -0.910 [-100.000, 9.844], mean action: 1.503 [0.000, 3.000], mean observation: 0.171 [-0.598, 1.193], loss: 9.984460, mean_absolute_error: 23.129768, mean_q: 30.285099\n",
      "  34917/700000: episode: 63, duration: 5.945s, episode steps: 1000, steps per second: 168, episode reward: -14.268, mean reward: -0.014 [-24.209, 21.963], mean action: 1.631 [0.000, 3.000], mean observation: 0.115 [-0.620, 1.000], loss: 7.097182, mean_absolute_error: 23.342640, mean_q: 30.884027\n",
      "  35917/700000: episode: 64, duration: 6.591s, episode steps: 1000, steps per second: 152, episode reward: -0.501, mean reward: -0.001 [-23.080, 22.929], mean action: 1.622 [0.000, 3.000], mean observation: 0.111 [-0.506, 1.000], loss: 7.093402, mean_absolute_error: 23.793484, mean_q: 31.419294\n",
      "  36917/700000: episode: 65, duration: 6.267s, episode steps: 1000, steps per second: 160, episode reward: 54.905, mean reward: 0.055 [-20.273, 15.085], mean action: 1.762 [0.000, 3.000], mean observation: 0.107 [-0.644, 1.000], loss: 7.643657, mean_absolute_error: 24.468000, mean_q: 32.234173\n",
      "  37917/700000: episode: 66, duration: 6.204s, episode steps: 1000, steps per second: 161, episode reward: 43.247, mean reward: 0.043 [-21.197, 24.070], mean action: 1.908 [0.000, 3.000], mean observation: 0.175 [-0.648, 1.000], loss: 5.885668, mean_absolute_error: 24.889605, mean_q: 32.923248\n",
      "  38917/700000: episode: 67, duration: 5.635s, episode steps: 1000, steps per second: 177, episode reward: 55.993, mean reward: 0.056 [-21.988, 21.925], mean action: 1.635 [0.000, 3.000], mean observation: 0.158 [-0.847, 1.120], loss: 7.058157, mean_absolute_error: 25.160618, mean_q: 33.449841\n",
      "  39917/700000: episode: 68, duration: 5.841s, episode steps: 1000, steps per second: 171, episode reward: -135.573, mean reward: -0.136 [-5.265, 4.168], mean action: 1.772 [0.000, 3.000], mean observation: 0.069 [-0.471, 0.948], loss: 6.200335, mean_absolute_error: 25.692875, mean_q: 34.201195\n",
      "  40897/700000: episode: 69, duration: 6.262s, episode steps: 980, steps per second: 156, episode reward: 135.132, mean reward: 0.138 [-17.431, 100.000], mean action: 1.529 [0.000, 3.000], mean observation: 0.081 [-0.625, 1.000], loss: 7.087414, mean_absolute_error: 26.559877, mean_q: 35.214321\n",
      "  41594/700000: episode: 70, duration: 4.452s, episode steps: 697, steps per second: 157, episode reward: 147.948, mean reward: 0.212 [-23.225, 100.000], mean action: 1.845 [0.000, 3.000], mean observation: 0.104 [-0.610, 1.000], loss: 6.262092, mean_absolute_error: 27.384138, mean_q: 36.451035\n",
      "  42594/700000: episode: 71, duration: 5.719s, episode steps: 1000, steps per second: 175, episode reward: 25.466, mean reward: 0.025 [-19.410, 22.995], mean action: 1.364 [0.000, 3.000], mean observation: 0.119 [-0.689, 1.000], loss: 7.584976, mean_absolute_error: 27.868574, mean_q: 36.977345\n",
      "  43127/700000: episode: 72, duration: 2.892s, episode steps: 533, steps per second: 184, episode reward: -206.505, mean reward: -0.387 [-100.000, 5.031], mean action: 1.597 [0.000, 3.000], mean observation: -0.015 [-1.002, 1.046], loss: 4.621442, mean_absolute_error: 28.508585, mean_q: 38.018921\n",
      "  43606/700000: episode: 73, duration: 2.565s, episode steps: 479, steps per second: 187, episode reward: 127.657, mean reward: 0.267 [-18.805, 100.000], mean action: 1.380 [0.000, 3.000], mean observation: 0.049 [-1.209, 1.000], loss: 5.714087, mean_absolute_error: 28.832792, mean_q: 38.455441\n",
      "  43957/700000: episode: 74, duration: 1.811s, episode steps: 351, steps per second: 194, episode reward: -317.244, mean reward: -0.904 [-100.000, 21.754], mean action: 2.043 [0.000, 3.000], mean observation: 0.011 [-1.303, 1.000], loss: 8.973917, mean_absolute_error: 29.274450, mean_q: 39.328594\n",
      "  44215/700000: episode: 75, duration: 1.300s, episode steps: 258, steps per second: 198, episode reward: -336.593, mean reward: -1.305 [-100.000, 4.708], mean action: 1.562 [0.000, 3.000], mean observation: -0.011 [-1.006, 1.425], loss: 5.992324, mean_absolute_error: 29.227081, mean_q: 39.027298\n",
      "  44354/700000: episode: 76, duration: 0.720s, episode steps: 139, steps per second: 193, episode reward: -28.215, mean reward: -0.203 [-100.000, 10.458], mean action: 1.820 [0.000, 3.000], mean observation: -0.046 [-0.831, 2.569], loss: 8.697634, mean_absolute_error: 29.543390, mean_q: 39.464596\n",
      "  45354/700000: episode: 77, duration: 6.141s, episode steps: 1000, steps per second: 163, episode reward: 55.868, mean reward: 0.056 [-23.452, 22.656], mean action: 1.348 [0.000, 3.000], mean observation: 0.107 [-0.991, 1.000], loss: 7.726683, mean_absolute_error: 29.785105, mean_q: 39.782757\n",
      "  45516/700000: episode: 78, duration: 0.809s, episode steps: 162, steps per second: 200, episode reward: -37.187, mean reward: -0.230 [-100.000, 77.690], mean action: 1.815 [0.000, 3.000], mean observation: -0.050 [-1.498, 1.081], loss: 7.502961, mean_absolute_error: 29.662264, mean_q: 39.468616\n",
      "  45722/700000: episode: 79, duration: 1.041s, episode steps: 206, steps per second: 198, episode reward: -113.609, mean reward: -0.551 [-100.000, 13.110], mean action: 1.835 [0.000, 3.000], mean observation: 0.018 [-0.655, 1.000], loss: 14.253892, mean_absolute_error: 30.107492, mean_q: 40.064240\n",
      "  46722/700000: episode: 80, duration: 5.584s, episode steps: 1000, steps per second: 179, episode reward: -47.457, mean reward: -0.047 [-21.031, 23.170], mean action: 2.379 [0.000, 3.000], mean observation: 0.061 [-0.871, 1.000], loss: 8.512039, mean_absolute_error: 30.078793, mean_q: 40.020702\n",
      "  46852/700000: episode: 81, duration: 0.651s, episode steps: 130, steps per second: 200, episode reward: -111.788, mean reward: -0.860 [-100.000, 25.532], mean action: 1.523 [0.000, 3.000], mean observation: 0.155 [-3.652, 1.023], loss: 9.407411, mean_absolute_error: 29.782625, mean_q: 39.706333\n",
      "  47266/700000: episode: 82, duration: 2.154s, episode steps: 414, steps per second: 192, episode reward: 141.064, mean reward: 0.341 [-9.113, 100.000], mean action: 1.159 [0.000, 3.000], mean observation: -0.049 [-0.995, 1.000], loss: 6.916877, mean_absolute_error: 30.140842, mean_q: 40.120396\n",
      "  47350/700000: episode: 83, duration: 0.416s, episode steps: 84, steps per second: 202, episode reward: -173.902, mean reward: -2.070 [-100.000, 10.952], mean action: 1.048 [0.000, 3.000], mean observation: 0.067 [-5.301, 1.000], loss: 17.992798, mean_absolute_error: 30.168602, mean_q: 40.153618\n",
      "  47449/700000: episode: 84, duration: 0.534s, episode steps: 99, steps per second: 185, episode reward: -108.726, mean reward: -1.098 [-100.000, 22.340], mean action: 1.758 [0.000, 3.000], mean observation: -0.053 [-1.244, 1.000], loss: 13.041425, mean_absolute_error: 30.188364, mean_q: 40.407661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  47879/700000: episode: 85, duration: 2.356s, episode steps: 430, steps per second: 183, episode reward: 103.491, mean reward: 0.241 [-13.137, 100.000], mean action: 1.551 [0.000, 3.000], mean observation: 0.008 [-0.764, 1.000], loss: 5.830829, mean_absolute_error: 30.400738, mean_q: 40.380692\n",
      "  48063/700000: episode: 86, duration: 0.910s, episode steps: 184, steps per second: 202, episode reward: -61.753, mean reward: -0.336 [-100.000, 12.998], mean action: 1.467 [0.000, 3.000], mean observation: 0.028 [-0.891, 1.000], loss: 7.501368, mean_absolute_error: 30.173916, mean_q: 40.005604\n",
      "  48436/700000: episode: 87, duration: 1.954s, episode steps: 373, steps per second: 191, episode reward: 237.069, mean reward: 0.636 [-19.548, 100.000], mean action: 1.413 [0.000, 3.000], mean observation: 0.103 [-0.710, 1.000], loss: 8.899549, mean_absolute_error: 30.239313, mean_q: 40.085289\n",
      "  48821/700000: episode: 88, duration: 2.096s, episode steps: 385, steps per second: 184, episode reward: 125.326, mean reward: 0.326 [-11.857, 100.000], mean action: 1.384 [0.000, 3.000], mean observation: -0.033 [-1.095, 1.000], loss: 7.711617, mean_absolute_error: 30.798067, mean_q: 41.004395\n",
      "  48903/700000: episode: 89, duration: 0.401s, episode steps: 82, steps per second: 205, episode reward: -457.481, mean reward: -5.579 [-100.000, 2.610], mean action: 0.890 [0.000, 3.000], mean observation: -0.270 [-6.788, 0.961], loss: 4.211959, mean_absolute_error: 30.938036, mean_q: 41.041897\n",
      "  49289/700000: episode: 90, duration: 2.013s, episode steps: 386, steps per second: 192, episode reward: 250.863, mean reward: 0.650 [-16.951, 100.000], mean action: 1.593 [0.000, 3.000], mean observation: 0.121 [-0.793, 1.662], loss: 8.550453, mean_absolute_error: 30.681074, mean_q: 40.682224\n",
      "  49383/700000: episode: 91, duration: 0.464s, episode steps: 94, steps per second: 203, episode reward: -469.865, mean reward: -4.999 [-100.000, 135.712], mean action: 1.564 [0.000, 3.000], mean observation: -0.293 [-4.958, 4.159], loss: 12.250390, mean_absolute_error: 30.377871, mean_q: 40.377007\n",
      "  49461/700000: episode: 92, duration: 0.388s, episode steps: 78, steps per second: 201, episode reward: -671.186, mean reward: -8.605 [-100.000, 1.873], mean action: 2.449 [0.000, 3.000], mean observation: -0.316 [-4.842, 2.005], loss: 14.097275, mean_absolute_error: 30.905664, mean_q: 40.841236\n",
      "  49604/700000: episode: 93, duration: 0.711s, episode steps: 143, steps per second: 201, episode reward: -527.286, mean reward: -3.687 [-100.000, 59.658], mean action: 1.378 [0.000, 3.000], mean observation: -0.224 [-4.520, 4.827], loss: 6.980827, mean_absolute_error: 30.944090, mean_q: 41.105007\n",
      "  49904/700000: episode: 94, duration: 1.585s, episode steps: 300, steps per second: 189, episode reward: 165.373, mean reward: 0.551 [-11.596, 100.000], mean action: 2.177 [0.000, 3.000], mean observation: 0.106 [-0.579, 1.000], loss: 8.936082, mean_absolute_error: 31.082676, mean_q: 41.488342\n",
      "  50088/700000: episode: 95, duration: 0.922s, episode steps: 184, steps per second: 200, episode reward: -82.575, mean reward: -0.449 [-100.000, 14.197], mean action: 1.402 [0.000, 3.000], mean observation: 0.054 [-0.846, 1.000], loss: 9.209554, mean_absolute_error: 31.044895, mean_q: 41.281536\n",
      "  50186/700000: episode: 96, duration: 0.483s, episode steps: 98, steps per second: 203, episode reward: -483.889, mean reward: -4.938 [-100.000, 3.744], mean action: 1.306 [0.000, 3.000], mean observation: -0.230 [-3.292, 0.943], loss: 7.890750, mean_absolute_error: 30.868895, mean_q: 40.907196\n",
      "  50430/700000: episode: 97, duration: 1.233s, episode steps: 244, steps per second: 198, episode reward: 208.916, mean reward: 0.856 [-9.018, 100.000], mean action: 1.430 [0.000, 3.000], mean observation: 0.035 [-0.546, 1.000], loss: 35.555206, mean_absolute_error: 31.426979, mean_q: 41.639835\n",
      "  50728/700000: episode: 98, duration: 1.533s, episode steps: 298, steps per second: 194, episode reward: 166.783, mean reward: 0.560 [-8.401, 100.000], mean action: 1.332 [0.000, 3.000], mean observation: 0.042 [-0.662, 1.000], loss: 31.438795, mean_absolute_error: 31.332539, mean_q: 41.783146\n",
      "  51154/700000: episode: 99, duration: 2.244s, episode steps: 426, steps per second: 190, episode reward: 172.972, mean reward: 0.406 [-18.414, 100.000], mean action: 1.629 [0.000, 3.000], mean observation: -0.001 [-1.199, 1.000], loss: 5.367303, mean_absolute_error: 31.430788, mean_q: 41.727753\n",
      "  51607/700000: episode: 100, duration: 2.475s, episode steps: 453, steps per second: 183, episode reward: -157.844, mean reward: -0.348 [-100.000, 22.824], mean action: 2.007 [0.000, 3.000], mean observation: -0.022 [-0.964, 1.000], loss: 16.718454, mean_absolute_error: 31.586864, mean_q: 42.045292\n",
      "  52266/700000: episode: 101, duration: 3.684s, episode steps: 659, steps per second: 179, episode reward: 183.005, mean reward: 0.278 [-18.718, 100.000], mean action: 1.484 [0.000, 3.000], mean observation: 0.121 [-0.589, 1.000], loss: 16.741476, mean_absolute_error: 31.600058, mean_q: 41.945030\n",
      "  52642/700000: episode: 102, duration: 1.966s, episode steps: 376, steps per second: 191, episode reward: 128.994, mean reward: 0.343 [-12.170, 100.000], mean action: 2.122 [0.000, 3.000], mean observation: -0.003 [-0.912, 1.000], loss: 7.145691, mean_absolute_error: 31.672379, mean_q: 42.067345\n",
      "  53326/700000: episode: 103, duration: 3.887s, episode steps: 684, steps per second: 176, episode reward: 186.708, mean reward: 0.273 [-12.296, 100.000], mean action: 1.294 [0.000, 3.000], mean observation: 0.125 [-0.876, 1.000], loss: 16.663116, mean_absolute_error: 31.864315, mean_q: 42.531830\n",
      "  53623/700000: episode: 104, duration: 1.533s, episode steps: 297, steps per second: 194, episode reward: 175.518, mean reward: 0.591 [-9.566, 100.000], mean action: 1.316 [0.000, 3.000], mean observation: 0.059 [-0.719, 1.000], loss: 7.217522, mean_absolute_error: 31.998178, mean_q: 42.664944\n",
      "  54157/700000: episode: 105, duration: 2.944s, episode steps: 534, steps per second: 181, episode reward: 171.225, mean reward: 0.321 [-19.381, 100.000], mean action: 1.665 [0.000, 3.000], mean observation: 0.081 [-0.697, 1.000], loss: 10.543837, mean_absolute_error: 32.108307, mean_q: 42.741089\n",
      "  54683/700000: episode: 106, duration: 2.907s, episode steps: 526, steps per second: 181, episode reward: 185.605, mean reward: 0.353 [-19.755, 100.000], mean action: 1.466 [0.000, 3.000], mean observation: 0.086 [-1.306, 1.000], loss: 12.811643, mean_absolute_error: 32.275517, mean_q: 43.171944\n",
      "  54850/700000: episode: 107, duration: 0.840s, episode steps: 167, steps per second: 199, episode reward: -225.723, mean reward: -1.352 [-100.000, 67.941], mean action: 1.665 [0.000, 3.000], mean observation: 0.007 [-2.040, 1.000], loss: 18.550014, mean_absolute_error: 32.676414, mean_q: 43.428280\n",
      "  54930/700000: episode: 108, duration: 0.402s, episode steps: 80, steps per second: 199, episode reward: 3.141, mean reward: 0.039 [-100.000, 18.274], mean action: 1.337 [0.000, 3.000], mean observation: -0.052 [-1.182, 1.176], loss: 20.615274, mean_absolute_error: 32.322948, mean_q: 42.864632\n",
      "  55043/700000: episode: 109, duration: 0.564s, episode steps: 113, steps per second: 200, episode reward: -117.899, mean reward: -1.043 [-100.000, 9.356], mean action: 1.389 [0.000, 3.000], mean observation: 0.030 [-0.994, 1.130], loss: 14.423031, mean_absolute_error: 32.366486, mean_q: 43.214783\n",
      "  55581/700000: episode: 110, duration: 3.060s, episode steps: 538, steps per second: 176, episode reward: 177.756, mean reward: 0.330 [-20.675, 100.000], mean action: 1.206 [0.000, 3.000], mean observation: 0.132 [-0.635, 1.000], loss: 8.597805, mean_absolute_error: 32.487484, mean_q: 43.572983\n",
      "  56581/700000: episode: 111, duration: 6.026s, episode steps: 1000, steps per second: 166, episode reward: 20.408, mean reward: 0.020 [-20.252, 21.938], mean action: 1.453 [0.000, 3.000], mean observation: 0.164 [-0.781, 1.000], loss: 9.715237, mean_absolute_error: 33.117645, mean_q: 44.425884\n",
      "  56920/700000: episode: 112, duration: 1.764s, episode steps: 339, steps per second: 192, episode reward: 180.961, mean reward: 0.534 [-19.334, 100.000], mean action: 1.463 [0.000, 3.000], mean observation: 0.099 [-0.612, 1.000], loss: 15.681260, mean_absolute_error: 33.881050, mean_q: 45.521721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  57028/700000: episode: 113, duration: 0.534s, episode steps: 108, steps per second: 202, episode reward: -12.682, mean reward: -0.117 [-100.000, 14.362], mean action: 1.519 [0.000, 3.000], mean observation: -0.078 [-1.099, 1.048], loss: 8.043242, mean_absolute_error: 34.251884, mean_q: 45.809505\n",
      "  57606/700000: episode: 114, duration: 3.076s, episode steps: 578, steps per second: 188, episode reward: 136.834, mean reward: 0.237 [-23.118, 100.000], mean action: 1.147 [0.000, 3.000], mean observation: 0.175 [-0.671, 1.000], loss: 18.500505, mean_absolute_error: 34.479733, mean_q: 46.412003\n",
      "  57737/700000: episode: 115, duration: 0.651s, episode steps: 131, steps per second: 201, episode reward: -47.566, mean reward: -0.363 [-100.000, 13.698], mean action: 1.679 [0.000, 3.000], mean observation: 0.071 [-1.049, 1.008], loss: 7.158634, mean_absolute_error: 34.140228, mean_q: 45.660431\n",
      "  57853/700000: episode: 116, duration: 0.566s, episode steps: 116, steps per second: 205, episode reward: -162.522, mean reward: -1.401 [-100.000, 6.293], mean action: 0.759 [0.000, 3.000], mean observation: 0.020 [-5.336, 1.000], loss: 31.482651, mean_absolute_error: 35.411976, mean_q: 47.647495\n",
      "  58342/700000: episode: 117, duration: 2.577s, episode steps: 489, steps per second: 190, episode reward: 214.787, mean reward: 0.439 [-18.435, 100.000], mean action: 0.924 [0.000, 3.000], mean observation: 0.152 [-1.005, 1.000], loss: 22.435070, mean_absolute_error: 35.021679, mean_q: 46.959507\n",
      "  58632/700000: episode: 118, duration: 1.502s, episode steps: 290, steps per second: 193, episode reward: -32.611, mean reward: -0.112 [-100.000, 13.765], mean action: 1.721 [0.000, 3.000], mean observation: 0.006 [-0.741, 1.000], loss: 9.373485, mean_absolute_error: 35.346409, mean_q: 47.343502\n",
      "  58723/700000: episode: 119, duration: 0.456s, episode steps: 91, steps per second: 200, episode reward: -329.284, mean reward: -3.619 [-100.000, 2.909], mean action: 1.637 [0.000, 3.000], mean observation: -0.124 [-1.942, 4.045], loss: 6.505744, mean_absolute_error: 34.995811, mean_q: 47.054321\n",
      "  58955/700000: episode: 120, duration: 1.158s, episode steps: 232, steps per second: 200, episode reward: 208.182, mean reward: 0.897 [-20.168, 100.000], mean action: 0.901 [0.000, 3.000], mean observation: 0.120 [-0.817, 1.125], loss: 26.681057, mean_absolute_error: 35.966908, mean_q: 48.092907\n",
      "  59098/700000: episode: 121, duration: 0.715s, episode steps: 143, steps per second: 200, episode reward: -487.837, mean reward: -3.411 [-100.000, 3.486], mean action: 1.448 [0.000, 3.000], mean observation: -0.162 [-3.297, 0.933], loss: 24.125473, mean_absolute_error: 35.685944, mean_q: 47.961678\n",
      "  59450/700000: episode: 122, duration: 1.874s, episode steps: 352, steps per second: 188, episode reward: 138.205, mean reward: 0.393 [-19.261, 100.000], mean action: 1.830 [0.000, 3.000], mean observation: 0.125 [-0.746, 1.000], loss: 15.325214, mean_absolute_error: 35.986526, mean_q: 48.243053\n",
      "  59539/700000: episode: 123, duration: 0.452s, episode steps: 89, steps per second: 197, episode reward: -102.847, mean reward: -1.156 [-100.000, 5.672], mean action: 1.978 [0.000, 3.000], mean observation: 0.002 [-1.064, 2.740], loss: 13.898377, mean_absolute_error: 35.928947, mean_q: 48.372623\n",
      "  59648/700000: episode: 124, duration: 0.543s, episode steps: 109, steps per second: 201, episode reward: -544.655, mean reward: -4.997 [-100.000, 1.987], mean action: 1.450 [0.000, 3.000], mean observation: -0.190 [-3.142, 0.977], loss: 14.230615, mean_absolute_error: 35.458385, mean_q: 47.295246\n",
      "  60055/700000: episode: 125, duration: 2.193s, episode steps: 407, steps per second: 186, episode reward: 212.269, mean reward: 0.522 [-20.738, 100.000], mean action: 1.162 [0.000, 3.000], mean observation: 0.107 [-0.747, 1.000], loss: 12.983590, mean_absolute_error: 36.397881, mean_q: 48.687988\n",
      "  60901/700000: episode: 126, duration: 5.052s, episode steps: 846, steps per second: 167, episode reward: -311.543, mean reward: -0.368 [-100.000, 34.216], mean action: 1.690 [0.000, 3.000], mean observation: 0.135 [-0.972, 1.603], loss: 12.226085, mean_absolute_error: 36.929253, mean_q: 49.509872\n",
      "  61164/700000: episode: 127, duration: 1.339s, episode steps: 263, steps per second: 196, episode reward: -112.179, mean reward: -0.427 [-100.000, 7.953], mean action: 1.821 [0.000, 3.000], mean observation: 0.027 [-1.441, 1.002], loss: 24.114138, mean_absolute_error: 37.750935, mean_q: 50.847172\n",
      "  61269/700000: episode: 128, duration: 0.526s, episode steps: 105, steps per second: 200, episode reward: -153.014, mean reward: -1.457 [-100.000, 20.194], mean action: 1.695 [0.000, 3.000], mean observation: -0.069 [-1.535, 5.893], loss: 5.842004, mean_absolute_error: 38.058895, mean_q: 51.158649\n",
      "  61417/700000: episode: 129, duration: 0.733s, episode steps: 148, steps per second: 202, episode reward: -323.832, mean reward: -2.188 [-100.000, 49.928], mean action: 1.345 [0.000, 3.000], mean observation: -0.131 [-2.739, 1.158], loss: 11.441129, mean_absolute_error: 37.736000, mean_q: 50.437553\n",
      "  61515/700000: episode: 130, duration: 0.487s, episode steps: 98, steps per second: 201, episode reward: -551.808, mean reward: -5.631 [-100.000, 1.812], mean action: 1.959 [0.000, 3.000], mean observation: -0.092 [-6.144, 1.500], loss: 29.261003, mean_absolute_error: 38.041199, mean_q: 50.985958\n",
      "  61701/700000: episode: 131, duration: 0.917s, episode steps: 186, steps per second: 203, episode reward: 223.861, mean reward: 1.204 [-7.131, 100.000], mean action: 1.204 [0.000, 3.000], mean observation: 0.083 [-0.987, 1.000], loss: 9.759547, mean_absolute_error: 38.581390, mean_q: 51.943893\n",
      "  61842/700000: episode: 132, duration: 0.707s, episode steps: 141, steps per second: 200, episode reward: 29.049, mean reward: 0.206 [-100.000, 13.072], mean action: 1.745 [0.000, 3.000], mean observation: 0.047 [-0.758, 1.000], loss: 12.232594, mean_absolute_error: 38.525738, mean_q: 51.439957\n",
      "  62009/700000: episode: 133, duration: 0.840s, episode steps: 167, steps per second: 199, episode reward: -50.990, mean reward: -0.305 [-100.000, 9.003], mean action: 1.629 [0.000, 3.000], mean observation: -0.057 [-1.272, 1.000], loss: 12.768511, mean_absolute_error: 38.721779, mean_q: 51.929546\n",
      "  62276/700000: episode: 134, duration: 1.425s, episode steps: 267, steps per second: 187, episode reward: 206.899, mean reward: 0.775 [-20.131, 100.000], mean action: 2.127 [0.000, 3.000], mean observation: 0.095 [-0.869, 1.000], loss: 29.226288, mean_absolute_error: 38.371048, mean_q: 51.490555\n",
      "  62908/700000: episode: 135, duration: 3.742s, episode steps: 632, steps per second: 169, episode reward: 210.757, mean reward: 0.333 [-21.252, 100.000], mean action: 1.206 [0.000, 3.000], mean observation: 0.164 [-0.598, 1.000], loss: 7.219030, mean_absolute_error: 38.816177, mean_q: 52.054630\n",
      "  63518/700000: episode: 136, duration: 3.626s, episode steps: 610, steps per second: 168, episode reward: 206.105, mean reward: 0.338 [-17.772, 100.000], mean action: 0.936 [0.000, 3.000], mean observation: 0.148 [-0.783, 1.000], loss: 11.982698, mean_absolute_error: 38.890476, mean_q: 52.099670\n",
      "  63741/700000: episode: 137, duration: 1.244s, episode steps: 223, steps per second: 179, episode reward: -194.482, mean reward: -0.872 [-100.000, 45.622], mean action: 1.879 [0.000, 3.000], mean observation: -0.006 [-0.802, 1.686], loss: 10.672573, mean_absolute_error: 38.943035, mean_q: 52.310642\n",
      "  63949/700000: episode: 138, duration: 1.044s, episode steps: 208, steps per second: 199, episode reward: -424.509, mean reward: -2.041 [-100.000, 4.581], mean action: 1.510 [0.000, 3.000], mean observation: -0.124 [-1.858, 2.295], loss: 18.934895, mean_absolute_error: 39.033298, mean_q: 52.105923\n",
      "  64183/700000: episode: 139, duration: 1.191s, episode steps: 234, steps per second: 197, episode reward: -247.118, mean reward: -1.056 [-100.000, 21.783], mean action: 1.551 [0.000, 3.000], mean observation: -0.011 [-0.915, 3.327], loss: 7.129324, mean_absolute_error: 38.500854, mean_q: 51.753170\n",
      "  64986/700000: episode: 140, duration: 4.561s, episode steps: 803, steps per second: 176, episode reward: 56.667, mean reward: 0.071 [-18.305, 100.000], mean action: 1.432 [0.000, 3.000], mean observation: -0.041 [-1.088, 1.000], loss: 10.370044, mean_absolute_error: 39.107132, mean_q: 52.510551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  65384/700000: episode: 141, duration: 2.338s, episode steps: 398, steps per second: 170, episode reward: -256.049, mean reward: -0.643 [-100.000, 19.037], mean action: 1.789 [0.000, 3.000], mean observation: -0.053 [-1.548, 1.000], loss: 7.871044, mean_absolute_error: 39.281235, mean_q: 52.745094\n",
      "  65901/700000: episode: 142, duration: 3.120s, episode steps: 517, steps per second: 166, episode reward: 213.922, mean reward: 0.414 [-17.375, 100.000], mean action: 1.308 [0.000, 3.000], mean observation: 0.112 [-0.771, 1.015], loss: 14.653708, mean_absolute_error: 39.103958, mean_q: 52.284832\n",
      "  66414/700000: episode: 143, duration: 2.975s, episode steps: 513, steps per second: 172, episode reward: 92.562, mean reward: 0.180 [-17.843, 100.000], mean action: 1.750 [0.000, 3.000], mean observation: 0.015 [-0.892, 1.002], loss: 14.309364, mean_absolute_error: 39.376102, mean_q: 52.697056\n",
      "  67414/700000: episode: 144, duration: 6.109s, episode steps: 1000, steps per second: 164, episode reward: 59.870, mean reward: 0.060 [-21.766, 19.014], mean action: 1.269 [0.000, 3.000], mean observation: 0.123 [-0.722, 1.000], loss: 10.676396, mean_absolute_error: 39.369587, mean_q: 52.634907\n",
      "  67606/700000: episode: 145, duration: 0.979s, episode steps: 192, steps per second: 196, episode reward: -83.415, mean reward: -0.434 [-100.000, 13.291], mean action: 1.740 [0.000, 3.000], mean observation: 0.004 [-0.680, 1.000], loss: 13.724820, mean_absolute_error: 39.661076, mean_q: 53.098011\n",
      "  68088/700000: episode: 146, duration: 2.549s, episode steps: 482, steps per second: 189, episode reward: 211.829, mean reward: 0.439 [-17.651, 100.000], mean action: 1.571 [0.000, 3.000], mean observation: 0.118 [-0.636, 1.000], loss: 8.226537, mean_absolute_error: 39.709885, mean_q: 53.165237\n",
      "  68411/700000: episode: 147, duration: 1.683s, episode steps: 323, steps per second: 192, episode reward: 190.748, mean reward: 0.591 [-11.664, 100.000], mean action: 1.743 [0.000, 3.000], mean observation: 0.101 [-0.730, 1.000], loss: 9.931839, mean_absolute_error: 39.805969, mean_q: 53.214661\n",
      "  69179/700000: episode: 148, duration: 4.301s, episode steps: 768, steps per second: 179, episode reward: -224.897, mean reward: -0.293 [-100.000, 20.739], mean action: 1.836 [0.000, 3.000], mean observation: -0.014 [-1.021, 1.013], loss: 8.680333, mean_absolute_error: 40.016342, mean_q: 53.523762\n",
      "  69579/700000: episode: 149, duration: 2.198s, episode steps: 400, steps per second: 182, episode reward: 205.595, mean reward: 0.514 [-19.404, 100.000], mean action: 1.488 [0.000, 3.000], mean observation: 0.121 [-1.122, 1.000], loss: 12.614756, mean_absolute_error: 40.321808, mean_q: 53.820107\n",
      "  69802/700000: episode: 150, duration: 1.132s, episode steps: 223, steps per second: 197, episode reward: -180.218, mean reward: -0.808 [-100.000, 13.943], mean action: 1.511 [0.000, 3.000], mean observation: 0.027 [-1.253, 1.025], loss: 16.596230, mean_absolute_error: 40.398342, mean_q: 54.131283\n",
      "  70067/700000: episode: 151, duration: 1.337s, episode steps: 265, steps per second: 198, episode reward: 189.568, mean reward: 0.715 [-3.588, 100.000], mean action: 0.898 [0.000, 3.000], mean observation: 0.114 [-0.713, 1.220], loss: 11.930189, mean_absolute_error: 40.309502, mean_q: 53.953148\n",
      "  70447/700000: episode: 152, duration: 2.003s, episode steps: 380, steps per second: 190, episode reward: 203.837, mean reward: 0.536 [-6.053, 100.000], mean action: 1.311 [0.000, 3.000], mean observation: 0.086 [-0.569, 1.082], loss: 11.899139, mean_absolute_error: 40.407318, mean_q: 54.042530\n",
      "  70682/700000: episode: 153, duration: 1.195s, episode steps: 235, steps per second: 197, episode reward: -29.745, mean reward: -0.127 [-100.000, 14.074], mean action: 1.579 [0.000, 3.000], mean observation: 0.002 [-0.921, 1.566], loss: 4.748155, mean_absolute_error: 40.253284, mean_q: 53.932007\n",
      "  70765/700000: episode: 154, duration: 0.417s, episode steps: 83, steps per second: 199, episode reward: -132.212, mean reward: -1.593 [-100.000, 6.470], mean action: 1.325 [0.000, 3.000], mean observation: 0.131 [-2.676, 1.000], loss: 14.928515, mean_absolute_error: 40.712734, mean_q: 54.919914\n",
      "  71076/700000: episode: 155, duration: 1.661s, episode steps: 311, steps per second: 187, episode reward: 239.404, mean reward: 0.770 [-7.219, 100.000], mean action: 1.508 [0.000, 3.000], mean observation: 0.090 [-0.942, 1.021], loss: 14.492314, mean_absolute_error: 40.698025, mean_q: 54.364815\n",
      "  71603/700000: episode: 156, duration: 2.945s, episode steps: 527, steps per second: 179, episode reward: 232.536, mean reward: 0.441 [-22.614, 100.000], mean action: 1.027 [0.000, 3.000], mean observation: 0.142 [-0.684, 1.000], loss: 13.007277, mean_absolute_error: 40.542236, mean_q: 54.217934\n",
      "  72134/700000: episode: 157, duration: 3.097s, episode steps: 531, steps per second: 171, episode reward: 136.563, mean reward: 0.257 [-19.501, 100.000], mean action: 1.576 [0.000, 3.000], mean observation: 0.086 [-0.810, 1.053], loss: 11.063631, mean_absolute_error: 40.759151, mean_q: 54.462048\n",
      "  72649/700000: episode: 158, duration: 2.996s, episode steps: 515, steps per second: 172, episode reward: 188.315, mean reward: 0.366 [-18.827, 100.000], mean action: 1.322 [0.000, 3.000], mean observation: 0.066 [-0.553, 1.000], loss: 13.064436, mean_absolute_error: 40.498795, mean_q: 54.176968\n",
      "  72886/700000: episode: 159, duration: 1.277s, episode steps: 237, steps per second: 186, episode reward: 218.785, mean reward: 0.923 [-8.929, 100.000], mean action: 1.388 [0.000, 3.000], mean observation: 0.089 [-0.667, 1.291], loss: 9.421391, mean_absolute_error: 40.462563, mean_q: 54.393730\n",
      "  73584/700000: episode: 160, duration: 4.014s, episode steps: 698, steps per second: 174, episode reward: 187.844, mean reward: 0.269 [-19.126, 100.000], mean action: 2.021 [0.000, 3.000], mean observation: 0.163 [-0.855, 1.000], loss: 11.467352, mean_absolute_error: 40.834450, mean_q: 54.601242\n",
      "  73869/700000: episode: 161, duration: 1.519s, episode steps: 285, steps per second: 188, episode reward: 173.613, mean reward: 0.609 [-9.116, 100.000], mean action: 1.488 [0.000, 3.000], mean observation: 0.046 [-0.674, 1.000], loss: 10.294760, mean_absolute_error: 41.028667, mean_q: 54.687370\n",
      "  74869/700000: episode: 162, duration: 6.257s, episode steps: 1000, steps per second: 160, episode reward: 36.794, mean reward: 0.037 [-22.743, 13.287], mean action: 1.522 [0.000, 3.000], mean observation: 0.171 [-0.660, 1.000], loss: 10.562772, mean_absolute_error: 40.888504, mean_q: 54.747692\n",
      "  75100/700000: episode: 163, duration: 1.233s, episode steps: 231, steps per second: 187, episode reward: -53.835, mean reward: -0.233 [-100.000, 11.103], mean action: 1.446 [0.000, 3.000], mean observation: 0.023 [-1.512, 1.180], loss: 13.492139, mean_absolute_error: 41.121407, mean_q: 54.876453\n",
      "  75255/700000: episode: 164, duration: 0.837s, episode steps: 155, steps per second: 185, episode reward: -14.645, mean reward: -0.094 [-100.000, 12.571], mean action: 1.819 [0.000, 3.000], mean observation: 0.049 [-0.671, 1.000], loss: 13.718719, mean_absolute_error: 40.679611, mean_q: 54.094997\n",
      "  76254/700000: episode: 165, duration: 5.929s, episode steps: 999, steps per second: 169, episode reward: 190.131, mean reward: 0.190 [-19.935, 100.000], mean action: 1.250 [0.000, 3.000], mean observation: 0.199 [-0.752, 1.000], loss: 10.302396, mean_absolute_error: 40.792877, mean_q: 54.598965\n",
      "  76504/700000: episode: 166, duration: 1.320s, episode steps: 250, steps per second: 189, episode reward: -24.917, mean reward: -0.100 [-100.000, 15.073], mean action: 1.760 [0.000, 3.000], mean observation: 0.017 [-0.972, 1.000], loss: 7.243363, mean_absolute_error: 41.130901, mean_q: 55.085854\n",
      "  76910/700000: episode: 167, duration: 2.237s, episode steps: 406, steps per second: 181, episode reward: 100.624, mean reward: 0.248 [-26.686, 100.000], mean action: 1.574 [0.000, 3.000], mean observation: -0.027 [-0.843, 1.000], loss: 12.403782, mean_absolute_error: 41.792915, mean_q: 55.790909\n",
      "  77276/700000: episode: 168, duration: 2.102s, episode steps: 366, steps per second: 174, episode reward: 245.815, mean reward: 0.672 [-20.031, 100.000], mean action: 1.459 [0.000, 3.000], mean observation: 0.075 [-1.148, 1.000], loss: 13.337495, mean_absolute_error: 42.475750, mean_q: 56.836067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  77447/700000: episode: 169, duration: 0.902s, episode steps: 171, steps per second: 190, episode reward: -49.296, mean reward: -0.288 [-100.000, 15.879], mean action: 1.620 [0.000, 3.000], mean observation: -0.028 [-1.141, 1.000], loss: 14.976621, mean_absolute_error: 42.348072, mean_q: 56.527309\n",
      "  77821/700000: episode: 170, duration: 2.109s, episode steps: 374, steps per second: 177, episode reward: 216.360, mean reward: 0.579 [-10.714, 100.000], mean action: 1.444 [0.000, 3.000], mean observation: 0.111 [-0.589, 1.000], loss: 7.942354, mean_absolute_error: 42.850498, mean_q: 57.476376\n",
      "  77997/700000: episode: 171, duration: 0.873s, episode steps: 176, steps per second: 202, episode reward: -56.812, mean reward: -0.323 [-100.000, 9.736], mean action: 1.358 [0.000, 3.000], mean observation: 0.038 [-0.745, 1.000], loss: 14.248040, mean_absolute_error: 42.632629, mean_q: 57.106045\n",
      "  78475/700000: episode: 172, duration: 2.530s, episode steps: 478, steps per second: 189, episode reward: 229.754, mean reward: 0.481 [-13.240, 100.000], mean action: 1.766 [0.000, 3.000], mean observation: 0.143 [-0.629, 1.000], loss: 18.813816, mean_absolute_error: 42.723587, mean_q: 57.098900\n",
      "  78776/700000: episode: 173, duration: 1.520s, episode steps: 301, steps per second: 198, episode reward: -74.633, mean reward: -0.248 [-100.000, 10.615], mean action: 1.648 [0.000, 3.000], mean observation: 0.124 [-0.741, 1.000], loss: 25.811598, mean_absolute_error: 42.808670, mean_q: 57.244686\n",
      "  79293/700000: episode: 174, duration: 2.726s, episode steps: 517, steps per second: 190, episode reward: 182.306, mean reward: 0.353 [-19.116, 100.000], mean action: 0.969 [0.000, 3.000], mean observation: 0.059 [-1.631, 1.000], loss: 14.879909, mean_absolute_error: 42.658504, mean_q: 57.138729\n",
      "  79493/700000: episode: 175, duration: 1.009s, episode steps: 200, steps per second: 198, episode reward: -13.539, mean reward: -0.068 [-100.000, 17.997], mean action: 1.750 [0.000, 3.000], mean observation: 0.071 [-1.563, 1.002], loss: 25.562681, mean_absolute_error: 42.981983, mean_q: 57.231377\n",
      "  80012/700000: episode: 176, duration: 2.812s, episode steps: 519, steps per second: 185, episode reward: 179.550, mean reward: 0.346 [-22.643, 100.000], mean action: 1.468 [0.000, 3.000], mean observation: 0.166 [-0.813, 1.000], loss: 10.802071, mean_absolute_error: 43.034966, mean_q: 57.500065\n",
      "  80528/700000: episode: 177, duration: 2.895s, episode steps: 516, steps per second: 178, episode reward: 194.301, mean reward: 0.377 [-18.223, 100.000], mean action: 1.246 [0.000, 3.000], mean observation: 0.075 [-1.378, 1.000], loss: 11.820948, mean_absolute_error: 43.581356, mean_q: 58.283485\n",
      "  80947/700000: episode: 178, duration: 2.137s, episode steps: 419, steps per second: 196, episode reward: 157.562, mean reward: 0.376 [-21.479, 100.000], mean action: 1.389 [0.000, 3.000], mean observation: 0.230 [-0.820, 1.000], loss: 19.968992, mean_absolute_error: 43.202286, mean_q: 57.942482\n",
      "  81225/700000: episode: 179, duration: 1.450s, episode steps: 278, steps per second: 192, episode reward: -31.107, mean reward: -0.112 [-100.000, 10.912], mean action: 1.525 [0.000, 3.000], mean observation: 0.131 [-0.672, 1.000], loss: 9.502209, mean_absolute_error: 42.987206, mean_q: 57.529613\n",
      "  81403/700000: episode: 180, duration: 0.893s, episode steps: 178, steps per second: 199, episode reward: -75.927, mean reward: -0.427 [-100.000, 11.161], mean action: 1.590 [0.000, 3.000], mean observation: 0.067 [-0.683, 1.597], loss: 20.385235, mean_absolute_error: 43.357357, mean_q: 57.745316\n",
      "  81989/700000: episode: 181, duration: 3.290s, episode steps: 586, steps per second: 178, episode reward: -155.485, mean reward: -0.265 [-100.000, 18.118], mean action: 1.488 [0.000, 3.000], mean observation: 0.074 [-0.970, 1.000], loss: 13.901857, mean_absolute_error: 43.674549, mean_q: 58.484756\n",
      "  82256/700000: episode: 182, duration: 1.365s, episode steps: 267, steps per second: 196, episode reward: -99.647, mean reward: -0.373 [-100.000, 19.657], mean action: 1.727 [0.000, 3.000], mean observation: 0.152 [-0.897, 1.040], loss: 13.755933, mean_absolute_error: 43.436134, mean_q: 58.177860\n",
      "  83256/700000: episode: 183, duration: 5.472s, episode steps: 1000, steps per second: 183, episode reward: 15.420, mean reward: 0.015 [-22.063, 23.086], mean action: 1.631 [0.000, 3.000], mean observation: 0.268 [-0.773, 1.000], loss: 12.911933, mean_absolute_error: 44.440228, mean_q: 59.531315\n",
      "  83696/700000: episode: 184, duration: 2.344s, episode steps: 440, steps per second: 188, episode reward: 215.356, mean reward: 0.489 [-10.383, 100.000], mean action: 1.470 [0.000, 3.000], mean observation: 0.064 [-0.601, 1.010], loss: 11.322732, mean_absolute_error: 44.535645, mean_q: 59.349281\n",
      "  84139/700000: episode: 185, duration: 2.523s, episode steps: 443, steps per second: 176, episode reward: 209.410, mean reward: 0.473 [-18.632, 100.000], mean action: 1.512 [0.000, 3.000], mean observation: 0.086 [-0.519, 1.000], loss: 14.065748, mean_absolute_error: 44.820984, mean_q: 60.042202\n",
      "  84683/700000: episode: 186, duration: 3.020s, episode steps: 544, steps per second: 180, episode reward: 169.790, mean reward: 0.312 [-17.274, 100.000], mean action: 1.572 [0.000, 3.000], mean observation: 0.039 [-1.052, 1.000], loss: 9.425241, mean_absolute_error: 44.708580, mean_q: 59.986485\n",
      "  85592/700000: episode: 187, duration: 5.386s, episode steps: 909, steps per second: 169, episode reward: 173.768, mean reward: 0.191 [-20.588, 100.000], mean action: 1.736 [0.000, 3.000], mean observation: 0.157 [-0.942, 1.000], loss: 13.352490, mean_absolute_error: 44.649696, mean_q: 59.830406\n",
      "  86067/700000: episode: 188, duration: 2.557s, episode steps: 475, steps per second: 186, episode reward: 209.059, mean reward: 0.440 [-11.414, 100.000], mean action: 1.141 [0.000, 3.000], mean observation: 0.099 [-0.731, 1.000], loss: 14.438156, mean_absolute_error: 45.130566, mean_q: 60.368469\n",
      "  86629/700000: episode: 189, duration: 2.960s, episode steps: 562, steps per second: 190, episode reward: -156.332, mean reward: -0.278 [-100.000, 16.757], mean action: 1.480 [0.000, 3.000], mean observation: 0.227 [-1.165, 1.030], loss: 11.192086, mean_absolute_error: 44.483795, mean_q: 59.857025\n",
      "  87055/700000: episode: 190, duration: 2.285s, episode steps: 426, steps per second: 186, episode reward: 170.888, mean reward: 0.401 [-24.133, 100.000], mean action: 1.507 [0.000, 3.000], mean observation: 0.124 [-0.588, 1.000], loss: 9.500916, mean_absolute_error: 44.980385, mean_q: 60.413815\n",
      "  87192/700000: episode: 191, duration: 0.693s, episode steps: 137, steps per second: 198, episode reward: -42.873, mean reward: -0.313 [-100.000, 17.576], mean action: 1.905 [0.000, 3.000], mean observation: 0.038 [-0.725, 1.000], loss: 29.575253, mean_absolute_error: 44.676765, mean_q: 59.883232\n",
      "  87250/700000: episode: 192, duration: 0.290s, episode steps: 58, steps per second: 200, episode reward: -246.890, mean reward: -4.257 [-100.000, 7.109], mean action: 1.000 [0.000, 3.000], mean observation: -0.292 [-4.214, 1.000], loss: 23.131140, mean_absolute_error: 44.927254, mean_q: 60.255337\n",
      "  87914/700000: episode: 193, duration: 3.664s, episode steps: 664, steps per second: 181, episode reward: 142.338, mean reward: 0.214 [-18.867, 100.000], mean action: 1.373 [0.000, 3.000], mean observation: 0.083 [-0.979, 1.036], loss: 13.921748, mean_absolute_error: 44.976955, mean_q: 60.320141\n",
      "  88280/700000: episode: 194, duration: 1.908s, episode steps: 366, steps per second: 192, episode reward: 191.256, mean reward: 0.523 [-19.590, 100.000], mean action: 0.940 [0.000, 3.000], mean observation: 0.147 [-0.633, 1.281], loss: 13.906788, mean_absolute_error: 44.943329, mean_q: 60.108288\n",
      "  88529/700000: episode: 195, duration: 1.250s, episode steps: 249, steps per second: 199, episode reward: 217.568, mean reward: 0.874 [-3.585, 100.000], mean action: 1.269 [0.000, 3.000], mean observation: 0.069 [-0.617, 1.000], loss: 11.543275, mean_absolute_error: 44.676975, mean_q: 59.860371\n",
      "  88721/700000: episode: 196, duration: 0.962s, episode steps: 192, steps per second: 200, episode reward: -82.360, mean reward: -0.429 [-100.000, 34.102], mean action: 1.562 [0.000, 3.000], mean observation: -0.050 [-0.731, 1.000], loss: 17.124687, mean_absolute_error: 44.860229, mean_q: 60.132092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  88836/700000: episode: 197, duration: 0.575s, episode steps: 115, steps per second: 200, episode reward: -82.686, mean reward: -0.719 [-100.000, 10.085], mean action: 1.635 [0.000, 3.000], mean observation: -0.075 [-0.794, 3.042], loss: 9.529130, mean_absolute_error: 44.650139, mean_q: 59.883198\n",
      "  89270/700000: episode: 198, duration: 2.258s, episode steps: 434, steps per second: 192, episode reward: 228.050, mean reward: 0.525 [-11.476, 100.000], mean action: 1.187 [0.000, 3.000], mean observation: 0.134 [-0.617, 1.000], loss: 10.896939, mean_absolute_error: 44.655975, mean_q: 59.767757\n",
      "  89782/700000: episode: 199, duration: 2.892s, episode steps: 512, steps per second: 177, episode reward: 177.516, mean reward: 0.347 [-18.342, 100.000], mean action: 1.658 [0.000, 3.000], mean observation: 0.090 [-1.041, 1.266], loss: 8.942348, mean_absolute_error: 45.020309, mean_q: 60.283909\n",
      "  90782/700000: episode: 200, duration: 5.573s, episode steps: 1000, steps per second: 179, episode reward: 26.983, mean reward: 0.027 [-22.912, 20.968], mean action: 1.560 [0.000, 3.000], mean observation: 0.160 [-0.679, 1.000], loss: 13.292988, mean_absolute_error: 45.407421, mean_q: 60.740795\n",
      "  91526/700000: episode: 201, duration: 4.184s, episode steps: 744, steps per second: 178, episode reward: 178.712, mean reward: 0.240 [-19.208, 100.000], mean action: 1.558 [0.000, 3.000], mean observation: 0.212 [-0.846, 1.000], loss: 17.585081, mean_absolute_error: 46.089142, mean_q: 61.554657\n",
      "  91646/700000: episode: 202, duration: 0.593s, episode steps: 120, steps per second: 202, episode reward: -344.318, mean reward: -2.869 [-100.000, 36.075], mean action: 1.117 [0.000, 3.000], mean observation: -0.252 [-3.021, 1.000], loss: 23.238914, mean_absolute_error: 46.246880, mean_q: 61.584812\n",
      "  92347/700000: episode: 203, duration: 3.938s, episode steps: 701, steps per second: 178, episode reward: 103.134, mean reward: 0.147 [-19.682, 100.000], mean action: 2.061 [0.000, 3.000], mean observation: 0.145 [-0.909, 1.000], loss: 12.866438, mean_absolute_error: 45.788559, mean_q: 61.328308\n",
      "  92907/700000: episode: 204, duration: 3.170s, episode steps: 560, steps per second: 177, episode reward: 98.710, mean reward: 0.176 [-26.343, 100.000], mean action: 1.664 [0.000, 3.000], mean observation: 0.064 [-0.708, 1.000], loss: 13.273552, mean_absolute_error: 45.669113, mean_q: 60.880566\n",
      "  93590/700000: episode: 205, duration: 3.803s, episode steps: 683, steps per second: 180, episode reward: 193.827, mean reward: 0.284 [-20.702, 100.000], mean action: 1.076 [0.000, 3.000], mean observation: 0.227 [-0.879, 1.000], loss: 19.630861, mean_absolute_error: 46.307632, mean_q: 61.939182\n",
      "  93746/700000: episode: 206, duration: 0.784s, episode steps: 156, steps per second: 199, episode reward: -256.196, mean reward: -1.642 [-100.000, 16.664], mean action: 1.551 [0.000, 3.000], mean observation: -0.162 [-2.593, 1.000], loss: 22.005541, mean_absolute_error: 46.935177, mean_q: 62.784817\n",
      "  93814/700000: episode: 207, duration: 0.338s, episode steps: 68, steps per second: 201, episode reward: -90.135, mean reward: -1.326 [-100.000, 20.276], mean action: 1.191 [0.000, 3.000], mean observation: 0.063 [-2.274, 1.000], loss: 16.209837, mean_absolute_error: 47.154438, mean_q: 63.549107\n",
      "  94041/700000: episode: 208, duration: 1.136s, episode steps: 227, steps per second: 200, episode reward: 212.346, mean reward: 0.935 [-17.958, 100.000], mean action: 1.163 [0.000, 3.000], mean observation: 0.079 [-0.797, 1.000], loss: 42.758785, mean_absolute_error: 46.763523, mean_q: 62.718876\n",
      "  94297/700000: episode: 209, duration: 1.309s, episode steps: 256, steps per second: 196, episode reward: -121.385, mean reward: -0.474 [-100.000, 4.057], mean action: 1.902 [0.000, 3.000], mean observation: -0.105 [-1.001, 0.927], loss: 20.052332, mean_absolute_error: 46.099888, mean_q: 61.701813\n",
      "  94489/700000: episode: 210, duration: 0.966s, episode steps: 192, steps per second: 199, episode reward: -17.314, mean reward: -0.090 [-100.000, 15.392], mean action: 1.698 [0.000, 3.000], mean observation: -0.009 [-1.220, 1.000], loss: 21.318388, mean_absolute_error: 46.383274, mean_q: 62.140575\n",
      "  94813/700000: episode: 211, duration: 1.648s, episode steps: 324, steps per second: 197, episode reward: 189.007, mean reward: 0.583 [-18.303, 100.000], mean action: 1.198 [0.000, 3.000], mean observation: 0.112 [-0.595, 1.000], loss: 23.842470, mean_absolute_error: 46.912979, mean_q: 62.829102\n",
      "  95381/700000: episode: 212, duration: 3.124s, episode steps: 568, steps per second: 182, episode reward: 159.152, mean reward: 0.280 [-17.519, 100.000], mean action: 1.806 [0.000, 3.000], mean observation: 0.132 [-0.715, 1.000], loss: 14.393082, mean_absolute_error: 46.976048, mean_q: 62.773273\n",
      "  95796/700000: episode: 213, duration: 2.177s, episode steps: 415, steps per second: 191, episode reward: 221.144, mean reward: 0.533 [-18.300, 100.000], mean action: 1.193 [0.000, 3.000], mean observation: 0.136 [-0.597, 1.003], loss: 13.535639, mean_absolute_error: 47.023819, mean_q: 62.880524\n",
      "  96699/700000: episode: 214, duration: 4.838s, episode steps: 903, steps per second: 187, episode reward: 211.704, mean reward: 0.234 [-21.416, 100.000], mean action: 0.930 [0.000, 3.000], mean observation: 0.187 [-0.759, 1.180], loss: 16.286922, mean_absolute_error: 46.988388, mean_q: 62.680416\n",
      "  96805/700000: episode: 215, duration: 0.522s, episode steps: 106, steps per second: 203, episode reward: -148.043, mean reward: -1.397 [-100.000, 11.354], mean action: 1.189 [0.000, 3.000], mean observation: -0.083 [-0.964, 3.066], loss: 21.094826, mean_absolute_error: 47.119846, mean_q: 62.493916\n",
      "  97256/700000: episode: 216, duration: 2.414s, episode steps: 451, steps per second: 187, episode reward: 219.357, mean reward: 0.486 [-13.885, 100.000], mean action: 1.392 [0.000, 3.000], mean observation: 0.115 [-0.942, 1.004], loss: 13.852838, mean_absolute_error: 47.262646, mean_q: 63.000095\n",
      "  97974/700000: episode: 217, duration: 3.885s, episode steps: 718, steps per second: 185, episode reward: 226.312, mean reward: 0.315 [-21.962, 100.000], mean action: 1.025 [0.000, 3.000], mean observation: 0.223 [-0.622, 1.000], loss: 16.060450, mean_absolute_error: 47.192673, mean_q: 63.080475\n",
      "  98297/700000: episode: 218, duration: 1.666s, episode steps: 323, steps per second: 194, episode reward: 220.507, mean reward: 0.683 [-5.714, 100.000], mean action: 1.393 [0.000, 3.000], mean observation: 0.089 [-0.636, 1.028], loss: 15.456859, mean_absolute_error: 47.519833, mean_q: 63.335926\n",
      "  98634/700000: episode: 219, duration: 1.785s, episode steps: 337, steps per second: 189, episode reward: 185.737, mean reward: 0.551 [-14.711, 100.000], mean action: 1.350 [0.000, 3.000], mean observation: 0.105 [-0.736, 1.000], loss: 13.681552, mean_absolute_error: 47.408314, mean_q: 63.231083\n",
      "  99041/700000: episode: 220, duration: 2.181s, episode steps: 407, steps per second: 187, episode reward: 262.017, mean reward: 0.644 [-17.424, 100.000], mean action: 1.533 [0.000, 3.000], mean observation: 0.098 [-0.940, 1.122], loss: 18.381779, mean_absolute_error: 47.162777, mean_q: 63.008591\n",
      "  99422/700000: episode: 221, duration: 1.955s, episode steps: 381, steps per second: 195, episode reward: 246.405, mean reward: 0.647 [-10.249, 100.000], mean action: 1.223 [0.000, 3.000], mean observation: 0.148 [-0.713, 1.013], loss: 7.336553, mean_absolute_error: 46.801971, mean_q: 62.517105\n",
      "  99989/700000: episode: 222, duration: 3.261s, episode steps: 567, steps per second: 174, episode reward: 167.188, mean reward: 0.295 [-20.193, 100.000], mean action: 1.661 [0.000, 3.000], mean observation: 0.055 [-1.266, 1.017], loss: 8.976238, mean_absolute_error: 47.030174, mean_q: 62.849701\n",
      " 100606/700000: episode: 223, duration: 3.310s, episode steps: 617, steps per second: 186, episode reward: 228.636, mean reward: 0.371 [-19.994, 100.000], mean action: 1.143 [0.000, 3.000], mean observation: 0.183 [-0.630, 1.000], loss: 12.568546, mean_absolute_error: 46.983154, mean_q: 62.870777\n",
      " 100697/700000: episode: 224, duration: 0.449s, episode steps: 91, steps per second: 203, episode reward: -233.807, mean reward: -2.569 [-100.000, 24.058], mean action: 1.077 [0.000, 3.000], mean observation: -0.188 [-3.574, 1.000], loss: 13.307749, mean_absolute_error: 47.072968, mean_q: 62.859299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 101057/700000: episode: 225, duration: 1.859s, episode steps: 360, steps per second: 194, episode reward: 228.583, mean reward: 0.635 [-20.841, 100.000], mean action: 1.517 [0.000, 3.000], mean observation: 0.053 [-0.808, 1.000], loss: 10.600630, mean_absolute_error: 46.216526, mean_q: 61.799984\n",
      " 101171/700000: episode: 226, duration: 0.572s, episode steps: 114, steps per second: 199, episode reward: -47.904, mean reward: -0.420 [-100.000, 15.635], mean action: 1.798 [0.000, 3.000], mean observation: -0.029 [-0.784, 1.609], loss: 6.442275, mean_absolute_error: 46.399052, mean_q: 62.117992\n",
      " 101449/700000: episode: 227, duration: 1.411s, episode steps: 278, steps per second: 197, episode reward: 189.169, mean reward: 0.680 [-17.902, 100.000], mean action: 1.209 [0.000, 3.000], mean observation: 0.062 [-1.025, 1.000], loss: 7.210200, mean_absolute_error: 46.505829, mean_q: 62.171341\n",
      " 101759/700000: episode: 228, duration: 1.582s, episode steps: 310, steps per second: 196, episode reward: 201.749, mean reward: 0.651 [-10.734, 100.000], mean action: 1.116 [0.000, 3.000], mean observation: 0.131 [-0.730, 1.081], loss: 11.022635, mean_absolute_error: 46.482647, mean_q: 61.773884\n",
      " 102070/700000: episode: 229, duration: 1.609s, episode steps: 311, steps per second: 193, episode reward: 218.228, mean reward: 0.702 [-8.157, 100.000], mean action: 1.585 [0.000, 3.000], mean observation: 0.045 [-0.703, 1.000], loss: 13.142003, mean_absolute_error: 46.500805, mean_q: 61.930000\n",
      " 103070/700000: episode: 230, duration: 5.460s, episode steps: 1000, steps per second: 183, episode reward: 105.058, mean reward: 0.105 [-19.417, 22.423], mean action: 0.826 [0.000, 3.000], mean observation: 0.210 [-0.718, 1.000], loss: 12.100705, mean_absolute_error: 46.576660, mean_q: 62.241314\n",
      " 103357/700000: episode: 231, duration: 1.460s, episode steps: 287, steps per second: 197, episode reward: 212.744, mean reward: 0.741 [-8.656, 100.000], mean action: 1.383 [0.000, 3.000], mean observation: 0.087 [-0.769, 1.002], loss: 17.589249, mean_absolute_error: 46.185654, mean_q: 61.486336\n",
      " 103623/700000: episode: 232, duration: 1.371s, episode steps: 266, steps per second: 194, episode reward: -75.758, mean reward: -0.285 [-100.000, 16.868], mean action: 1.650 [0.000, 3.000], mean observation: -0.014 [-1.690, 1.000], loss: 9.882463, mean_absolute_error: 46.499363, mean_q: 62.011307\n",
      " 103933/700000: episode: 233, duration: 1.620s, episode steps: 310, steps per second: 191, episode reward: 238.756, mean reward: 0.770 [-9.562, 100.000], mean action: 1.603 [0.000, 3.000], mean observation: 0.121 [-0.632, 1.000], loss: 6.162591, mean_absolute_error: 46.370670, mean_q: 61.719936\n",
      " 104271/700000: episode: 234, duration: 1.758s, episode steps: 338, steps per second: 192, episode reward: 210.730, mean reward: 0.623 [-13.561, 100.000], mean action: 1.352 [0.000, 3.000], mean observation: 0.080 [-0.648, 1.000], loss: 8.527728, mean_absolute_error: 46.762394, mean_q: 62.077213\n",
      " 104873/700000: episode: 235, duration: 3.139s, episode steps: 602, steps per second: 192, episode reward: 228.493, mean reward: 0.380 [-23.582, 100.000], mean action: 0.885 [0.000, 3.000], mean observation: 0.177 [-0.535, 1.000], loss: 10.477958, mean_absolute_error: 46.511559, mean_q: 61.738068\n",
      " 104986/700000: episode: 236, duration: 0.567s, episode steps: 113, steps per second: 199, episode reward: -82.070, mean reward: -0.726 [-100.000, 19.764], mean action: 1.743 [0.000, 3.000], mean observation: -0.097 [-0.877, 1.496], loss: 10.192936, mean_absolute_error: 46.228104, mean_q: 61.374035\n",
      " 105621/700000: episode: 237, duration: 3.455s, episode steps: 635, steps per second: 184, episode reward: -144.438, mean reward: -0.227 [-100.000, 10.450], mean action: 1.704 [0.000, 3.000], mean observation: -0.014 [-1.327, 1.000], loss: 10.122838, mean_absolute_error: 46.291355, mean_q: 61.464630\n",
      " 106203/700000: episode: 238, duration: 3.065s, episode steps: 582, steps per second: 190, episode reward: 256.505, mean reward: 0.441 [-20.860, 100.000], mean action: 0.985 [0.000, 3.000], mean observation: 0.145 [-0.764, 1.000], loss: 10.095047, mean_absolute_error: 46.336151, mean_q: 61.463890\n",
      " 106555/700000: episode: 239, duration: 2.016s, episode steps: 352, steps per second: 175, episode reward: 231.188, mean reward: 0.657 [-17.708, 100.000], mean action: 1.256 [0.000, 3.000], mean observation: 0.077 [-0.786, 1.000], loss: 11.128025, mean_absolute_error: 46.609974, mean_q: 61.602005\n",
      " 106775/700000: episode: 240, duration: 1.141s, episode steps: 220, steps per second: 193, episode reward: 197.214, mean reward: 0.896 [-3.056, 100.000], mean action: 1.241 [0.000, 3.000], mean observation: 0.081 [-0.823, 1.000], loss: 6.475170, mean_absolute_error: 46.309631, mean_q: 61.284191\n",
      " 107167/700000: episode: 241, duration: 2.124s, episode steps: 392, steps per second: 185, episode reward: 223.935, mean reward: 0.571 [-10.661, 100.000], mean action: 1.232 [0.000, 3.000], mean observation: 0.180 [-0.915, 1.000], loss: 10.540795, mean_absolute_error: 46.963886, mean_q: 62.188755\n",
      " 107252/700000: episode: 242, duration: 0.546s, episode steps: 85, steps per second: 156, episode reward: -31.506, mean reward: -0.371 [-100.000, 15.651], mean action: 1.753 [0.000, 3.000], mean observation: -0.026 [-0.945, 1.000], loss: 4.230478, mean_absolute_error: 46.394707, mean_q: 60.946255\n",
      " 107413/700000: episode: 243, duration: 0.844s, episode steps: 161, steps per second: 191, episode reward: -4.929, mean reward: -0.031 [-100.000, 16.205], mean action: 1.814 [0.000, 3.000], mean observation: 0.012 [-1.427, 1.000], loss: 15.958295, mean_absolute_error: 46.696194, mean_q: 61.560112\n",
      " 107707/700000: episode: 244, duration: 1.621s, episode steps: 294, steps per second: 181, episode reward: 208.809, mean reward: 0.710 [-17.655, 100.000], mean action: 1.493 [0.000, 3.000], mean observation: 0.046 [-0.849, 1.000], loss: 12.450782, mean_absolute_error: 46.658703, mean_q: 61.640041\n",
      " 108036/700000: episode: 245, duration: 1.739s, episode steps: 329, steps per second: 189, episode reward: 206.296, mean reward: 0.627 [-20.554, 100.000], mean action: 1.286 [0.000, 3.000], mean observation: 0.085 [-0.658, 1.000], loss: 9.136786, mean_absolute_error: 46.329254, mean_q: 61.492809\n",
      " 109012/700000: episode: 246, duration: 5.566s, episode steps: 976, steps per second: 175, episode reward: 209.967, mean reward: 0.215 [-19.939, 100.000], mean action: 1.303 [0.000, 3.000], mean observation: 0.193 [-0.640, 1.271], loss: 15.422122, mean_absolute_error: 45.994961, mean_q: 61.010380\n",
      " 109642/700000: episode: 247, duration: 3.462s, episode steps: 630, steps per second: 182, episode reward: 193.147, mean reward: 0.307 [-17.938, 100.000], mean action: 0.954 [0.000, 3.000], mean observation: 0.128 [-0.589, 1.000], loss: 11.591278, mean_absolute_error: 45.482868, mean_q: 60.459946\n",
      " 109858/700000: episode: 248, duration: 1.087s, episode steps: 216, steps per second: 199, episode reward: 197.783, mean reward: 0.916 [-2.937, 100.000], mean action: 1.324 [0.000, 3.000], mean observation: 0.063 [-0.769, 1.000], loss: 16.216667, mean_absolute_error: 45.038120, mean_q: 59.974033\n",
      " 110290/700000: episode: 249, duration: 2.368s, episode steps: 432, steps per second: 182, episode reward: 215.746, mean reward: 0.499 [-19.179, 100.000], mean action: 1.271 [0.000, 3.000], mean observation: 0.117 [-0.760, 1.000], loss: 10.410021, mean_absolute_error: 45.153355, mean_q: 60.188557\n",
      " 110598/700000: episode: 250, duration: 1.548s, episode steps: 308, steps per second: 199, episode reward: 250.425, mean reward: 0.813 [-18.298, 100.000], mean action: 1.169 [0.000, 3.000], mean observation: 0.130 [-0.793, 1.009], loss: 17.571877, mean_absolute_error: 44.856812, mean_q: 59.857651\n",
      " 110881/700000: episode: 251, duration: 1.434s, episode steps: 283, steps per second: 197, episode reward: 137.181, mean reward: 0.485 [-9.432, 100.000], mean action: 1.314 [0.000, 3.000], mean observation: -0.048 [-0.906, 1.022], loss: 12.143295, mean_absolute_error: 45.032436, mean_q: 60.275024\n",
      " 111352/700000: episode: 252, duration: 2.609s, episode steps: 471, steps per second: 181, episode reward: 186.382, mean reward: 0.396 [-19.912, 100.000], mean action: 1.038 [0.000, 3.000], mean observation: 0.128 [-1.022, 1.000], loss: 12.973235, mean_absolute_error: 44.963989, mean_q: 60.279133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 111594/700000: episode: 253, duration: 1.210s, episode steps: 242, steps per second: 200, episode reward: 236.174, mean reward: 0.976 [-2.768, 100.000], mean action: 1.322 [0.000, 3.000], mean observation: 0.096 [-0.761, 1.017], loss: 23.965893, mean_absolute_error: 45.349480, mean_q: 60.534996\n",
      " 111699/700000: episode: 254, duration: 0.526s, episode steps: 105, steps per second: 200, episode reward: -73.376, mean reward: -0.699 [-100.000, 9.466], mean action: 1.486 [0.000, 3.000], mean observation: 0.026 [-1.209, 1.000], loss: 43.706772, mean_absolute_error: 45.252140, mean_q: 60.406254\n",
      " 111824/700000: episode: 255, duration: 0.612s, episode steps: 125, steps per second: 204, episode reward: -361.334, mean reward: -2.891 [-100.000, 97.707], mean action: 1.192 [0.000, 3.000], mean observation: -0.156 [-3.443, 1.000], loss: 16.565248, mean_absolute_error: 45.091572, mean_q: 60.498219\n",
      " 112120/700000: episode: 256, duration: 1.537s, episode steps: 296, steps per second: 193, episode reward: 153.005, mean reward: 0.517 [-19.130, 100.000], mean action: 1.287 [0.000, 3.000], mean observation: 0.028 [-1.272, 1.000], loss: 10.475961, mean_absolute_error: 44.883923, mean_q: 60.052074\n",
      " 112607/700000: episode: 257, duration: 2.638s, episode steps: 487, steps per second: 185, episode reward: 214.347, mean reward: 0.440 [-19.243, 100.000], mean action: 0.988 [0.000, 3.000], mean observation: 0.167 [-0.716, 1.000], loss: 13.388772, mean_absolute_error: 45.317337, mean_q: 60.264996\n",
      " 112934/700000: episode: 258, duration: 1.686s, episode steps: 327, steps per second: 194, episode reward: 206.728, mean reward: 0.632 [-19.479, 100.000], mean action: 1.183 [0.000, 3.000], mean observation: 0.075 [-1.067, 1.000], loss: 9.404323, mean_absolute_error: 45.470890, mean_q: 60.815243\n",
      " 113258/700000: episode: 259, duration: 1.706s, episode steps: 324, steps per second: 190, episode reward: -54.112, mean reward: -0.167 [-100.000, 22.148], mean action: 1.710 [0.000, 3.000], mean observation: -0.002 [-1.660, 1.000], loss: 13.617902, mean_absolute_error: 45.457455, mean_q: 60.732109\n",
      " 113500/700000: episode: 260, duration: 1.247s, episode steps: 242, steps per second: 194, episode reward: -239.326, mean reward: -0.989 [-100.000, 36.415], mean action: 1.810 [0.000, 3.000], mean observation: 0.042 [-1.135, 1.541], loss: 13.768893, mean_absolute_error: 45.503517, mean_q: 60.702450\n",
      " 113906/700000: episode: 261, duration: 2.197s, episode steps: 406, steps per second: 185, episode reward: 186.311, mean reward: 0.459 [-17.495, 100.000], mean action: 1.517 [0.000, 3.000], mean observation: 0.068 [-0.729, 1.000], loss: 15.273927, mean_absolute_error: 44.986366, mean_q: 60.101551\n",
      " 114167/700000: episode: 262, duration: 1.330s, episode steps: 261, steps per second: 196, episode reward: 214.724, mean reward: 0.823 [-9.227, 100.000], mean action: 1.701 [0.000, 3.000], mean observation: 0.112 [-0.827, 1.000], loss: 8.899165, mean_absolute_error: 45.517174, mean_q: 60.869560\n",
      " 114569/700000: episode: 263, duration: 2.075s, episode steps: 402, steps per second: 194, episode reward: 210.366, mean reward: 0.523 [-17.463, 100.000], mean action: 0.739 [0.000, 3.000], mean observation: 0.161 [-0.862, 1.000], loss: 13.037238, mean_absolute_error: 45.709015, mean_q: 61.182442\n",
      " 114795/700000: episode: 264, duration: 1.262s, episode steps: 226, steps per second: 179, episode reward: 189.747, mean reward: 0.840 [-18.309, 100.000], mean action: 1.133 [0.000, 3.000], mean observation: 0.072 [-0.835, 1.000], loss: 9.173100, mean_absolute_error: 45.890732, mean_q: 61.416046\n",
      " 115438/700000: episode: 265, duration: 4.552s, episode steps: 643, steps per second: 141, episode reward: 193.838, mean reward: 0.301 [-18.196, 100.000], mean action: 1.076 [0.000, 3.000], mean observation: 0.179 [-0.616, 1.000], loss: 11.832234, mean_absolute_error: 46.185101, mean_q: 61.628220\n",
      " 115868/700000: episode: 266, duration: 3.290s, episode steps: 430, steps per second: 131, episode reward: 192.663, mean reward: 0.448 [-17.547, 100.000], mean action: 1.014 [0.000, 3.000], mean observation: 0.126 [-0.714, 1.000], loss: 9.680915, mean_absolute_error: 46.233990, mean_q: 61.692913\n",
      " 116868/700000: episode: 267, duration: 6.920s, episode steps: 1000, steps per second: 145, episode reward: 115.302, mean reward: 0.115 [-19.781, 23.532], mean action: 1.935 [0.000, 3.000], mean observation: 0.216 [-1.336, 1.000], loss: 10.578018, mean_absolute_error: 46.502350, mean_q: 62.111431\n",
      " 117135/700000: episode: 268, duration: 1.379s, episode steps: 267, steps per second: 194, episode reward: 276.085, mean reward: 1.034 [-9.291, 100.000], mean action: 1.131 [0.000, 3.000], mean observation: 0.132 [-0.939, 1.000], loss: 13.590268, mean_absolute_error: 46.453495, mean_q: 62.076149\n",
      " 118135/700000: episode: 269, duration: 5.718s, episode steps: 1000, steps per second: 175, episode reward: 67.782, mean reward: 0.068 [-23.491, 23.374], mean action: 1.473 [0.000, 3.000], mean observation: 0.210 [-1.025, 1.000], loss: 12.151183, mean_absolute_error: 46.922371, mean_q: 62.518032\n",
      " 118605/700000: episode: 270, duration: 2.552s, episode steps: 470, steps per second: 184, episode reward: 217.765, mean reward: 0.463 [-10.586, 100.000], mean action: 1.147 [0.000, 3.000], mean observation: 0.092 [-1.103, 1.000], loss: 11.674793, mean_absolute_error: 47.510586, mean_q: 63.391724\n",
      " 118684/700000: episode: 271, duration: 0.422s, episode steps: 79, steps per second: 187, episode reward: -104.614, mean reward: -1.324 [-100.000, 8.927], mean action: 1.063 [0.000, 3.000], mean observation: 0.074 [-1.029, 3.038], loss: 10.450576, mean_absolute_error: 47.953300, mean_q: 63.756905\n",
      " 118963/700000: episode: 272, duration: 1.430s, episode steps: 279, steps per second: 195, episode reward: 142.039, mean reward: 0.509 [-19.485, 100.000], mean action: 1.125 [0.000, 3.000], mean observation: 0.121 [-0.973, 1.000], loss: 8.078456, mean_absolute_error: 47.648609, mean_q: 63.840363\n",
      " 119132/700000: episode: 273, duration: 0.864s, episode steps: 169, steps per second: 196, episode reward: -44.267, mean reward: -0.262 [-100.000, 9.717], mean action: 1.905 [0.000, 3.000], mean observation: -0.003 [-1.182, 1.000], loss: 17.028738, mean_absolute_error: 47.881199, mean_q: 64.128258\n",
      " 119471/700000: episode: 274, duration: 1.801s, episode steps: 339, steps per second: 188, episode reward: 157.356, mean reward: 0.464 [-10.255, 100.000], mean action: 1.968 [0.000, 3.000], mean observation: 0.024 [-0.689, 1.000], loss: 14.360888, mean_absolute_error: 48.502228, mean_q: 64.681778\n",
      " 119888/700000: episode: 275, duration: 2.224s, episode steps: 417, steps per second: 188, episode reward: 159.127, mean reward: 0.382 [-21.650, 100.000], mean action: 1.657 [0.000, 3.000], mean observation: 0.066 [-1.388, 1.000], loss: 11.181647, mean_absolute_error: 47.798748, mean_q: 63.785053\n",
      " 120240/700000: episode: 276, duration: 1.811s, episode steps: 352, steps per second: 194, episode reward: 243.463, mean reward: 0.692 [-18.327, 100.000], mean action: 1.236 [0.000, 3.000], mean observation: 0.149 [-0.695, 1.586], loss: 12.375180, mean_absolute_error: 48.044216, mean_q: 64.089287\n",
      " 120682/700000: episode: 277, duration: 2.273s, episode steps: 442, steps per second: 194, episode reward: 200.212, mean reward: 0.453 [-22.577, 100.000], mean action: 0.792 [0.000, 3.000], mean observation: 0.180 [-0.808, 1.000], loss: 12.425319, mean_absolute_error: 48.218243, mean_q: 64.419182\n",
      " 121029/700000: episode: 278, duration: 1.827s, episode steps: 347, steps per second: 190, episode reward: 211.775, mean reward: 0.610 [-10.112, 100.000], mean action: 0.942 [0.000, 3.000], mean observation: 0.143 [-0.828, 1.000], loss: 15.874254, mean_absolute_error: 48.329525, mean_q: 64.614944\n",
      " 121298/700000: episode: 279, duration: 1.397s, episode steps: 269, steps per second: 193, episode reward: 234.914, mean reward: 0.873 [-17.467, 100.000], mean action: 1.022 [0.000, 3.000], mean observation: 0.130 [-0.828, 1.000], loss: 12.464317, mean_absolute_error: 47.741821, mean_q: 63.652431\n",
      " 121574/700000: episode: 280, duration: 1.393s, episode steps: 276, steps per second: 198, episode reward: 245.533, mean reward: 0.890 [-9.278, 100.000], mean action: 1.243 [0.000, 3.000], mean observation: 0.087 [-0.711, 1.009], loss: 13.276352, mean_absolute_error: 48.036102, mean_q: 64.361900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 121807/700000: episode: 281, duration: 1.188s, episode steps: 233, steps per second: 196, episode reward: 252.348, mean reward: 1.083 [-10.780, 100.000], mean action: 1.275 [0.000, 3.000], mean observation: 0.108 [-1.265, 1.000], loss: 8.885400, mean_absolute_error: 47.866833, mean_q: 63.960720\n",
      " 122477/700000: episode: 282, duration: 3.770s, episode steps: 670, steps per second: 178, episode reward: 187.886, mean reward: 0.280 [-17.955, 100.000], mean action: 1.169 [0.000, 3.000], mean observation: 0.126 [-0.711, 1.011], loss: 13.037686, mean_absolute_error: 47.850037, mean_q: 63.809292\n",
      " 122817/700000: episode: 283, duration: 1.778s, episode steps: 340, steps per second: 191, episode reward: 215.473, mean reward: 0.634 [-17.761, 100.000], mean action: 1.059 [0.000, 3.000], mean observation: 0.137 [-0.708, 1.000], loss: 13.747090, mean_absolute_error: 47.836979, mean_q: 63.997162\n",
      " 123108/700000: episode: 284, duration: 1.472s, episode steps: 291, steps per second: 198, episode reward: 236.492, mean reward: 0.813 [-18.487, 100.000], mean action: 1.265 [0.000, 3.000], mean observation: 0.125 [-0.805, 1.000], loss: 8.424420, mean_absolute_error: 47.374569, mean_q: 63.445110\n",
      " 123854/700000: episode: 285, duration: 4.079s, episode steps: 746, steps per second: 183, episode reward: 164.027, mean reward: 0.220 [-20.199, 100.000], mean action: 1.130 [0.000, 3.000], mean observation: 0.107 [-1.250, 1.000], loss: 15.110166, mean_absolute_error: 47.502468, mean_q: 63.581066\n",
      " 124104/700000: episode: 286, duration: 1.307s, episode steps: 250, steps per second: 191, episode reward: 136.833, mean reward: 0.547 [-13.992, 100.000], mean action: 1.628 [0.000, 3.000], mean observation: 0.035 [-1.382, 1.000], loss: 13.113403, mean_absolute_error: 47.128365, mean_q: 62.930828\n",
      " 124392/700000: episode: 287, duration: 1.496s, episode steps: 288, steps per second: 192, episode reward: 203.456, mean reward: 0.706 [-10.315, 100.000], mean action: 1.625 [0.000, 3.000], mean observation: 0.055 [-1.100, 1.000], loss: 13.115014, mean_absolute_error: 47.589390, mean_q: 63.518661\n",
      " 125105/700000: episode: 288, duration: 3.959s, episode steps: 713, steps per second: 180, episode reward: 138.367, mean reward: 0.194 [-18.779, 100.000], mean action: 1.278 [0.000, 3.000], mean observation: 0.097 [-0.810, 1.000], loss: 9.070601, mean_absolute_error: 47.395790, mean_q: 63.368408\n",
      " 125417/700000: episode: 289, duration: 1.597s, episode steps: 312, steps per second: 195, episode reward: 230.690, mean reward: 0.739 [-10.691, 100.000], mean action: 1.083 [0.000, 3.000], mean observation: 0.108 [-0.957, 1.013], loss: 14.432988, mean_absolute_error: 47.354588, mean_q: 63.199200\n",
      " 125556/700000: episode: 290, duration: 0.695s, episode steps: 139, steps per second: 200, episode reward: -42.395, mean reward: -0.305 [-100.000, 15.469], mean action: 1.799 [0.000, 3.000], mean observation: -0.018 [-1.477, 1.000], loss: 16.772556, mean_absolute_error: 47.391785, mean_q: 63.439762\n",
      " 125723/700000: episode: 291, duration: 0.847s, episode steps: 167, steps per second: 197, episode reward: -48.507, mean reward: -0.290 [-100.000, 11.816], mean action: 1.922 [0.000, 3.000], mean observation: 0.015 [-0.585, 1.130], loss: 9.047386, mean_absolute_error: 47.377911, mean_q: 63.304455\n",
      " 126431/700000: episode: 292, duration: 3.813s, episode steps: 708, steps per second: 186, episode reward: 126.442, mean reward: 0.179 [-19.807, 100.000], mean action: 1.100 [0.000, 3.000], mean observation: 0.048 [-0.681, 1.000], loss: 11.260337, mean_absolute_error: 47.423775, mean_q: 63.435482\n",
      " 127189/700000: episode: 293, duration: 4.449s, episode steps: 758, steps per second: 170, episode reward: 134.065, mean reward: 0.177 [-18.818, 100.000], mean action: 2.528 [0.000, 3.000], mean observation: 0.187 [-0.572, 1.000], loss: 11.058062, mean_absolute_error: 47.589783, mean_q: 63.545773\n",
      " 127338/700000: episode: 294, duration: 0.762s, episode steps: 149, steps per second: 195, episode reward: -48.173, mean reward: -0.323 [-100.000, 11.314], mean action: 1.826 [0.000, 3.000], mean observation: 0.050 [-1.089, 1.000], loss: 21.847706, mean_absolute_error: 47.479321, mean_q: 63.651131\n",
      " 127599/700000: episode: 295, duration: 1.326s, episode steps: 261, steps per second: 197, episode reward: 219.686, mean reward: 0.842 [-9.252, 100.000], mean action: 1.149 [0.000, 3.000], mean observation: 0.139 [-0.756, 1.000], loss: 13.542398, mean_absolute_error: 47.541965, mean_q: 63.612309\n",
      " 127740/700000: episode: 296, duration: 0.789s, episode steps: 141, steps per second: 179, episode reward: -17.232, mean reward: -0.122 [-100.000, 20.265], mean action: 1.794 [0.000, 3.000], mean observation: 0.032 [-0.739, 1.775], loss: 12.518536, mean_absolute_error: 47.452808, mean_q: 63.235817\n",
      " 128270/700000: episode: 297, duration: 3.154s, episode steps: 530, steps per second: 168, episode reward: 118.472, mean reward: 0.224 [-10.982, 100.000], mean action: 1.702 [0.000, 3.000], mean observation: 0.014 [-0.726, 1.000], loss: 11.407033, mean_absolute_error: 47.752522, mean_q: 63.827625\n",
      " 128755/700000: episode: 298, duration: 2.914s, episode steps: 485, steps per second: 166, episode reward: 194.011, mean reward: 0.400 [-18.715, 100.000], mean action: 1.095 [0.000, 3.000], mean observation: 0.082 [-0.766, 1.000], loss: 9.859262, mean_absolute_error: 47.691158, mean_q: 63.839443\n",
      " 129038/700000: episode: 299, duration: 1.605s, episode steps: 283, steps per second: 176, episode reward: 166.219, mean reward: 0.587 [-19.921, 100.000], mean action: 1.435 [0.000, 3.000], mean observation: 0.059 [-0.581, 1.000], loss: 9.349141, mean_absolute_error: 48.247253, mean_q: 64.416336\n",
      " 129403/700000: episode: 300, duration: 1.960s, episode steps: 365, steps per second: 186, episode reward: 186.291, mean reward: 0.510 [-17.825, 100.000], mean action: 1.501 [0.000, 3.000], mean observation: 0.071 [-0.842, 1.000], loss: 8.251800, mean_absolute_error: 47.706177, mean_q: 63.549423\n",
      " 129795/700000: episode: 301, duration: 2.135s, episode steps: 392, steps per second: 184, episode reward: 177.750, mean reward: 0.453 [-7.075, 100.000], mean action: 1.607 [0.000, 3.000], mean observation: 0.033 [-1.092, 1.011], loss: 12.213103, mean_absolute_error: 47.731739, mean_q: 63.633446\n",
      " 130779/700000: episode: 302, duration: 5.563s, episode steps: 984, steps per second: 177, episode reward: -361.627, mean reward: -0.368 [-100.000, 23.491], mean action: 1.814 [0.000, 3.000], mean observation: 0.073 [-1.097, 1.560], loss: 11.692966, mean_absolute_error: 47.894863, mean_q: 64.085075\n",
      " 131048/700000: episode: 303, duration: 1.385s, episode steps: 269, steps per second: 194, episode reward: 214.356, mean reward: 0.797 [-17.637, 100.000], mean action: 1.141 [0.000, 3.000], mean observation: 0.101 [-0.924, 1.000], loss: 9.561595, mean_absolute_error: 48.021149, mean_q: 64.336639\n",
      " 131726/700000: episode: 304, duration: 3.809s, episode steps: 678, steps per second: 178, episode reward: 194.601, mean reward: 0.287 [-20.096, 100.000], mean action: 1.811 [0.000, 3.000], mean observation: 0.190 [-0.688, 1.000], loss: 9.910015, mean_absolute_error: 48.091766, mean_q: 64.203621\n",
      " 132067/700000: episode: 305, duration: 1.816s, episode steps: 341, steps per second: 188, episode reward: 214.818, mean reward: 0.630 [-9.035, 100.000], mean action: 1.452 [0.000, 3.000], mean observation: 0.035 [-1.112, 1.000], loss: 11.090307, mean_absolute_error: 48.362598, mean_q: 64.718094\n",
      " 132356/700000: episode: 306, duration: 1.505s, episode steps: 289, steps per second: 192, episode reward: 200.795, mean reward: 0.695 [-19.187, 100.000], mean action: 1.547 [0.000, 3.000], mean observation: 0.076 [-0.713, 1.000], loss: 10.012070, mean_absolute_error: 48.011326, mean_q: 64.058426\n",
      " 132775/700000: episode: 307, duration: 2.254s, episode steps: 419, steps per second: 186, episode reward: 156.342, mean reward: 0.373 [-10.637, 100.000], mean action: 1.496 [0.000, 3.000], mean observation: 0.096 [-0.589, 1.000], loss: 9.775686, mean_absolute_error: 48.011761, mean_q: 64.092163\n",
      " 132988/700000: episode: 308, duration: 1.086s, episode steps: 213, steps per second: 196, episode reward: 202.596, mean reward: 0.951 [-13.238, 100.000], mean action: 1.254 [0.000, 3.000], mean observation: 0.101 [-1.018, 1.000], loss: 9.576942, mean_absolute_error: 47.952034, mean_q: 63.870800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 133169/700000: episode: 309, duration: 0.910s, episode steps: 181, steps per second: 199, episode reward: -33.987, mean reward: -0.188 [-100.000, 17.041], mean action: 1.674 [0.000, 3.000], mean observation: 0.060 [-0.973, 1.000], loss: 14.069118, mean_absolute_error: 48.080952, mean_q: 63.753162\n",
      " 133588/700000: episode: 310, duration: 2.233s, episode steps: 419, steps per second: 188, episode reward: -463.565, mean reward: -1.106 [-100.000, 5.086], mean action: 1.699 [0.000, 3.000], mean observation: 0.020 [-1.321, 1.505], loss: 13.661745, mean_absolute_error: 47.809322, mean_q: 63.600368\n",
      " 133828/700000: episode: 311, duration: 1.219s, episode steps: 240, steps per second: 197, episode reward: 220.440, mean reward: 0.918 [-7.966, 100.000], mean action: 1.225 [0.000, 3.000], mean observation: 0.100 [-0.908, 1.000], loss: 11.370530, mean_absolute_error: 47.576214, mean_q: 63.517529\n",
      " 134321/700000: episode: 312, duration: 2.751s, episode steps: 493, steps per second: 179, episode reward: 204.005, mean reward: 0.414 [-11.359, 100.000], mean action: 1.172 [0.000, 3.000], mean observation: 0.124 [-1.391, 1.000], loss: 10.364429, mean_absolute_error: 47.814823, mean_q: 63.655060\n",
      " 134767/700000: episode: 313, duration: 2.301s, episode steps: 446, steps per second: 194, episode reward: 242.784, mean reward: 0.544 [-17.903, 100.000], mean action: 0.783 [0.000, 3.000], mean observation: 0.168 [-0.624, 1.283], loss: 16.916725, mean_absolute_error: 47.614498, mean_q: 63.366753\n",
      " 135381/700000: episode: 314, duration: 3.382s, episode steps: 614, steps per second: 182, episode reward: 186.297, mean reward: 0.303 [-12.517, 100.000], mean action: 1.461 [0.000, 3.000], mean observation: 0.128 [-1.019, 1.000], loss: 9.887083, mean_absolute_error: 47.533867, mean_q: 63.309204\n",
      " 135550/700000: episode: 315, duration: 0.858s, episode steps: 169, steps per second: 197, episode reward: -255.357, mean reward: -1.511 [-100.000, 72.642], mean action: 1.787 [0.000, 3.000], mean observation: -0.002 [-2.489, 1.076], loss: 9.295151, mean_absolute_error: 47.646477, mean_q: 63.660282\n",
      " 135816/700000: episode: 316, duration: 1.376s, episode steps: 266, steps per second: 193, episode reward: 235.143, mean reward: 0.884 [-2.455, 100.000], mean action: 1.207 [0.000, 3.000], mean observation: 0.106 [-0.810, 1.000], loss: 12.272549, mean_absolute_error: 47.475636, mean_q: 63.518364\n",
      " 136135/700000: episode: 317, duration: 1.664s, episode steps: 319, steps per second: 192, episode reward: 184.852, mean reward: 0.579 [-18.992, 100.000], mean action: 1.295 [0.000, 3.000], mean observation: 0.074 [-0.651, 1.000], loss: 6.922748, mean_absolute_error: 47.803654, mean_q: 63.917076\n",
      " 136406/700000: episode: 318, duration: 1.410s, episode steps: 271, steps per second: 192, episode reward: 223.148, mean reward: 0.823 [-10.378, 100.000], mean action: 1.435 [0.000, 3.000], mean observation: 0.142 [-0.862, 1.451], loss: 10.848594, mean_absolute_error: 47.950523, mean_q: 63.839706\n",
      " 136984/700000: episode: 319, duration: 3.260s, episode steps: 578, steps per second: 177, episode reward: 199.046, mean reward: 0.344 [-18.704, 100.000], mean action: 1.197 [0.000, 3.000], mean observation: 0.087 [-0.443, 1.000], loss: 14.220120, mean_absolute_error: 47.973057, mean_q: 63.684013\n",
      " 137335/700000: episode: 320, duration: 1.885s, episode steps: 351, steps per second: 186, episode reward: 223.782, mean reward: 0.638 [-18.738, 100.000], mean action: 1.630 [0.000, 3.000], mean observation: 0.144 [-0.952, 1.000], loss: 6.800210, mean_absolute_error: 47.682381, mean_q: 63.442184\n",
      " 138217/700000: episode: 321, duration: 5.386s, episode steps: 882, steps per second: 164, episode reward: 167.988, mean reward: 0.190 [-20.381, 100.000], mean action: 1.049 [0.000, 3.000], mean observation: 0.109 [-1.276, 1.000], loss: 12.307629, mean_absolute_error: 48.008087, mean_q: 63.979073\n",
      " 138579/700000: episode: 322, duration: 3.202s, episode steps: 362, steps per second: 113, episode reward: 203.410, mean reward: 0.562 [-19.016, 100.000], mean action: 1.564 [0.000, 3.000], mean observation: 0.183 [-0.777, 1.000], loss: 12.679850, mean_absolute_error: 48.106937, mean_q: 63.909931\n",
      " 139271/700000: episode: 323, duration: 4.717s, episode steps: 692, steps per second: 147, episode reward: 101.533, mean reward: 0.147 [-21.754, 100.000], mean action: 1.366 [0.000, 3.000], mean observation: 0.067 [-0.830, 1.000], loss: 15.796217, mean_absolute_error: 48.287289, mean_q: 64.583527\n",
      " 139572/700000: episode: 324, duration: 1.986s, episode steps: 301, steps per second: 152, episode reward: 229.162, mean reward: 0.761 [-11.701, 100.000], mean action: 1.365 [0.000, 3.000], mean observation: 0.047 [-0.840, 1.317], loss: 29.134928, mean_absolute_error: 48.897919, mean_q: 65.641769\n",
      " 139850/700000: episode: 325, duration: 1.871s, episode steps: 278, steps per second: 149, episode reward: 216.403, mean reward: 0.778 [-8.458, 100.000], mean action: 1.129 [0.000, 3.000], mean observation: 0.103 [-0.820, 1.000], loss: 24.165096, mean_absolute_error: 48.128387, mean_q: 64.577637\n",
      " 139930/700000: episode: 326, duration: 0.495s, episode steps: 80, steps per second: 161, episode reward: -64.095, mean reward: -0.801 [-100.000, 10.184], mean action: 1.262 [0.000, 3.000], mean observation: -0.101 [-1.853, 1.000], loss: 11.424935, mean_absolute_error: 49.282936, mean_q: 66.373146\n",
      " 140037/700000: episode: 327, duration: 0.660s, episode steps: 107, steps per second: 162, episode reward: -27.855, mean reward: -0.260 [-100.000, 19.355], mean action: 1.336 [0.000, 3.000], mean observation: -0.007 [-1.049, 1.204], loss: 11.439772, mean_absolute_error: 49.432312, mean_q: 66.462830\n",
      " 140388/700000: episode: 328, duration: 2.220s, episode steps: 351, steps per second: 158, episode reward: 163.023, mean reward: 0.464 [-24.852, 100.000], mean action: 1.652 [0.000, 3.000], mean observation: 0.039 [-1.060, 1.000], loss: 10.874372, mean_absolute_error: 48.913189, mean_q: 65.651550\n",
      " 140810/700000: episode: 329, duration: 2.754s, episode steps: 422, steps per second: 153, episode reward: 215.205, mean reward: 0.510 [-18.694, 100.000], mean action: 0.886 [0.000, 3.000], mean observation: 0.105 [-0.935, 1.013], loss: 34.880531, mean_absolute_error: 49.146233, mean_q: 65.978088\n",
      " 140937/700000: episode: 330, duration: 0.713s, episode steps: 127, steps per second: 178, episode reward: -48.053, mean reward: -0.378 [-100.000, 9.545], mean action: 1.488 [0.000, 3.000], mean observation: 0.019 [-1.232, 1.000], loss: 12.920898, mean_absolute_error: 49.053059, mean_q: 65.511795\n",
      " 141355/700000: episode: 331, duration: 2.792s, episode steps: 418, steps per second: 150, episode reward: 151.557, mean reward: 0.363 [-14.503, 100.000], mean action: 1.813 [0.000, 3.000], mean observation: 0.082 [-0.671, 1.000], loss: 10.312984, mean_absolute_error: 49.592087, mean_q: 66.593605\n",
      " 141496/700000: episode: 332, duration: 0.878s, episode steps: 141, steps per second: 161, episode reward: -13.117, mean reward: -0.093 [-100.000, 19.198], mean action: 1.617 [0.000, 3.000], mean observation: 0.088 [-0.971, 1.113], loss: 14.459637, mean_absolute_error: 50.389477, mean_q: 67.542885\n",
      " 142445/700000: episode: 333, duration: 6.502s, episode steps: 949, steps per second: 146, episode reward: 174.478, mean reward: 0.184 [-23.244, 100.000], mean action: 1.846 [0.000, 3.000], mean observation: 0.247 [-1.019, 1.000], loss: 12.168427, mean_absolute_error: 50.537312, mean_q: 67.604507\n",
      " 142532/700000: episode: 334, duration: 0.616s, episode steps: 87, steps per second: 141, episode reward: -36.358, mean reward: -0.418 [-100.000, 17.439], mean action: 1.862 [0.000, 3.000], mean observation: -0.031 [-1.284, 1.000], loss: 27.298416, mean_absolute_error: 51.205318, mean_q: 68.696404\n",
      " 142812/700000: episode: 335, duration: 2.132s, episode steps: 280, steps per second: 131, episode reward: 243.912, mean reward: 0.871 [-10.537, 100.000], mean action: 1.171 [0.000, 3.000], mean observation: 0.111 [-0.728, 1.106], loss: 16.952709, mean_absolute_error: 51.107338, mean_q: 68.145309\n",
      " 143053/700000: episode: 336, duration: 1.641s, episode steps: 241, steps per second: 147, episode reward: -317.841, mean reward: -1.319 [-100.000, 3.336], mean action: 1.656 [0.000, 3.000], mean observation: 0.030 [-0.931, 2.267], loss: 7.574257, mean_absolute_error: 50.517658, mean_q: 67.636986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 143237/700000: episode: 337, duration: 1.789s, episode steps: 184, steps per second: 103, episode reward: 222.141, mean reward: 1.207 [-17.658, 100.000], mean action: 1.337 [0.000, 3.000], mean observation: 0.069 [-1.115, 1.000], loss: 10.408929, mean_absolute_error: 50.818478, mean_q: 68.141083\n",
      " 143372/700000: episode: 338, duration: 0.860s, episode steps: 135, steps per second: 157, episode reward: -22.194, mean reward: -0.164 [-100.000, 17.785], mean action: 1.489 [0.000, 3.000], mean observation: 0.079 [-0.953, 1.997], loss: 5.544368, mean_absolute_error: 50.830837, mean_q: 67.930672\n",
      " 143594/700000: episode: 339, duration: 1.524s, episode steps: 222, steps per second: 146, episode reward: -24.819, mean reward: -0.112 [-100.000, 11.910], mean action: 1.721 [0.000, 3.000], mean observation: 0.037 [-0.949, 1.180], loss: 17.057508, mean_absolute_error: 50.299702, mean_q: 67.235558\n",
      " 143777/700000: episode: 340, duration: 1.327s, episode steps: 183, steps per second: 138, episode reward: 183.200, mean reward: 1.001 [-9.242, 100.000], mean action: 2.180 [0.000, 3.000], mean observation: 0.141 [-0.961, 1.000], loss: 17.264027, mean_absolute_error: 50.839520, mean_q: 67.853226\n",
      " 143930/700000: episode: 341, duration: 0.789s, episode steps: 153, steps per second: 194, episode reward: -50.172, mean reward: -0.328 [-100.000, 12.986], mean action: 1.503 [0.000, 3.000], mean observation: -0.098 [-0.738, 1.000], loss: 5.828592, mean_absolute_error: 50.999119, mean_q: 68.163383\n",
      " 144329/700000: episode: 342, duration: 2.208s, episode steps: 399, steps per second: 181, episode reward: 210.489, mean reward: 0.528 [-18.645, 100.000], mean action: 1.266 [0.000, 3.000], mean observation: 0.190 [-0.810, 1.000], loss: 11.711258, mean_absolute_error: 50.800251, mean_q: 67.769226\n",
      " 144893/700000: episode: 343, duration: 3.053s, episode steps: 564, steps per second: 185, episode reward: 210.705, mean reward: 0.374 [-19.007, 100.000], mean action: 1.082 [0.000, 3.000], mean observation: 0.135 [-0.767, 1.002], loss: 12.826071, mean_absolute_error: 50.593300, mean_q: 67.530602\n",
      " 145154/700000: episode: 344, duration: 1.392s, episode steps: 261, steps per second: 187, episode reward: 180.510, mean reward: 0.692 [-18.721, 100.000], mean action: 1.674 [0.000, 3.000], mean observation: 0.048 [-0.670, 1.000], loss: 11.920887, mean_absolute_error: 50.961140, mean_q: 67.954559\n",
      " 146154/700000: episode: 345, duration: 6.773s, episode steps: 1000, steps per second: 148, episode reward: 75.752, mean reward: 0.076 [-21.539, 21.520], mean action: 1.434 [0.000, 3.000], mean observation: 0.226 [-0.674, 1.000], loss: 11.474162, mean_absolute_error: 50.681591, mean_q: 67.449982\n",
      " 146270/700000: episode: 346, duration: 0.661s, episode steps: 116, steps per second: 176, episode reward: -81.273, mean reward: -0.701 [-100.000, 7.206], mean action: 1.802 [0.000, 3.000], mean observation: -0.022 [-1.388, 1.000], loss: 5.500060, mean_absolute_error: 50.732521, mean_q: 67.307053\n",
      " 146573/700000: episode: 347, duration: 1.924s, episode steps: 303, steps per second: 157, episode reward: 196.627, mean reward: 0.649 [-20.236, 100.000], mean action: 0.878 [0.000, 3.000], mean observation: 0.125 [-0.897, 1.000], loss: 6.490350, mean_absolute_error: 50.485981, mean_q: 67.523666\n",
      " 146820/700000: episode: 348, duration: 1.507s, episode steps: 247, steps per second: 164, episode reward: -278.774, mean reward: -1.129 [-100.000, 13.964], mean action: 1.964 [0.000, 3.000], mean observation: 0.082 [-3.683, 1.110], loss: 18.163189, mean_absolute_error: 50.751755, mean_q: 67.499275\n",
      " 147040/700000: episode: 349, duration: 1.243s, episode steps: 220, steps per second: 177, episode reward: 167.765, mean reward: 0.763 [-13.629, 100.000], mean action: 1.336 [0.000, 3.000], mean observation: 0.045 [-1.315, 1.000], loss: 12.419104, mean_absolute_error: 49.754143, mean_q: 66.308533\n",
      " 147300/700000: episode: 350, duration: 1.526s, episode steps: 260, steps per second: 170, episode reward: 209.790, mean reward: 0.807 [-19.786, 100.000], mean action: 1.162 [0.000, 3.000], mean observation: 0.056 [-0.637, 1.000], loss: 16.024460, mean_absolute_error: 50.187958, mean_q: 66.991989\n",
      " 147413/700000: episode: 351, duration: 0.655s, episode steps: 113, steps per second: 173, episode reward: -87.996, mean reward: -0.779 [-100.000, 11.772], mean action: 1.770 [0.000, 3.000], mean observation: -0.202 [-1.145, 1.569], loss: 9.030479, mean_absolute_error: 50.011589, mean_q: 66.767090\n",
      " 147681/700000: episode: 352, duration: 1.539s, episode steps: 268, steps per second: 174, episode reward: 195.006, mean reward: 0.728 [-4.383, 100.000], mean action: 1.489 [0.000, 3.000], mean observation: 0.053 [-0.584, 1.000], loss: 11.904552, mean_absolute_error: 49.834866, mean_q: 66.117683\n",
      " 148328/700000: episode: 353, duration: 4.200s, episode steps: 647, steps per second: 154, episode reward: 205.621, mean reward: 0.318 [-19.660, 100.000], mean action: 1.386 [0.000, 3.000], mean observation: 0.204 [-0.757, 1.000], loss: 7.567828, mean_absolute_error: 50.035797, mean_q: 66.642609\n",
      " 148996/700000: episode: 354, duration: 4.915s, episode steps: 668, steps per second: 136, episode reward: 145.889, mean reward: 0.218 [-19.427, 100.000], mean action: 1.051 [0.000, 3.000], mean observation: 0.108 [-0.934, 1.000], loss: 15.193042, mean_absolute_error: 50.006245, mean_q: 66.789185\n",
      " 149541/700000: episode: 355, duration: 3.955s, episode steps: 545, steps per second: 138, episode reward: -291.114, mean reward: -0.534 [-100.000, 21.922], mean action: 1.341 [0.000, 3.000], mean observation: 0.169 [-1.243, 1.490], loss: 14.432630, mean_absolute_error: 49.526642, mean_q: 66.088257\n",
      " 149944/700000: episode: 356, duration: 2.468s, episode steps: 403, steps per second: 163, episode reward: 220.277, mean reward: 0.547 [-19.613, 100.000], mean action: 1.117 [0.000, 3.000], mean observation: 0.199 [-1.013, 1.212], loss: 17.770517, mean_absolute_error: 49.747498, mean_q: 66.289749\n",
      " 150268/700000: episode: 357, duration: 2.034s, episode steps: 324, steps per second: 159, episode reward: 230.058, mean reward: 0.710 [-18.578, 100.000], mean action: 1.219 [0.000, 3.000], mean observation: 0.104 [-0.750, 1.007], loss: 8.943583, mean_absolute_error: 49.807056, mean_q: 66.537552\n",
      " 151268/700000: episode: 358, duration: 6.828s, episode steps: 1000, steps per second: 146, episode reward: 226.115, mean reward: 0.226 [-20.434, 100.000], mean action: 0.683 [0.000, 3.000], mean observation: 0.183 [-0.633, 1.000], loss: 11.197721, mean_absolute_error: 49.640797, mean_q: 66.172493\n",
      " 151390/700000: episode: 359, duration: 0.674s, episode steps: 122, steps per second: 181, episode reward: -32.113, mean reward: -0.263 [-100.000, 18.939], mean action: 1.393 [0.000, 3.000], mean observation: 0.114 [-1.231, 1.000], loss: 5.906426, mean_absolute_error: 49.766956, mean_q: 66.343773\n",
      " 151622/700000: episode: 360, duration: 1.373s, episode steps: 232, steps per second: 169, episode reward: 215.955, mean reward: 0.931 [-7.757, 100.000], mean action: 1.526 [0.000, 3.000], mean observation: 0.041 [-0.548, 1.000], loss: 12.122853, mean_absolute_error: 49.952194, mean_q: 66.540543\n",
      " 152190/700000: episode: 361, duration: 3.658s, episode steps: 568, steps per second: 155, episode reward: 184.663, mean reward: 0.325 [-18.276, 100.000], mean action: 1.331 [0.000, 3.000], mean observation: 0.059 [-0.764, 1.053], loss: 12.942736, mean_absolute_error: 49.749794, mean_q: 66.452148\n",
      " 152859/700000: episode: 362, duration: 4.334s, episode steps: 669, steps per second: 154, episode reward: 189.539, mean reward: 0.283 [-22.995, 100.000], mean action: 1.410 [0.000, 3.000], mean observation: 0.215 [-0.931, 1.000], loss: 11.050011, mean_absolute_error: 49.364555, mean_q: 65.894646\n",
      " 153374/700000: episode: 363, duration: 3.188s, episode steps: 515, steps per second: 162, episode reward: 193.083, mean reward: 0.375 [-17.860, 100.000], mean action: 0.998 [0.000, 3.000], mean observation: 0.186 [-0.837, 1.000], loss: 10.725286, mean_absolute_error: 48.976223, mean_q: 65.543816\n",
      " 153948/700000: episode: 364, duration: 3.617s, episode steps: 574, steps per second: 159, episode reward: 191.495, mean reward: 0.334 [-10.995, 100.000], mean action: 0.834 [0.000, 3.000], mean observation: 0.136 [-1.259, 1.000], loss: 13.096383, mean_absolute_error: 49.052593, mean_q: 65.463676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 154141/700000: episode: 365, duration: 1.050s, episode steps: 193, steps per second: 184, episode reward: 212.302, mean reward: 1.100 [-4.405, 100.000], mean action: 1.161 [0.000, 3.000], mean observation: 0.027 [-0.890, 1.000], loss: 17.944920, mean_absolute_error: 48.974701, mean_q: 65.398781\n",
      " 154264/700000: episode: 366, duration: 0.915s, episode steps: 123, steps per second: 134, episode reward: -15.645, mean reward: -0.127 [-100.000, 16.470], mean action: 1.699 [0.000, 3.000], mean observation: 0.035 [-0.926, 1.000], loss: 27.124872, mean_absolute_error: 48.943577, mean_q: 65.232361\n",
      " 154369/700000: episode: 367, duration: 0.525s, episode steps: 105, steps per second: 200, episode reward: -2.367, mean reward: -0.023 [-100.000, 22.428], mean action: 1.733 [0.000, 3.000], mean observation: 0.025 [-0.853, 1.428], loss: 8.112839, mean_absolute_error: 49.219025, mean_q: 65.837509\n",
      " 154824/700000: episode: 368, duration: 2.523s, episode steps: 455, steps per second: 180, episode reward: -222.967, mean reward: -0.490 [-100.000, 17.164], mean action: 1.501 [0.000, 3.000], mean observation: 0.165 [-0.984, 1.006], loss: 11.199764, mean_absolute_error: 49.176941, mean_q: 65.492081\n",
      " 155156/700000: episode: 369, duration: 2.005s, episode steps: 332, steps per second: 166, episode reward: 206.794, mean reward: 0.623 [-5.798, 100.000], mean action: 1.494 [0.000, 3.000], mean observation: 0.065 [-1.331, 1.000], loss: 10.559903, mean_absolute_error: 49.610706, mean_q: 66.242645\n",
      " 156156/700000: episode: 370, duration: 7.259s, episode steps: 1000, steps per second: 138, episode reward: -94.564, mean reward: -0.095 [-24.097, 21.012], mean action: 1.470 [0.000, 3.000], mean observation: 0.146 [-0.945, 1.000], loss: 11.069898, mean_absolute_error: 49.755760, mean_q: 66.418861\n",
      " 156899/700000: episode: 371, duration: 4.548s, episode steps: 743, steps per second: 163, episode reward: 247.840, mean reward: 0.334 [-17.553, 100.000], mean action: 0.647 [0.000, 3.000], mean observation: 0.205 [-0.833, 1.278], loss: 10.929238, mean_absolute_error: 49.987404, mean_q: 66.700119\n",
      " 157010/700000: episode: 372, duration: 0.655s, episode steps: 111, steps per second: 170, episode reward: -112.903, mean reward: -1.017 [-100.000, 9.546], mean action: 1.757 [0.000, 3.000], mean observation: -0.087 [-0.904, 2.019], loss: 16.126740, mean_absolute_error: 50.563114, mean_q: 67.389236\n",
      " 157222/700000: episode: 373, duration: 1.383s, episode steps: 212, steps per second: 153, episode reward: 216.377, mean reward: 1.021 [-2.977, 100.000], mean action: 1.528 [0.000, 3.000], mean observation: 0.036 [-0.762, 1.000], loss: 8.757575, mean_absolute_error: 49.849915, mean_q: 66.316811\n",
      " 157399/700000: episode: 374, duration: 1.007s, episode steps: 177, steps per second: 176, episode reward: 236.234, mean reward: 1.335 [-3.391, 100.000], mean action: 1.311 [0.000, 3.000], mean observation: 0.093 [-0.886, 1.003], loss: 12.016072, mean_absolute_error: 50.569851, mean_q: 67.586777\n",
      " 157668/700000: episode: 375, duration: 1.553s, episode steps: 269, steps per second: 173, episode reward: 202.637, mean reward: 0.753 [-18.209, 100.000], mean action: 1.219 [0.000, 3.000], mean observation: 0.123 [-0.885, 1.000], loss: 10.639011, mean_absolute_error: 49.899693, mean_q: 66.550301\n",
      " 157880/700000: episode: 376, duration: 1.264s, episode steps: 212, steps per second: 168, episode reward: 202.639, mean reward: 0.956 [-3.929, 100.000], mean action: 1.344 [0.000, 3.000], mean observation: 0.083 [-0.911, 1.000], loss: 9.808531, mean_absolute_error: 50.720219, mean_q: 67.586739\n",
      " 158177/700000: episode: 377, duration: 1.686s, episode steps: 297, steps per second: 176, episode reward: 184.816, mean reward: 0.622 [-11.274, 100.000], mean action: 1.020 [0.000, 3.000], mean observation: 0.172 [-1.485, 1.000], loss: 11.065143, mean_absolute_error: 50.645821, mean_q: 67.606995\n",
      " 158275/700000: episode: 378, duration: 0.548s, episode steps: 98, steps per second: 179, episode reward: -233.273, mean reward: -2.380 [-100.000, 3.223], mean action: 1.929 [0.000, 3.000], mean observation: -0.139 [-1.502, 0.924], loss: 10.665646, mean_absolute_error: 50.916222, mean_q: 68.030518\n",
      " 158456/700000: episode: 379, duration: 0.941s, episode steps: 181, steps per second: 192, episode reward: 223.180, mean reward: 1.233 [-2.571, 100.000], mean action: 1.166 [0.000, 3.000], mean observation: 0.079 [-0.852, 1.000], loss: 9.352495, mean_absolute_error: 50.188362, mean_q: 67.134758\n",
      " 158662/700000: episode: 380, duration: 1.135s, episode steps: 206, steps per second: 182, episode reward: 162.024, mean reward: 0.787 [-13.750, 100.000], mean action: 1.646 [0.000, 3.000], mean observation: 0.036 [-0.810, 1.000], loss: 12.765966, mean_absolute_error: 50.577358, mean_q: 67.394188\n",
      " 159396/700000: episode: 381, duration: 4.987s, episode steps: 734, steps per second: 147, episode reward: 228.468, mean reward: 0.311 [-21.574, 100.000], mean action: 1.414 [0.000, 3.000], mean observation: 0.180 [-0.865, 1.223], loss: 10.737753, mean_absolute_error: 50.446011, mean_q: 67.295525\n",
      " 159550/700000: episode: 382, duration: 0.871s, episode steps: 154, steps per second: 177, episode reward: -155.352, mean reward: -1.009 [-100.000, 12.726], mean action: 1.630 [0.000, 3.000], mean observation: -0.122 [-0.965, 1.073], loss: 12.779730, mean_absolute_error: 50.118587, mean_q: 66.761086\n",
      " 159719/700000: episode: 383, duration: 0.944s, episode steps: 169, steps per second: 179, episode reward: 226.408, mean reward: 1.340 [-2.768, 100.000], mean action: 1.361 [0.000, 3.000], mean observation: 0.051 [-0.802, 1.000], loss: 14.306958, mean_absolute_error: 50.059559, mean_q: 66.713112\n",
      " 160010/700000: episode: 384, duration: 1.624s, episode steps: 291, steps per second: 179, episode reward: -285.142, mean reward: -0.980 [-100.000, 25.116], mean action: 1.433 [0.000, 3.000], mean observation: -0.019 [-1.280, 1.743], loss: 14.907310, mean_absolute_error: 50.256359, mean_q: 66.938103\n",
      " 160486/700000: episode: 385, duration: 2.866s, episode steps: 476, steps per second: 166, episode reward: 217.873, mean reward: 0.458 [-19.113, 100.000], mean action: 0.887 [0.000, 3.000], mean observation: 0.176 [-0.940, 1.000], loss: 11.201142, mean_absolute_error: 50.072422, mean_q: 66.771385\n",
      " 160631/700000: episode: 386, duration: 0.894s, episode steps: 145, steps per second: 162, episode reward: -28.781, mean reward: -0.198 [-100.000, 16.915], mean action: 1.669 [0.000, 3.000], mean observation: 0.067 [-0.927, 1.423], loss: 12.506348, mean_absolute_error: 50.370575, mean_q: 67.255791\n",
      " 160907/700000: episode: 387, duration: 1.754s, episode steps: 276, steps per second: 157, episode reward: 221.902, mean reward: 0.804 [-17.960, 100.000], mean action: 1.011 [0.000, 3.000], mean observation: 0.129 [-0.810, 1.000], loss: 15.540573, mean_absolute_error: 50.012337, mean_q: 66.904045\n",
      " 161153/700000: episode: 388, duration: 1.804s, episode steps: 246, steps per second: 136, episode reward: 211.140, mean reward: 0.858 [-17.409, 100.000], mean action: 0.992 [0.000, 3.000], mean observation: 0.153 [-0.852, 1.188], loss: 13.589553, mean_absolute_error: 50.248840, mean_q: 67.079369\n",
      " 161927/700000: episode: 389, duration: 5.105s, episode steps: 774, steps per second: 152, episode reward: 170.664, mean reward: 0.220 [-19.854, 100.000], mean action: 1.053 [0.000, 3.000], mean observation: 0.184 [-0.967, 1.000], loss: 12.657845, mean_absolute_error: 50.274769, mean_q: 66.933350\n",
      " 162261/700000: episode: 390, duration: 2.073s, episode steps: 334, steps per second: 161, episode reward: 203.928, mean reward: 0.611 [-12.750, 100.000], mean action: 2.051 [0.000, 3.000], mean observation: 0.095 [-0.752, 1.000], loss: 9.565601, mean_absolute_error: 50.288712, mean_q: 66.804085\n",
      " 162561/700000: episode: 391, duration: 1.837s, episode steps: 300, steps per second: 163, episode reward: -86.063, mean reward: -0.287 [-100.000, 10.883], mean action: 1.723 [0.000, 3.000], mean observation: -0.028 [-1.089, 1.000], loss: 10.513347, mean_absolute_error: 49.368759, mean_q: 65.786667\n",
      " 162801/700000: episode: 392, duration: 1.364s, episode steps: 240, steps per second: 176, episode reward: -405.669, mean reward: -1.690 [-100.000, 3.060], mean action: 1.400 [0.000, 3.000], mean observation: -0.051 [-1.727, 2.306], loss: 17.386852, mean_absolute_error: 49.455093, mean_q: 65.694359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 162970/700000: episode: 393, duration: 1.073s, episode steps: 169, steps per second: 158, episode reward: 10.891, mean reward: 0.064 [-100.000, 16.766], mean action: 1.710 [0.000, 3.000], mean observation: 0.077 [-0.681, 1.411], loss: 13.298999, mean_absolute_error: 49.650223, mean_q: 66.116196\n",
      " 163203/700000: episode: 394, duration: 1.716s, episode steps: 233, steps per second: 136, episode reward: -405.242, mean reward: -1.739 [-100.000, 3.466], mean action: 1.378 [0.000, 3.000], mean observation: -0.063 [-1.511, 1.931], loss: 7.146236, mean_absolute_error: 49.297901, mean_q: 65.900536\n",
      " 163600/700000: episode: 395, duration: 2.473s, episode steps: 397, steps per second: 161, episode reward: 202.835, mean reward: 0.511 [-17.625, 100.000], mean action: 1.005 [0.000, 3.000], mean observation: 0.182 [-0.902, 1.000], loss: 8.736560, mean_absolute_error: 49.541973, mean_q: 66.057556\n",
      " 163719/700000: episode: 396, duration: 0.681s, episode steps: 119, steps per second: 175, episode reward: -198.939, mean reward: -1.672 [-100.000, 3.038], mean action: 1.647 [0.000, 3.000], mean observation: -0.113 [-1.010, 0.941], loss: 11.267647, mean_absolute_error: 49.875698, mean_q: 66.287743\n",
      " 164100/700000: episode: 397, duration: 2.544s, episode steps: 381, steps per second: 150, episode reward: 241.313, mean reward: 0.633 [-21.069, 100.000], mean action: 1.060 [0.000, 3.000], mean observation: 0.153 [-0.889, 1.000], loss: 10.551256, mean_absolute_error: 49.541744, mean_q: 65.696709\n",
      " 164759/700000: episode: 398, duration: 4.824s, episode steps: 659, steps per second: 137, episode reward: 202.798, mean reward: 0.308 [-20.782, 100.000], mean action: 1.209 [0.000, 3.000], mean observation: 0.108 [-1.463, 1.015], loss: 10.735630, mean_absolute_error: 49.461105, mean_q: 65.820503\n",
      " 165305/700000: episode: 399, duration: 3.279s, episode steps: 546, steps per second: 167, episode reward: 226.238, mean reward: 0.414 [-18.590, 100.000], mean action: 0.842 [0.000, 3.000], mean observation: 0.157 [-0.795, 1.001], loss: 11.877117, mean_absolute_error: 49.084209, mean_q: 65.298737\n",
      " 165624/700000: episode: 400, duration: 1.796s, episode steps: 319, steps per second: 178, episode reward: 163.800, mean reward: 0.513 [-17.431, 100.000], mean action: 1.665 [0.000, 3.000], mean observation: 0.149 [-0.938, 1.179], loss: 7.643530, mean_absolute_error: 48.863201, mean_q: 65.166962\n",
      " 166047/700000: episode: 401, duration: 2.289s, episode steps: 423, steps per second: 185, episode reward: 219.179, mean reward: 0.518 [-20.548, 100.000], mean action: 1.314 [0.000, 3.000], mean observation: 0.114 [-1.526, 1.000], loss: 11.939585, mean_absolute_error: 49.432449, mean_q: 65.711060\n",
      " 166263/700000: episode: 402, duration: 1.351s, episode steps: 216, steps per second: 160, episode reward: 184.642, mean reward: 0.855 [-8.834, 100.000], mean action: 1.245 [0.000, 3.000], mean observation: 0.037 [-0.867, 1.000], loss: 8.028722, mean_absolute_error: 49.214325, mean_q: 65.252144\n",
      " 166371/700000: episode: 403, duration: 0.564s, episode steps: 108, steps per second: 191, episode reward: -241.416, mean reward: -2.235 [-100.000, 2.053], mean action: 1.981 [0.000, 3.000], mean observation: -0.049 [-1.184, 1.065], loss: 11.756535, mean_absolute_error: 49.224014, mean_q: 65.066933\n",
      " 166610/700000: episode: 404, duration: 1.282s, episode steps: 239, steps per second: 186, episode reward: 221.383, mean reward: 0.926 [-2.828, 100.000], mean action: 1.297 [0.000, 3.000], mean observation: 0.106 [-0.623, 1.000], loss: 9.993376, mean_absolute_error: 49.289677, mean_q: 65.272911\n",
      " 166878/700000: episode: 405, duration: 1.879s, episode steps: 268, steps per second: 143, episode reward: 235.373, mean reward: 0.878 [-19.311, 100.000], mean action: 1.507 [0.000, 3.000], mean observation: 0.110 [-0.900, 1.252], loss: 16.192642, mean_absolute_error: 49.159222, mean_q: 65.039536\n",
      " 167266/700000: episode: 406, duration: 2.222s, episode steps: 388, steps per second: 175, episode reward: 172.857, mean reward: 0.446 [-13.768, 100.000], mean action: 1.139 [0.000, 3.000], mean observation: 0.094 [-0.708, 1.000], loss: 11.922442, mean_absolute_error: 49.028400, mean_q: 65.213562\n",
      " 168266/700000: episode: 407, duration: 7.378s, episode steps: 1000, steps per second: 136, episode reward: 42.765, mean reward: 0.043 [-19.983, 21.094], mean action: 1.979 [0.000, 3.000], mean observation: 0.146 [-0.930, 1.000], loss: 11.118519, mean_absolute_error: 48.562584, mean_q: 64.420120\n",
      " 168746/700000: episode: 408, duration: 2.789s, episode steps: 480, steps per second: 172, episode reward: 106.182, mean reward: 0.221 [-10.018, 100.000], mean action: 1.454 [0.000, 3.000], mean observation: 0.007 [-0.968, 1.000], loss: 10.556414, mean_absolute_error: 48.464314, mean_q: 64.166550\n",
      " 169227/700000: episode: 409, duration: 2.688s, episode steps: 481, steps per second: 179, episode reward: 191.967, mean reward: 0.399 [-18.769, 100.000], mean action: 1.455 [0.000, 3.000], mean observation: 0.164 [-0.959, 1.000], loss: 11.136372, mean_absolute_error: 48.612717, mean_q: 64.616577\n",
      " 169736/700000: episode: 410, duration: 2.866s, episode steps: 509, steps per second: 178, episode reward: 195.947, mean reward: 0.385 [-18.979, 100.000], mean action: 0.762 [0.000, 3.000], mean observation: 0.175 [-0.896, 1.000], loss: 9.969646, mean_absolute_error: 48.271862, mean_q: 64.280235\n",
      " 170140/700000: episode: 411, duration: 2.132s, episode steps: 404, steps per second: 189, episode reward: 237.508, mean reward: 0.588 [-18.168, 100.000], mean action: 0.775 [0.000, 3.000], mean observation: 0.170 [-0.964, 1.491], loss: 14.659389, mean_absolute_error: 48.532082, mean_q: 64.631142\n",
      " 170951/700000: episode: 412, duration: 4.862s, episode steps: 811, steps per second: 167, episode reward: 155.874, mean reward: 0.192 [-19.549, 100.000], mean action: 1.058 [0.000, 3.000], mean observation: 0.153 [-0.736, 1.000], loss: 11.520548, mean_absolute_error: 48.002434, mean_q: 63.831768\n",
      " 171295/700000: episode: 413, duration: 1.868s, episode steps: 344, steps per second: 184, episode reward: -277.734, mean reward: -0.807 [-100.000, 44.367], mean action: 1.500 [0.000, 3.000], mean observation: 0.019 [-0.943, 2.030], loss: 10.869992, mean_absolute_error: 48.175526, mean_q: 63.978645\n",
      " 171467/700000: episode: 414, duration: 0.932s, episode steps: 172, steps per second: 185, episode reward: -144.950, mean reward: -0.843 [-100.000, 19.643], mean action: 1.494 [0.000, 3.000], mean observation: -0.118 [-1.123, 1.000], loss: 15.144349, mean_absolute_error: 48.157467, mean_q: 64.232346\n",
      " 171626/700000: episode: 415, duration: 0.919s, episode steps: 159, steps per second: 173, episode reward: -177.167, mean reward: -1.114 [-100.000, 4.389], mean action: 1.654 [0.000, 3.000], mean observation: -0.124 [-1.006, 0.942], loss: 8.816085, mean_absolute_error: 48.371178, mean_q: 64.600380\n",
      " 172068/700000: episode: 416, duration: 2.554s, episode steps: 442, steps per second: 173, episode reward: -199.286, mean reward: -0.451 [-100.000, 23.913], mean action: 1.337 [0.000, 3.000], mean observation: 0.160 [-0.944, 1.661], loss: 21.095566, mean_absolute_error: 47.941090, mean_q: 63.797844\n",
      " 172267/700000: episode: 417, duration: 1.084s, episode steps: 199, steps per second: 184, episode reward: -161.712, mean reward: -0.813 [-100.000, 19.955], mean action: 1.588 [0.000, 3.000], mean observation: 0.012 [-0.763, 1.836], loss: 13.819164, mean_absolute_error: 48.289711, mean_q: 64.270798\n",
      " 172622/700000: episode: 418, duration: 2.047s, episode steps: 355, steps per second: 173, episode reward: 230.452, mean reward: 0.649 [-11.250, 100.000], mean action: 1.558 [0.000, 3.000], mean observation: 0.099 [-0.924, 1.000], loss: 11.130686, mean_absolute_error: 47.934589, mean_q: 63.536938\n",
      " 172959/700000: episode: 419, duration: 2.118s, episode steps: 337, steps per second: 159, episode reward: -38.206, mean reward: -0.113 [-100.000, 19.097], mean action: 1.760 [0.000, 3.000], mean observation: -0.063 [-0.886, 1.516], loss: 14.579679, mean_absolute_error: 47.817459, mean_q: 63.695747\n",
      " 173339/700000: episode: 420, duration: 2.253s, episode steps: 380, steps per second: 169, episode reward: 162.591, mean reward: 0.428 [-11.133, 100.000], mean action: 1.342 [0.000, 3.000], mean observation: 0.140 [-0.910, 1.000], loss: 8.847880, mean_absolute_error: 48.007648, mean_q: 64.008583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 173479/700000: episode: 421, duration: 0.749s, episode steps: 140, steps per second: 187, episode reward: -125.791, mean reward: -0.899 [-100.000, 10.829], mean action: 1.357 [0.000, 3.000], mean observation: -0.090 [-0.911, 1.352], loss: 8.958635, mean_absolute_error: 48.073704, mean_q: 64.405609\n",
      " 173741/700000: episode: 422, duration: 1.343s, episode steps: 262, steps per second: 195, episode reward: 218.335, mean reward: 0.833 [-18.801, 100.000], mean action: 1.179 [0.000, 3.000], mean observation: 0.096 [-0.887, 1.000], loss: 7.690593, mean_absolute_error: 48.049931, mean_q: 64.153229\n",
      " 174098/700000: episode: 423, duration: 2.196s, episode steps: 357, steps per second: 163, episode reward: 186.072, mean reward: 0.521 [-17.531, 100.000], mean action: 1.333 [0.000, 3.000], mean observation: 0.181 [-0.870, 1.000], loss: 10.326203, mean_absolute_error: 48.626938, mean_q: 64.676949\n",
      " 174333/700000: episode: 424, duration: 1.170s, episode steps: 235, steps per second: 201, episode reward: 203.338, mean reward: 0.865 [-7.462, 100.000], mean action: 0.974 [0.000, 3.000], mean observation: 0.111 [-0.797, 1.000], loss: 7.863366, mean_absolute_error: 48.217319, mean_q: 64.075607\n",
      " 174838/700000: episode: 425, duration: 2.692s, episode steps: 505, steps per second: 188, episode reward: 204.208, mean reward: 0.404 [-23.469, 100.000], mean action: 1.509 [0.000, 3.000], mean observation: 0.043 [-1.092, 1.000], loss: 15.864683, mean_absolute_error: 48.674332, mean_q: 65.074043\n",
      " 175032/700000: episode: 426, duration: 1.040s, episode steps: 194, steps per second: 187, episode reward: -212.939, mean reward: -1.098 [-100.000, 12.502], mean action: 1.670 [0.000, 3.000], mean observation: 0.101 [-0.739, 1.607], loss: 13.951211, mean_absolute_error: 48.857677, mean_q: 64.966293\n",
      " 175363/700000: episode: 427, duration: 2.053s, episode steps: 331, steps per second: 161, episode reward: 179.225, mean reward: 0.541 [-18.239, 100.000], mean action: 1.773 [0.000, 3.000], mean observation: -0.013 [-0.796, 1.169], loss: 12.589083, mean_absolute_error: 48.658863, mean_q: 64.845932\n",
      " 175548/700000: episode: 428, duration: 0.934s, episode steps: 185, steps per second: 198, episode reward: -282.133, mean reward: -1.525 [-100.000, 8.350], mean action: 1.768 [0.000, 3.000], mean observation: 0.138 [-0.754, 2.110], loss: 12.312160, mean_absolute_error: 48.184807, mean_q: 63.867752\n",
      " 175756/700000: episode: 429, duration: 1.067s, episode steps: 208, steps per second: 195, episode reward: 209.927, mean reward: 1.009 [-13.279, 100.000], mean action: 1.317 [0.000, 3.000], mean observation: 0.127 [-0.813, 1.000], loss: 7.759542, mean_absolute_error: 48.712551, mean_q: 64.931816\n",
      " 175969/700000: episode: 430, duration: 1.065s, episode steps: 213, steps per second: 200, episode reward: 209.619, mean reward: 0.984 [-8.363, 100.000], mean action: 1.258 [0.000, 3.000], mean observation: 0.081 [-1.000, 1.000], loss: 7.226775, mean_absolute_error: 48.257996, mean_q: 64.085678\n",
      " 176079/700000: episode: 431, duration: 0.546s, episode steps: 110, steps per second: 201, episode reward: -64.737, mean reward: -0.589 [-100.000, 12.971], mean action: 1.745 [0.000, 3.000], mean observation: 0.068 [-0.804, 1.000], loss: 8.087186, mean_absolute_error: 48.593876, mean_q: 64.708382\n",
      " 176972/700000: episode: 432, duration: 5.421s, episode steps: 893, steps per second: 165, episode reward: 142.535, mean reward: 0.160 [-19.179, 100.000], mean action: 1.384 [0.000, 3.000], mean observation: 0.157 [-1.237, 1.194], loss: 11.184251, mean_absolute_error: 48.182644, mean_q: 63.947239\n",
      " 177145/700000: episode: 433, duration: 0.871s, episode steps: 173, steps per second: 199, episode reward: -182.300, mean reward: -1.054 [-100.000, 29.938], mean action: 1.694 [0.000, 3.000], mean observation: 0.156 [-0.906, 1.629], loss: 8.703227, mean_absolute_error: 47.913483, mean_q: 63.829876\n",
      " 177379/700000: episode: 434, duration: 1.178s, episode steps: 234, steps per second: 199, episode reward: -278.382, mean reward: -1.190 [-100.000, 11.611], mean action: 1.496 [0.000, 3.000], mean observation: 0.034 [-1.909, 1.000], loss: 9.210636, mean_absolute_error: 47.962456, mean_q: 63.565323\n",
      " 177783/700000: episode: 435, duration: 2.111s, episode steps: 404, steps per second: 191, episode reward: 208.085, mean reward: 0.515 [-19.346, 100.000], mean action: 0.901 [0.000, 3.000], mean observation: 0.180 [-0.759, 1.000], loss: 13.731588, mean_absolute_error: 47.541393, mean_q: 63.095341\n",
      " 178020/700000: episode: 436, duration: 1.181s, episode steps: 237, steps per second: 201, episode reward: -193.972, mean reward: -0.818 [-100.000, 27.987], mean action: 1.451 [0.000, 3.000], mean observation: 0.132 [-0.792, 1.596], loss: 11.013938, mean_absolute_error: 48.190174, mean_q: 63.636929\n",
      " 178204/700000: episode: 437, duration: 0.923s, episode steps: 184, steps per second: 199, episode reward: -239.354, mean reward: -1.301 [-100.000, 9.250], mean action: 1.739 [0.000, 3.000], mean observation: 0.096 [-0.696, 2.404], loss: 11.071968, mean_absolute_error: 47.677364, mean_q: 63.196873\n",
      " 178876/700000: episode: 438, duration: 3.883s, episode steps: 672, steps per second: 173, episode reward: 133.857, mean reward: 0.199 [-23.878, 100.000], mean action: 1.994 [0.000, 3.000], mean observation: 0.112 [-0.912, 1.000], loss: 13.383326, mean_absolute_error: 48.141029, mean_q: 63.792603\n",
      " 179291/700000: episode: 439, duration: 2.149s, episode steps: 415, steps per second: 193, episode reward: 208.116, mean reward: 0.501 [-17.809, 100.000], mean action: 0.957 [0.000, 3.000], mean observation: 0.113 [-0.679, 1.000], loss: 8.857499, mean_absolute_error: 47.845707, mean_q: 63.237743\n",
      " 179950/700000: episode: 440, duration: 3.608s, episode steps: 659, steps per second: 183, episode reward: 151.564, mean reward: 0.230 [-22.633, 100.000], mean action: 1.287 [0.000, 3.000], mean observation: 0.205 [-0.960, 1.000], loss: 17.263285, mean_absolute_error: 47.955223, mean_q: 63.448975\n",
      " 180064/700000: episode: 441, duration: 0.563s, episode steps: 114, steps per second: 203, episode reward: -196.632, mean reward: -1.725 [-100.000, 1.826], mean action: 1.561 [0.000, 3.000], mean observation: 0.166 [-0.728, 1.162], loss: 8.835861, mean_absolute_error: 47.843079, mean_q: 63.114281\n",
      " 180368/700000: episode: 442, duration: 1.573s, episode steps: 304, steps per second: 193, episode reward: 195.517, mean reward: 0.643 [-18.265, 100.000], mean action: 1.280 [0.000, 3.000], mean observation: 0.135 [-0.797, 1.000], loss: 10.697186, mean_absolute_error: 47.405754, mean_q: 62.839394\n",
      " 180927/700000: episode: 443, duration: 3.094s, episode steps: 559, steps per second: 181, episode reward: 198.756, mean reward: 0.356 [-19.505, 100.000], mean action: 1.147 [0.000, 3.000], mean observation: 0.127 [-0.638, 1.031], loss: 10.386248, mean_absolute_error: 47.387878, mean_q: 62.708855\n",
      " 181702/700000: episode: 444, duration: 4.323s, episode steps: 775, steps per second: 179, episode reward: 199.914, mean reward: 0.258 [-19.581, 100.000], mean action: 0.968 [0.000, 3.000], mean observation: 0.174 [-1.754, 1.026], loss: 12.917203, mean_absolute_error: 47.304253, mean_q: 62.823689\n",
      " 182120/700000: episode: 445, duration: 2.205s, episode steps: 418, steps per second: 190, episode reward: 239.801, mean reward: 0.574 [-14.763, 100.000], mean action: 1.438 [0.000, 3.000], mean observation: 0.035 [-1.288, 1.000], loss: 10.160888, mean_absolute_error: 47.208134, mean_q: 62.582600\n",
      " 182352/700000: episode: 446, duration: 1.169s, episode steps: 232, steps per second: 198, episode reward: 189.406, mean reward: 0.816 [-9.362, 100.000], mean action: 1.358 [0.000, 3.000], mean observation: 0.069 [-0.765, 1.000], loss: 9.102987, mean_absolute_error: 46.764408, mean_q: 61.904598\n",
      " 182570/700000: episode: 447, duration: 1.167s, episode steps: 218, steps per second: 187, episode reward: 235.880, mean reward: 1.082 [-18.234, 100.000], mean action: 1.005 [0.000, 3.000], mean observation: 0.102 [-1.373, 1.000], loss: 10.946126, mean_absolute_error: 46.965015, mean_q: 62.559040\n",
      " 182806/700000: episode: 448, duration: 1.209s, episode steps: 236, steps per second: 195, episode reward: 225.642, mean reward: 0.956 [-17.576, 100.000], mean action: 1.004 [0.000, 3.000], mean observation: 0.099 [-0.725, 1.000], loss: 8.244151, mean_absolute_error: 47.004913, mean_q: 62.458553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 183615/700000: episode: 449, duration: 4.623s, episode steps: 809, steps per second: 175, episode reward: 169.281, mean reward: 0.209 [-22.492, 100.000], mean action: 1.252 [0.000, 3.000], mean observation: 0.190 [-0.790, 1.000], loss: 10.737769, mean_absolute_error: 47.056946, mean_q: 62.534271\n",
      " 183915/700000: episode: 450, duration: 1.543s, episode steps: 300, steps per second: 194, episode reward: 231.175, mean reward: 0.771 [-20.280, 100.000], mean action: 1.300 [0.000, 3.000], mean observation: 0.128 [-0.885, 1.000], loss: 10.827659, mean_absolute_error: 47.208538, mean_q: 62.905468\n",
      " 184301/700000: episode: 451, duration: 2.040s, episode steps: 386, steps per second: 189, episode reward: 174.585, mean reward: 0.452 [-17.228, 100.000], mean action: 1.360 [0.000, 3.000], mean observation: 0.148 [-0.835, 1.000], loss: 13.205199, mean_absolute_error: 47.214069, mean_q: 62.953091\n",
      " 184586/700000: episode: 452, duration: 1.481s, episode steps: 285, steps per second: 192, episode reward: 244.422, mean reward: 0.858 [-2.512, 100.000], mean action: 1.204 [0.000, 3.000], mean observation: 0.134 [-0.885, 1.000], loss: 7.410809, mean_absolute_error: 47.333202, mean_q: 63.313049\n",
      " 184733/700000: episode: 453, duration: 0.728s, episode steps: 147, steps per second: 202, episode reward: 1.137, mean reward: 0.008 [-100.000, 17.303], mean action: 1.687 [0.000, 3.000], mean observation: 0.000 [-1.722, 1.000], loss: 17.706135, mean_absolute_error: 47.210793, mean_q: 62.963448\n",
      " 184990/700000: episode: 454, duration: 1.314s, episode steps: 257, steps per second: 196, episode reward: 203.831, mean reward: 0.793 [-2.933, 100.000], mean action: 1.288 [0.000, 3.000], mean observation: 0.081 [-0.673, 1.000], loss: 14.302269, mean_absolute_error: 47.362312, mean_q: 63.205231\n",
      " 185140/700000: episode: 455, duration: 0.745s, episode steps: 150, steps per second: 201, episode reward: 227.227, mean reward: 1.515 [-12.630, 100.000], mean action: 1.500 [0.000, 3.000], mean observation: 0.036 [-0.985, 1.000], loss: 15.534166, mean_absolute_error: 47.304333, mean_q: 62.897839\n",
      " 185268/700000: episode: 456, duration: 0.643s, episode steps: 128, steps per second: 199, episode reward: -145.279, mean reward: -1.135 [-100.000, 2.829], mean action: 1.805 [0.000, 3.000], mean observation: -0.109 [-1.003, 0.935], loss: 8.988986, mean_absolute_error: 47.301208, mean_q: 62.869251\n",
      " 185484/700000: episode: 457, duration: 1.071s, episode steps: 216, steps per second: 202, episode reward: 237.365, mean reward: 1.099 [-2.878, 100.000], mean action: 0.968 [0.000, 3.000], mean observation: 0.116 [-1.383, 1.000], loss: 6.967620, mean_absolute_error: 47.443920, mean_q: 63.016167\n",
      " 185861/700000: episode: 458, duration: 2.013s, episode steps: 377, steps per second: 187, episode reward: -272.594, mean reward: -0.723 [-100.000, 5.063], mean action: 1.899 [0.000, 3.000], mean observation: -0.039 [-1.492, 0.927], loss: 16.205730, mean_absolute_error: 47.482365, mean_q: 62.858898\n",
      " 185958/700000: episode: 459, duration: 0.486s, episode steps: 97, steps per second: 200, episode reward: -226.943, mean reward: -2.340 [-100.000, 2.230], mean action: 1.361 [0.000, 3.000], mean observation: -0.101 [-1.267, 0.937], loss: 7.351568, mean_absolute_error: 47.144028, mean_q: 62.609852\n",
      " 186255/700000: episode: 460, duration: 1.516s, episode steps: 297, steps per second: 196, episode reward: 191.794, mean reward: 0.646 [-3.956, 100.000], mean action: 1.178 [0.000, 3.000], mean observation: 0.066 [-0.733, 1.000], loss: 9.347200, mean_absolute_error: 47.517879, mean_q: 62.766361\n",
      " 186413/700000: episode: 461, duration: 0.786s, episode steps: 158, steps per second: 201, episode reward: -100.820, mean reward: -0.638 [-100.000, 3.453], mean action: 1.576 [0.000, 3.000], mean observation: -0.107 [-1.002, 0.940], loss: 8.887980, mean_absolute_error: 47.281944, mean_q: 62.703075\n",
      " 186577/700000: episode: 462, duration: 0.819s, episode steps: 164, steps per second: 200, episode reward: 9.956, mean reward: 0.061 [-100.000, 17.216], mean action: 1.787 [0.000, 3.000], mean observation: 0.076 [-0.759, 1.000], loss: 11.445525, mean_absolute_error: 47.786949, mean_q: 63.033905\n",
      " 187577/700000: episode: 463, duration: 5.737s, episode steps: 1000, steps per second: 174, episode reward: 14.591, mean reward: 0.015 [-24.185, 24.084], mean action: 2.264 [0.000, 3.000], mean observation: 0.254 [-0.923, 1.000], loss: 9.022489, mean_absolute_error: 47.481777, mean_q: 62.982956\n",
      " 187832/700000: episode: 464, duration: 1.274s, episode steps: 255, steps per second: 200, episode reward: 243.083, mean reward: 0.953 [-9.429, 100.000], mean action: 1.341 [0.000, 3.000], mean observation: 0.101 [-0.962, 1.000], loss: 10.652666, mean_absolute_error: 47.755619, mean_q: 63.032490\n",
      " 188229/700000: episode: 465, duration: 2.087s, episode steps: 397, steps per second: 190, episode reward: 167.331, mean reward: 0.421 [-17.917, 100.000], mean action: 1.773 [0.000, 3.000], mean observation: 0.261 [-0.919, 1.000], loss: 7.554113, mean_absolute_error: 48.037495, mean_q: 63.629498\n",
      " 188522/700000: episode: 466, duration: 1.621s, episode steps: 293, steps per second: 181, episode reward: 213.896, mean reward: 0.730 [-9.073, 100.000], mean action: 1.468 [0.000, 3.000], mean observation: 0.074 [-0.896, 1.000], loss: 13.384174, mean_absolute_error: 47.988747, mean_q: 63.566113\n",
      " 188769/700000: episode: 467, duration: 1.287s, episode steps: 247, steps per second: 192, episode reward: 209.285, mean reward: 0.847 [-19.950, 100.000], mean action: 1.146 [0.000, 3.000], mean observation: 0.101 [-0.852, 1.000], loss: 10.757411, mean_absolute_error: 48.264343, mean_q: 63.888142\n",
      " 189471/700000: episode: 468, duration: 4.563s, episode steps: 702, steps per second: 154, episode reward: 159.940, mean reward: 0.228 [-22.475, 100.000], mean action: 1.376 [0.000, 3.000], mean observation: 0.166 [-1.345, 1.000], loss: 10.605074, mean_absolute_error: 48.089367, mean_q: 63.740208\n",
      " 189877/700000: episode: 469, duration: 2.088s, episode steps: 406, steps per second: 194, episode reward: 220.055, mean reward: 0.542 [-18.553, 100.000], mean action: 0.956 [0.000, 3.000], mean observation: 0.155 [-0.857, 1.000], loss: 10.037507, mean_absolute_error: 48.391430, mean_q: 64.480103\n",
      " 190877/700000: episode: 470, duration: 5.506s, episode steps: 1000, steps per second: 182, episode reward: 26.220, mean reward: 0.026 [-21.626, 21.059], mean action: 1.509 [0.000, 3.000], mean observation: 0.168 [-0.581, 1.000], loss: 12.737473, mean_absolute_error: 48.864838, mean_q: 64.684502\n",
      " 191127/700000: episode: 471, duration: 1.262s, episode steps: 250, steps per second: 198, episode reward: 214.747, mean reward: 0.859 [-9.861, 100.000], mean action: 1.456 [0.000, 3.000], mean observation: 0.070 [-0.797, 1.000], loss: 7.333817, mean_absolute_error: 48.573780, mean_q: 64.475632\n",
      " 191239/700000: episode: 472, duration: 0.554s, episode steps: 112, steps per second: 202, episode reward: -138.464, mean reward: -1.236 [-100.000, 6.155], mean action: 1.795 [0.000, 3.000], mean observation: 0.022 [-1.095, 3.847], loss: 3.954406, mean_absolute_error: 48.088291, mean_q: 63.930550\n",
      " 191602/700000: episode: 473, duration: 1.872s, episode steps: 363, steps per second: 194, episode reward: 263.201, mean reward: 0.725 [-18.399, 100.000], mean action: 1.253 [0.000, 3.000], mean observation: 0.151 [-0.886, 1.000], loss: 9.830785, mean_absolute_error: 48.407143, mean_q: 64.361649\n",
      " 192121/700000: episode: 474, duration: 2.839s, episode steps: 519, steps per second: 183, episode reward: 181.892, mean reward: 0.350 [-8.914, 100.000], mean action: 1.576 [0.000, 3.000], mean observation: 0.056 [-0.975, 1.481], loss: 8.417705, mean_absolute_error: 48.402390, mean_q: 64.182144\n",
      " 192214/700000: episode: 475, duration: 0.457s, episode steps: 93, steps per second: 204, episode reward: -289.060, mean reward: -3.108 [-100.000, 3.281], mean action: 1.634 [0.000, 3.000], mean observation: -0.143 [-1.948, 1.000], loss: 21.002735, mean_absolute_error: 48.612770, mean_q: 64.268967\n",
      " 193134/700000: episode: 476, duration: 4.917s, episode steps: 920, steps per second: 187, episode reward: 124.266, mean reward: 0.135 [-22.078, 100.000], mean action: 1.848 [0.000, 3.000], mean observation: 0.184 [-0.959, 1.000], loss: 10.850032, mean_absolute_error: 48.096722, mean_q: 63.704502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 193435/700000: episode: 477, duration: 1.559s, episode steps: 301, steps per second: 193, episode reward: 206.395, mean reward: 0.686 [-17.375, 100.000], mean action: 1.123 [0.000, 3.000], mean observation: 0.135 [-0.663, 1.084], loss: 12.704515, mean_absolute_error: 47.624107, mean_q: 63.086533\n",
      " 193852/700000: episode: 478, duration: 2.259s, episode steps: 417, steps per second: 185, episode reward: 191.944, mean reward: 0.460 [-17.438, 100.000], mean action: 1.295 [0.000, 3.000], mean observation: 0.089 [-1.474, 1.000], loss: 10.155291, mean_absolute_error: 47.617786, mean_q: 63.114388\n",
      " 194104/700000: episode: 479, duration: 1.266s, episode steps: 252, steps per second: 199, episode reward: 192.307, mean reward: 0.763 [-19.402, 100.000], mean action: 0.964 [0.000, 3.000], mean observation: 0.079 [-0.809, 1.000], loss: 16.099916, mean_absolute_error: 47.470428, mean_q: 62.468037\n",
      " 194238/700000: episode: 480, duration: 0.667s, episode steps: 134, steps per second: 201, episode reward: -35.990, mean reward: -0.269 [-100.000, 14.431], mean action: 1.791 [0.000, 3.000], mean observation: 0.019 [-0.899, 1.000], loss: 8.772993, mean_absolute_error: 48.036629, mean_q: 63.429016\n",
      " 194501/700000: episode: 481, duration: 1.343s, episode steps: 263, steps per second: 196, episode reward: 0.129, mean reward: 0.000 [-100.000, 17.878], mean action: 1.768 [0.000, 3.000], mean observation: 0.051 [-0.662, 1.000], loss: 22.510941, mean_absolute_error: 47.610859, mean_q: 62.872475\n",
      " 194648/700000: episode: 482, duration: 0.726s, episode steps: 147, steps per second: 202, episode reward: -4.649, mean reward: -0.032 [-100.000, 16.612], mean action: 1.551 [0.000, 3.000], mean observation: 0.024 [-0.804, 1.000], loss: 12.950732, mean_absolute_error: 47.635872, mean_q: 62.801445\n",
      " 195475/700000: episode: 483, duration: 4.528s, episode steps: 827, steps per second: 183, episode reward: -207.861, mean reward: -0.251 [-100.000, 20.438], mean action: 1.437 [0.000, 3.000], mean observation: 0.065 [-1.000, 1.214], loss: 8.492974, mean_absolute_error: 47.382225, mean_q: 62.597946\n",
      " 195600/700000: episode: 484, duration: 0.611s, episode steps: 125, steps per second: 204, episode reward: -150.089, mean reward: -1.201 [-100.000, 7.114], mean action: 1.128 [0.000, 3.000], mean observation: 0.182 [-0.822, 2.710], loss: 7.191733, mean_absolute_error: 46.988441, mean_q: 62.145477\n",
      " 195867/700000: episode: 485, duration: 1.349s, episode steps: 267, steps per second: 198, episode reward: 206.282, mean reward: 0.773 [-9.672, 100.000], mean action: 1.599 [0.000, 3.000], mean observation: 0.058 [-1.178, 1.000], loss: 17.062811, mean_absolute_error: 46.862492, mean_q: 62.050861\n",
      " 196146/700000: episode: 486, duration: 1.425s, episode steps: 279, steps per second: 196, episode reward: 219.007, mean reward: 0.785 [-17.508, 100.000], mean action: 1.222 [0.000, 3.000], mean observation: 0.097 [-0.832, 1.000], loss: 13.907772, mean_absolute_error: 46.969048, mean_q: 61.797070\n",
      " 196438/700000: episode: 487, duration: 1.488s, episode steps: 292, steps per second: 196, episode reward: 234.534, mean reward: 0.803 [-2.894, 100.000], mean action: 1.171 [0.000, 3.000], mean observation: 0.117 [-0.771, 1.000], loss: 9.044813, mean_absolute_error: 47.318886, mean_q: 62.694530\n",
      " 196879/700000: episode: 488, duration: 2.313s, episode steps: 441, steps per second: 191, episode reward: 252.949, mean reward: 0.574 [-17.764, 100.000], mean action: 1.283 [0.000, 3.000], mean observation: 0.102 [-0.991, 1.000], loss: 10.251355, mean_absolute_error: 47.361183, mean_q: 62.281570\n",
      " 197227/700000: episode: 489, duration: 1.815s, episode steps: 348, steps per second: 192, episode reward: 214.216, mean reward: 0.616 [-17.855, 100.000], mean action: 1.026 [0.000, 3.000], mean observation: 0.071 [-0.807, 1.000], loss: 14.105568, mean_absolute_error: 47.201366, mean_q: 62.189915\n",
      " 197569/700000: episode: 490, duration: 1.760s, episode steps: 342, steps per second: 194, episode reward: 219.494, mean reward: 0.642 [-18.950, 100.000], mean action: 1.152 [0.000, 3.000], mean observation: 0.102 [-0.800, 1.017], loss: 8.073511, mean_absolute_error: 47.349613, mean_q: 62.687790\n",
      " 197763/700000: episode: 491, duration: 0.971s, episode steps: 194, steps per second: 200, episode reward: -113.767, mean reward: -0.586 [-100.000, 2.566], mean action: 1.582 [0.000, 3.000], mean observation: -0.057 [-1.001, 0.946], loss: 13.843443, mean_absolute_error: 47.139889, mean_q: 62.273266\n",
      " 198006/700000: episode: 492, duration: 1.340s, episode steps: 243, steps per second: 181, episode reward: 196.885, mean reward: 0.810 [-6.824, 100.000], mean action: 1.058 [0.000, 3.000], mean observation: 0.092 [-0.924, 1.000], loss: 10.079108, mean_absolute_error: 47.161827, mean_q: 62.293236\n",
      " 198540/700000: episode: 493, duration: 2.839s, episode steps: 534, steps per second: 188, episode reward: 202.476, mean reward: 0.379 [-10.616, 100.000], mean action: 0.934 [0.000, 3.000], mean observation: 0.086 [-0.709, 1.000], loss: 10.320038, mean_absolute_error: 47.284416, mean_q: 62.437645\n",
      " 198799/700000: episode: 494, duration: 1.344s, episode steps: 259, steps per second: 193, episode reward: 248.414, mean reward: 0.959 [-8.644, 100.000], mean action: 1.730 [0.000, 3.000], mean observation: 0.096 [-0.881, 1.000], loss: 10.943597, mean_absolute_error: 47.908825, mean_q: 63.226765\n",
      " 199156/700000: episode: 495, duration: 1.828s, episode steps: 357, steps per second: 195, episode reward: 241.450, mean reward: 0.676 [-17.330, 100.000], mean action: 1.151 [0.000, 3.000], mean observation: 0.112 [-0.705, 1.000], loss: 11.595093, mean_absolute_error: 47.789719, mean_q: 63.021408\n",
      " 199434/700000: episode: 496, duration: 1.451s, episode steps: 278, steps per second: 192, episode reward: 140.732, mean reward: 0.506 [-10.432, 100.000], mean action: 2.371 [0.000, 3.000], mean observation: 0.106 [-0.830, 1.000], loss: 9.381746, mean_absolute_error: 47.886284, mean_q: 63.554714\n",
      " 199849/700000: episode: 497, duration: 2.114s, episode steps: 415, steps per second: 196, episode reward: 239.262, mean reward: 0.577 [-9.543, 100.000], mean action: 1.137 [0.000, 3.000], mean observation: 0.091 [-0.811, 1.000], loss: 10.594570, mean_absolute_error: 47.656353, mean_q: 63.227062\n",
      " 200329/700000: episode: 498, duration: 2.545s, episode steps: 480, steps per second: 189, episode reward: 220.239, mean reward: 0.459 [-18.713, 100.000], mean action: 0.985 [0.000, 3.000], mean observation: 0.148 [-0.774, 1.000], loss: 10.793542, mean_absolute_error: 47.812874, mean_q: 63.402004\n",
      " 200639/700000: episode: 499, duration: 1.590s, episode steps: 310, steps per second: 195, episode reward: 220.710, mean reward: 0.712 [-10.538, 100.000], mean action: 1.319 [0.000, 3.000], mean observation: 0.071 [-1.170, 1.012], loss: 15.085669, mean_absolute_error: 47.836742, mean_q: 63.354900\n",
      " 201080/700000: episode: 500, duration: 2.359s, episode steps: 441, steps per second: 187, episode reward: 239.981, mean reward: 0.544 [-18.860, 100.000], mean action: 1.093 [0.000, 3.000], mean observation: 0.123 [-0.735, 1.004], loss: 11.614918, mean_absolute_error: 47.740494, mean_q: 63.175541\n",
      " 201357/700000: episode: 501, duration: 1.409s, episode steps: 277, steps per second: 197, episode reward: -128.325, mean reward: -0.463 [-100.000, 22.460], mean action: 1.495 [0.000, 3.000], mean observation: -0.075 [-0.952, 1.000], loss: 10.817251, mean_absolute_error: 48.015167, mean_q: 63.497505\n",
      " 201655/700000: episode: 502, duration: 1.514s, episode steps: 298, steps per second: 197, episode reward: 218.167, mean reward: 0.732 [-19.463, 100.000], mean action: 1.185 [0.000, 3.000], mean observation: 0.107 [-1.293, 1.005], loss: 9.462246, mean_absolute_error: 47.969723, mean_q: 63.381359\n",
      " 202030/700000: episode: 503, duration: 1.966s, episode steps: 375, steps per second: 191, episode reward: 190.325, mean reward: 0.508 [-17.712, 100.000], mean action: 1.661 [0.000, 3.000], mean observation: 0.094 [-0.941, 1.000], loss: 10.185027, mean_absolute_error: 47.946239, mean_q: 63.801613\n",
      " 202419/700000: episode: 504, duration: 1.980s, episode steps: 389, steps per second: 197, episode reward: 203.139, mean reward: 0.522 [-17.631, 100.000], mean action: 0.766 [0.000, 3.000], mean observation: 0.156 [-0.828, 1.177], loss: 14.571563, mean_absolute_error: 48.059574, mean_q: 63.812447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 202830/700000: episode: 505, duration: 2.161s, episode steps: 411, steps per second: 190, episode reward: 226.031, mean reward: 0.550 [-19.143, 100.000], mean action: 1.399 [0.000, 3.000], mean observation: 0.117 [-0.790, 1.020], loss: 6.675127, mean_absolute_error: 48.404274, mean_q: 64.282364\n",
      " 203442/700000: episode: 506, duration: 3.222s, episode steps: 612, steps per second: 190, episode reward: 223.361, mean reward: 0.365 [-17.432, 100.000], mean action: 1.291 [0.000, 3.000], mean observation: 0.127 [-0.983, 1.128], loss: 12.361053, mean_absolute_error: 48.199406, mean_q: 63.730583\n",
      " 203785/700000: episode: 507, duration: 1.768s, episode steps: 343, steps per second: 194, episode reward: 234.721, mean reward: 0.684 [-20.815, 100.000], mean action: 0.988 [0.000, 3.000], mean observation: 0.105 [-0.794, 1.000], loss: 10.130342, mean_absolute_error: 47.989544, mean_q: 63.589279\n",
      " 203982/700000: episode: 508, duration: 0.991s, episode steps: 197, steps per second: 199, episode reward: -29.439, mean reward: -0.149 [-100.000, 17.084], mean action: 1.797 [0.000, 3.000], mean observation: 0.036 [-0.851, 1.378], loss: 8.791842, mean_absolute_error: 47.818546, mean_q: 63.471825\n",
      " 204073/700000: episode: 509, duration: 0.449s, episode steps: 91, steps per second: 203, episode reward: -54.444, mean reward: -0.598 [-100.000, 10.210], mean action: 1.780 [0.000, 3.000], mean observation: -0.061 [-1.764, 1.000], loss: 16.212231, mean_absolute_error: 47.221973, mean_q: 62.808327\n",
      " 204925/700000: episode: 510, duration: 4.856s, episode steps: 852, steps per second: 175, episode reward: 144.745, mean reward: 0.170 [-23.414, 100.000], mean action: 1.540 [0.000, 3.000], mean observation: 0.162 [-0.876, 1.000], loss: 7.710682, mean_absolute_error: 48.029549, mean_q: 63.785202\n",
      " 205796/700000: episode: 511, duration: 4.755s, episode steps: 871, steps per second: 183, episode reward: 207.691, mean reward: 0.238 [-20.406, 100.000], mean action: 1.016 [0.000, 3.000], mean observation: 0.174 [-1.010, 1.000], loss: 10.564261, mean_absolute_error: 47.810463, mean_q: 63.502068\n",
      " 206213/700000: episode: 512, duration: 2.140s, episode steps: 417, steps per second: 195, episode reward: 210.669, mean reward: 0.505 [-11.618, 100.000], mean action: 1.420 [0.000, 3.000], mean observation: 0.105 [-0.893, 1.000], loss: 9.959920, mean_absolute_error: 48.124943, mean_q: 63.880779\n",
      " 206353/700000: episode: 513, duration: 0.692s, episode steps: 140, steps per second: 202, episode reward: -64.009, mean reward: -0.457 [-100.000, 17.882], mean action: 1.886 [0.000, 3.000], mean observation: -0.069 [-0.776, 1.524], loss: 12.847648, mean_absolute_error: 47.640827, mean_q: 63.112236\n",
      " 206686/700000: episode: 514, duration: 1.720s, episode steps: 333, steps per second: 194, episode reward: 225.316, mean reward: 0.677 [-18.378, 100.000], mean action: 1.150 [0.000, 3.000], mean observation: 0.092 [-0.863, 1.000], loss: 9.638683, mean_absolute_error: 48.118912, mean_q: 64.057198\n",
      " 206970/700000: episode: 515, duration: 1.450s, episode steps: 284, steps per second: 196, episode reward: 260.522, mean reward: 0.917 [-19.239, 100.000], mean action: 1.405 [0.000, 3.000], mean observation: 0.072 [-0.893, 1.000], loss: 11.799169, mean_absolute_error: 48.096329, mean_q: 63.736176\n",
      " 207241/700000: episode: 516, duration: 1.377s, episode steps: 271, steps per second: 197, episode reward: 206.032, mean reward: 0.760 [-22.370, 100.000], mean action: 1.373 [0.000, 3.000], mean observation: 0.087 [-0.846, 1.000], loss: 11.049226, mean_absolute_error: 48.147820, mean_q: 63.866272\n",
      " 207566/700000: episode: 517, duration: 1.718s, episode steps: 325, steps per second: 189, episode reward: 208.909, mean reward: 0.643 [-22.200, 100.000], mean action: 2.151 [0.000, 3.000], mean observation: 0.114 [-1.153, 1.000], loss: 10.364223, mean_absolute_error: 48.091774, mean_q: 63.802753\n",
      " 207831/700000: episode: 518, duration: 1.331s, episode steps: 265, steps per second: 199, episode reward: 229.016, mean reward: 0.864 [-20.598, 100.000], mean action: 1.453 [0.000, 3.000], mean observation: 0.062 [-0.636, 1.000], loss: 13.816347, mean_absolute_error: 48.259598, mean_q: 64.249146\n",
      " 208080/700000: episode: 519, duration: 1.256s, episode steps: 249, steps per second: 198, episode reward: 208.698, mean reward: 0.838 [-2.868, 100.000], mean action: 1.538 [0.000, 3.000], mean observation: 0.111 [-0.980, 1.000], loss: 11.460537, mean_absolute_error: 47.817665, mean_q: 63.724178\n",
      " 208683/700000: episode: 520, duration: 3.181s, episode steps: 603, steps per second: 190, episode reward: 150.459, mean reward: 0.250 [-24.337, 100.000], mean action: 1.325 [0.000, 3.000], mean observation: 0.190 [-0.892, 1.000], loss: 12.373399, mean_absolute_error: 48.348850, mean_q: 64.350349\n",
      " 208958/700000: episode: 521, duration: 1.413s, episode steps: 275, steps per second: 195, episode reward: 213.468, mean reward: 0.776 [-19.766, 100.000], mean action: 1.342 [0.000, 3.000], mean observation: 0.098 [-0.785, 1.000], loss: 13.726359, mean_absolute_error: 48.592571, mean_q: 64.682610\n",
      " 209197/700000: episode: 522, duration: 1.199s, episode steps: 239, steps per second: 199, episode reward: 178.225, mean reward: 0.746 [-9.732, 100.000], mean action: 1.188 [0.000, 3.000], mean observation: 0.040 [-0.732, 1.000], loss: 9.879932, mean_absolute_error: 48.271965, mean_q: 64.368370\n",
      " 209877/700000: episode: 523, duration: 3.786s, episode steps: 680, steps per second: 180, episode reward: 169.331, mean reward: 0.249 [-20.527, 100.000], mean action: 1.250 [0.000, 3.000], mean observation: 0.166 [-0.898, 1.000], loss: 10.337468, mean_absolute_error: 48.379227, mean_q: 64.105431\n",
      " 210122/700000: episode: 524, duration: 1.240s, episode steps: 245, steps per second: 198, episode reward: 215.982, mean reward: 0.882 [-20.761, 100.000], mean action: 1.257 [0.000, 3.000], mean observation: 0.079 [-0.895, 1.000], loss: 16.683788, mean_absolute_error: 48.236927, mean_q: 64.252098\n",
      " 210482/700000: episode: 525, duration: 1.822s, episode steps: 360, steps per second: 198, episode reward: 225.612, mean reward: 0.627 [-11.776, 100.000], mean action: 1.183 [0.000, 3.000], mean observation: 0.081 [-1.236, 1.000], loss: 7.975471, mean_absolute_error: 48.765827, mean_q: 64.716347\n",
      " 210827/700000: episode: 526, duration: 1.783s, episode steps: 345, steps per second: 194, episode reward: 211.351, mean reward: 0.613 [-11.368, 100.000], mean action: 1.139 [0.000, 3.000], mean observation: 0.138 [-0.841, 1.000], loss: 12.773016, mean_absolute_error: 48.748383, mean_q: 64.597610\n",
      " 211068/700000: episode: 527, duration: 1.314s, episode steps: 241, steps per second: 183, episode reward: -32.301, mean reward: -0.134 [-100.000, 10.415], mean action: 1.622 [0.000, 3.000], mean observation: -0.014 [-0.507, 1.126], loss: 10.544502, mean_absolute_error: 49.059704, mean_q: 65.165932\n",
      " 211350/700000: episode: 528, duration: 1.509s, episode steps: 282, steps per second: 187, episode reward: 223.549, mean reward: 0.793 [-9.816, 100.000], mean action: 1.553 [0.000, 3.000], mean observation: 0.053 [-0.883, 1.000], loss: 11.018502, mean_absolute_error: 48.972862, mean_q: 65.193550\n",
      " 211765/700000: episode: 529, duration: 2.286s, episode steps: 415, steps per second: 182, episode reward: 199.448, mean reward: 0.481 [-17.976, 100.000], mean action: 0.648 [0.000, 3.000], mean observation: 0.143 [-0.830, 1.000], loss: 12.166177, mean_absolute_error: 48.766975, mean_q: 64.840836\n",
      " 212161/700000: episode: 530, duration: 3.788s, episode steps: 396, steps per second: 105, episode reward: 238.027, mean reward: 0.601 [-18.911, 100.000], mean action: 1.323 [0.000, 3.000], mean observation: 0.085 [-0.809, 1.012], loss: 10.697299, mean_absolute_error: 48.831963, mean_q: 65.069542\n",
      " 212590/700000: episode: 531, duration: 3.015s, episode steps: 429, steps per second: 142, episode reward: 236.192, mean reward: 0.551 [-19.270, 100.000], mean action: 1.522 [0.000, 3.000], mean observation: 0.090 [-0.925, 1.000], loss: 12.927052, mean_absolute_error: 49.122055, mean_q: 65.205963\n",
      " 213006/700000: episode: 532, duration: 2.630s, episode steps: 416, steps per second: 158, episode reward: 213.474, mean reward: 0.513 [-19.118, 100.000], mean action: 1.500 [0.000, 3.000], mean observation: 0.088 [-0.601, 1.000], loss: 9.844201, mean_absolute_error: 48.881809, mean_q: 65.116585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 213458/700000: episode: 533, duration: 2.344s, episode steps: 452, steps per second: 193, episode reward: 206.512, mean reward: 0.457 [-9.600, 100.000], mean action: 1.619 [0.000, 3.000], mean observation: 0.037 [-0.748, 1.093], loss: 10.793458, mean_absolute_error: 49.066166, mean_q: 65.254288\n",
      " 213778/700000: episode: 534, duration: 1.683s, episode steps: 320, steps per second: 190, episode reward: 235.016, mean reward: 0.734 [-3.490, 100.000], mean action: 1.144 [0.000, 3.000], mean observation: 0.138 [-0.685, 1.000], loss: 8.520460, mean_absolute_error: 48.916466, mean_q: 65.290421\n",
      " 214048/700000: episode: 535, duration: 1.380s, episode steps: 270, steps per second: 196, episode reward: 212.647, mean reward: 0.788 [-8.078, 100.000], mean action: 1.189 [0.000, 3.000], mean observation: 0.103 [-1.218, 1.000], loss: 11.021686, mean_absolute_error: 48.900059, mean_q: 64.663742\n",
      " 214420/700000: episode: 536, duration: 1.972s, episode steps: 372, steps per second: 189, episode reward: 235.067, mean reward: 0.632 [-18.864, 100.000], mean action: 1.323 [0.000, 3.000], mean observation: 0.063 [-0.839, 1.000], loss: 9.269444, mean_absolute_error: 48.619446, mean_q: 64.384575\n",
      " 214722/700000: episode: 537, duration: 1.545s, episode steps: 302, steps per second: 195, episode reward: 194.335, mean reward: 0.643 [-11.251, 100.000], mean action: 1.123 [0.000, 3.000], mean observation: 0.163 [-0.776, 1.214], loss: 14.431767, mean_absolute_error: 48.698494, mean_q: 64.711388\n",
      " 214994/700000: episode: 538, duration: 1.383s, episode steps: 272, steps per second: 197, episode reward: 189.879, mean reward: 0.698 [-17.820, 100.000], mean action: 1.037 [0.000, 3.000], mean observation: 0.104 [-1.092, 1.000], loss: 7.533774, mean_absolute_error: 48.683704, mean_q: 64.497597\n",
      " 215226/700000: episode: 539, duration: 1.174s, episode steps: 232, steps per second: 198, episode reward: 196.280, mean reward: 0.846 [-17.885, 100.000], mean action: 1.297 [0.000, 3.000], mean observation: 0.075 [-0.843, 1.000], loss: 8.019188, mean_absolute_error: 49.041023, mean_q: 65.101707\n",
      " 215689/700000: episode: 540, duration: 2.462s, episode steps: 463, steps per second: 188, episode reward: 165.531, mean reward: 0.358 [-18.441, 100.000], mean action: 2.255 [0.000, 3.000], mean observation: 0.129 [-0.948, 1.000], loss: 12.263799, mean_absolute_error: 48.498005, mean_q: 64.225899\n",
      " 215798/700000: episode: 541, duration: 0.553s, episode steps: 109, steps per second: 197, episode reward: -100.784, mean reward: -0.925 [-100.000, 17.291], mean action: 1.459 [0.000, 3.000], mean observation: 0.114 [-1.346, 1.000], loss: 11.466731, mean_absolute_error: 48.020832, mean_q: 63.763348\n",
      " 215900/700000: episode: 542, duration: 0.518s, episode steps: 102, steps per second: 197, episode reward: -100.936, mean reward: -0.990 [-100.000, 11.048], mean action: 1.598 [0.000, 3.000], mean observation: 0.127 [-1.120, 3.034], loss: 18.166271, mean_absolute_error: 47.927322, mean_q: 63.591366\n",
      " 216158/700000: episode: 543, duration: 1.303s, episode steps: 258, steps per second: 198, episode reward: -43.263, mean reward: -0.168 [-100.000, 19.006], mean action: 1.798 [0.000, 3.000], mean observation: 0.021 [-0.986, 1.000], loss: 8.120538, mean_absolute_error: 48.616474, mean_q: 64.193741\n",
      " 216346/700000: episode: 544, duration: 0.953s, episode steps: 188, steps per second: 197, episode reward: 201.861, mean reward: 1.074 [-8.425, 100.000], mean action: 1.638 [0.000, 3.000], mean observation: 0.034 [-0.808, 1.000], loss: 9.291701, mean_absolute_error: 48.559582, mean_q: 64.328911\n",
      " 216613/700000: episode: 545, duration: 1.377s, episode steps: 267, steps per second: 194, episode reward: 204.564, mean reward: 0.766 [-17.684, 100.000], mean action: 1.315 [0.000, 3.000], mean observation: 0.081 [-0.851, 1.000], loss: 7.216240, mean_absolute_error: 48.934685, mean_q: 64.839378\n",
      " 216752/700000: episode: 546, duration: 0.704s, episode steps: 139, steps per second: 197, episode reward: -33.152, mean reward: -0.239 [-100.000, 11.128], mean action: 1.835 [0.000, 3.000], mean observation: 0.129 [-1.055, 1.000], loss: 8.260540, mean_absolute_error: 48.551315, mean_q: 64.696106\n",
      " 217197/700000: episode: 547, duration: 2.402s, episode steps: 445, steps per second: 185, episode reward: 187.073, mean reward: 0.420 [-17.409, 100.000], mean action: 1.998 [0.000, 3.000], mean observation: 0.093 [-0.911, 1.479], loss: 12.007084, mean_absolute_error: 48.964237, mean_q: 64.723091\n",
      " 217468/700000: episode: 548, duration: 1.421s, episode steps: 271, steps per second: 191, episode reward: 204.221, mean reward: 0.754 [-11.491, 100.000], mean action: 1.609 [0.000, 3.000], mean observation: 0.090 [-0.752, 1.000], loss: 9.410628, mean_absolute_error: 49.086823, mean_q: 64.775818\n",
      " 217910/700000: episode: 549, duration: 2.311s, episode steps: 442, steps per second: 191, episode reward: 240.777, mean reward: 0.545 [-18.347, 100.000], mean action: 0.984 [0.000, 3.000], mean observation: 0.127 [-0.804, 1.000], loss: 10.180511, mean_absolute_error: 48.893803, mean_q: 64.775215\n",
      " 218649/700000: episode: 550, duration: 4.167s, episode steps: 739, steps per second: 177, episode reward: 183.566, mean reward: 0.248 [-20.233, 100.000], mean action: 0.953 [0.000, 3.000], mean observation: 0.210 [-1.113, 1.000], loss: 10.442745, mean_absolute_error: 48.493111, mean_q: 64.386292\n",
      " 218964/700000: episode: 551, duration: 1.675s, episode steps: 315, steps per second: 188, episode reward: 197.785, mean reward: 0.628 [-9.983, 100.000], mean action: 1.381 [0.000, 3.000], mean observation: 0.100 [-0.751, 1.000], loss: 14.065829, mean_absolute_error: 48.723431, mean_q: 64.715927\n",
      " 219597/700000: episode: 552, duration: 3.442s, episode steps: 633, steps per second: 184, episode reward: 229.469, mean reward: 0.363 [-18.638, 100.000], mean action: 1.202 [0.000, 3.000], mean observation: 0.115 [-0.744, 1.000], loss: 9.671893, mean_absolute_error: 48.508945, mean_q: 64.510742\n",
      " 219959/700000: episode: 553, duration: 1.916s, episode steps: 362, steps per second: 189, episode reward: 255.901, mean reward: 0.707 [-20.670, 100.000], mean action: 1.481 [0.000, 3.000], mean observation: 0.142 [-0.746, 1.000], loss: 9.982781, mean_absolute_error: 48.447105, mean_q: 64.273415\n",
      " 220285/700000: episode: 554, duration: 1.696s, episode steps: 326, steps per second: 192, episode reward: 222.744, mean reward: 0.683 [-18.491, 100.000], mean action: 1.132 [0.000, 3.000], mean observation: 0.115 [-0.788, 1.000], loss: 12.233739, mean_absolute_error: 48.557659, mean_q: 64.378922\n",
      " 220559/700000: episode: 555, duration: 1.418s, episode steps: 274, steps per second: 193, episode reward: 182.932, mean reward: 0.668 [-9.169, 100.000], mean action: 1.584 [0.000, 3.000], mean observation: 0.049 [-0.831, 1.000], loss: 8.543953, mean_absolute_error: 48.873615, mean_q: 65.019447\n",
      " 220879/700000: episode: 556, duration: 1.640s, episode steps: 320, steps per second: 195, episode reward: 201.499, mean reward: 0.630 [-10.924, 100.000], mean action: 1.553 [0.000, 3.000], mean observation: 0.026 [-0.612, 1.000], loss: 9.136625, mean_absolute_error: 48.836205, mean_q: 64.582970\n",
      " 221465/700000: episode: 557, duration: 3.098s, episode steps: 586, steps per second: 189, episode reward: 241.277, mean reward: 0.412 [-19.807, 100.000], mean action: 1.017 [0.000, 3.000], mean observation: 0.173 [-0.612, 1.000], loss: 5.424305, mean_absolute_error: 48.807526, mean_q: 64.628639\n",
      " 221753/700000: episode: 558, duration: 1.481s, episode steps: 288, steps per second: 194, episode reward: 211.080, mean reward: 0.733 [-10.098, 100.000], mean action: 1.007 [0.000, 3.000], mean observation: 0.096 [-0.754, 1.267], loss: 7.033099, mean_absolute_error: 48.720078, mean_q: 64.520195\n",
      " 222166/700000: episode: 559, duration: 2.241s, episode steps: 413, steps per second: 184, episode reward: 212.217, mean reward: 0.514 [-18.104, 100.000], mean action: 1.061 [0.000, 3.000], mean observation: 0.150 [-0.761, 1.000], loss: 8.024755, mean_absolute_error: 49.068760, mean_q: 65.025131\n",
      " 222402/700000: episode: 560, duration: 1.221s, episode steps: 236, steps per second: 193, episode reward: 266.397, mean reward: 1.129 [-13.153, 100.000], mean action: 1.292 [0.000, 3.000], mean observation: 0.126 [-0.792, 1.080], loss: 11.451869, mean_absolute_error: 49.074184, mean_q: 64.961342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 222985/700000: episode: 561, duration: 3.156s, episode steps: 583, steps per second: 185, episode reward: 202.467, mean reward: 0.347 [-21.142, 100.000], mean action: 0.998 [0.000, 3.000], mean observation: 0.170 [-0.856, 1.000], loss: 7.186007, mean_absolute_error: 48.733330, mean_q: 64.805542\n",
      " 223594/700000: episode: 562, duration: 3.363s, episode steps: 609, steps per second: 181, episode reward: 208.247, mean reward: 0.342 [-17.554, 100.000], mean action: 1.159 [0.000, 3.000], mean observation: 0.103 [-0.662, 1.000], loss: 9.578723, mean_absolute_error: 48.722069, mean_q: 64.411690\n",
      " 223958/700000: episode: 563, duration: 1.838s, episode steps: 364, steps per second: 198, episode reward: -64.540, mean reward: -0.177 [-100.000, 18.680], mean action: 1.184 [0.000, 3.000], mean observation: 0.173 [-1.137, 1.040], loss: 12.484550, mean_absolute_error: 48.525986, mean_q: 64.462303\n",
      " 224285/700000: episode: 564, duration: 1.690s, episode steps: 327, steps per second: 194, episode reward: 206.268, mean reward: 0.631 [-17.523, 100.000], mean action: 1.272 [0.000, 3.000], mean observation: 0.051 [-0.639, 1.000], loss: 9.907785, mean_absolute_error: 48.452297, mean_q: 64.217583\n",
      " 224633/700000: episode: 565, duration: 1.840s, episode steps: 348, steps per second: 189, episode reward: 210.891, mean reward: 0.606 [-12.916, 100.000], mean action: 1.658 [0.000, 3.000], mean observation: 0.099 [-0.993, 1.000], loss: 13.079141, mean_absolute_error: 48.256184, mean_q: 64.039856\n",
      " 225221/700000: episode: 566, duration: 3.148s, episode steps: 588, steps per second: 187, episode reward: 230.663, mean reward: 0.392 [-19.628, 100.000], mean action: 1.066 [0.000, 3.000], mean observation: 0.118 [-0.888, 1.000], loss: 13.490258, mean_absolute_error: 48.801430, mean_q: 64.898819\n",
      " 225608/700000: episode: 567, duration: 2.032s, episode steps: 387, steps per second: 190, episode reward: 229.189, mean reward: 0.592 [-12.380, 100.000], mean action: 1.142 [0.000, 3.000], mean observation: 0.064 [-0.859, 1.000], loss: 11.060398, mean_absolute_error: 48.885872, mean_q: 64.562134\n",
      " 226102/700000: episode: 568, duration: 2.574s, episode steps: 494, steps per second: 192, episode reward: 240.264, mean reward: 0.486 [-19.494, 100.000], mean action: 1.079 [0.000, 3.000], mean observation: 0.182 [-0.884, 1.390], loss: 7.820391, mean_absolute_error: 48.580292, mean_q: 64.386810\n",
      " 226405/700000: episode: 569, duration: 1.577s, episode steps: 303, steps per second: 192, episode reward: 201.996, mean reward: 0.667 [-17.381, 100.000], mean action: 1.040 [0.000, 3.000], mean observation: 0.099 [-0.833, 1.000], loss: 15.512709, mean_absolute_error: 48.744114, mean_q: 64.235306\n",
      " 226760/700000: episode: 570, duration: 1.815s, episode steps: 355, steps per second: 196, episode reward: 250.828, mean reward: 0.707 [-9.630, 100.000], mean action: 1.434 [0.000, 3.000], mean observation: 0.065 [-0.767, 1.000], loss: 7.822095, mean_absolute_error: 48.693420, mean_q: 64.510109\n",
      " 226925/700000: episode: 571, duration: 0.832s, episode steps: 165, steps per second: 198, episode reward: -167.523, mean reward: -1.015 [-100.000, 11.165], mean action: 1.600 [0.000, 3.000], mean observation: 0.111 [-0.828, 1.006], loss: 10.368661, mean_absolute_error: 48.925282, mean_q: 65.086555\n",
      " 227340/700000: episode: 572, duration: 2.235s, episode steps: 415, steps per second: 186, episode reward: 225.957, mean reward: 0.544 [-9.119, 100.000], mean action: 1.325 [0.000, 3.000], mean observation: 0.043 [-1.269, 1.000], loss: 12.224534, mean_absolute_error: 48.901642, mean_q: 64.784622\n",
      " 227541/700000: episode: 573, duration: 1.012s, episode steps: 201, steps per second: 199, episode reward: 229.602, mean reward: 1.142 [-17.338, 100.000], mean action: 1.234 [0.000, 3.000], mean observation: 0.083 [-0.675, 1.000], loss: 10.468693, mean_absolute_error: 49.596855, mean_q: 65.517639\n",
      " 228144/700000: episode: 574, duration: 3.161s, episode steps: 603, steps per second: 191, episode reward: 240.824, mean reward: 0.399 [-17.568, 100.000], mean action: 0.983 [0.000, 3.000], mean observation: 0.172 [-0.943, 1.000], loss: 9.117004, mean_absolute_error: 49.260811, mean_q: 65.330879\n",
      " 228410/700000: episode: 575, duration: 1.365s, episode steps: 266, steps per second: 195, episode reward: 251.712, mean reward: 0.946 [-9.603, 100.000], mean action: 1.650 [0.000, 3.000], mean observation: 0.042 [-1.335, 1.000], loss: 8.983079, mean_absolute_error: 49.004227, mean_q: 64.788887\n",
      " 228729/700000: episode: 576, duration: 1.658s, episode steps: 319, steps per second: 192, episode reward: 215.718, mean reward: 0.676 [-17.730, 100.000], mean action: 1.348 [0.000, 3.000], mean observation: 0.083 [-0.871, 1.120], loss: 11.745337, mean_absolute_error: 48.763107, mean_q: 64.598343\n",
      " 228943/700000: episode: 577, duration: 1.084s, episode steps: 214, steps per second: 197, episode reward: 243.994, mean reward: 1.140 [-3.041, 100.000], mean action: 1.234 [0.000, 3.000], mean observation: 0.098 [-0.821, 1.000], loss: 9.603102, mean_absolute_error: 48.470776, mean_q: 64.240150\n",
      " 229182/700000: episode: 578, duration: 1.204s, episode steps: 239, steps per second: 199, episode reward: 199.914, mean reward: 0.836 [-9.631, 100.000], mean action: 1.226 [0.000, 3.000], mean observation: 0.080 [-0.665, 1.000], loss: 8.204988, mean_absolute_error: 48.475750, mean_q: 64.036926\n",
      " 229483/700000: episode: 579, duration: 1.526s, episode steps: 301, steps per second: 197, episode reward: 204.889, mean reward: 0.681 [-17.604, 100.000], mean action: 1.412 [0.000, 3.000], mean observation: 0.055 [-0.821, 1.000], loss: 9.751137, mean_absolute_error: 48.512142, mean_q: 64.313034\n",
      " 229775/700000: episode: 580, duration: 1.473s, episode steps: 292, steps per second: 198, episode reward: 211.472, mean reward: 0.724 [-13.605, 100.000], mean action: 1.092 [0.000, 3.000], mean observation: 0.154 [-1.271, 1.000], loss: 9.728922, mean_absolute_error: 48.808769, mean_q: 64.751328\n",
      " 230254/700000: episode: 581, duration: 2.525s, episode steps: 479, steps per second: 190, episode reward: 210.037, mean reward: 0.438 [-18.472, 100.000], mean action: 0.843 [0.000, 3.000], mean observation: 0.128 [-0.882, 1.000], loss: 12.193721, mean_absolute_error: 49.046772, mean_q: 64.957367\n",
      " 230729/700000: episode: 582, duration: 2.544s, episode steps: 475, steps per second: 187, episode reward: 198.189, mean reward: 0.417 [-19.646, 100.000], mean action: 1.425 [0.000, 3.000], mean observation: 0.084 [-1.111, 1.000], loss: 10.465934, mean_absolute_error: 48.903717, mean_q: 65.011055\n",
      " 231098/700000: episode: 583, duration: 1.919s, episode steps: 369, steps per second: 192, episode reward: 220.548, mean reward: 0.598 [-19.650, 100.000], mean action: 1.295 [0.000, 3.000], mean observation: 0.084 [-0.780, 1.000], loss: 8.249646, mean_absolute_error: 48.747047, mean_q: 64.835602\n",
      " 231415/700000: episode: 584, duration: 1.639s, episode steps: 317, steps per second: 193, episode reward: 200.899, mean reward: 0.634 [-5.115, 100.000], mean action: 1.271 [0.000, 3.000], mean observation: 0.055 [-0.985, 1.000], loss: 12.493089, mean_absolute_error: 48.572803, mean_q: 64.504646\n",
      " 231750/700000: episode: 585, duration: 1.765s, episode steps: 335, steps per second: 190, episode reward: 197.028, mean reward: 0.588 [-17.829, 100.000], mean action: 1.421 [0.000, 3.000], mean observation: 0.018 [-0.624, 1.000], loss: 10.001519, mean_absolute_error: 48.625393, mean_q: 64.665512\n",
      " 232192/700000: episode: 586, duration: 2.334s, episode steps: 442, steps per second: 189, episode reward: 204.138, mean reward: 0.462 [-18.753, 100.000], mean action: 0.977 [0.000, 3.000], mean observation: 0.119 [-0.744, 1.000], loss: 7.267582, mean_absolute_error: 48.610409, mean_q: 64.583931\n",
      " 232350/700000: episode: 587, duration: 0.800s, episode steps: 158, steps per second: 198, episode reward: -55.129, mean reward: -0.349 [-100.000, 36.161], mean action: 1.880 [0.000, 3.000], mean observation: 0.104 [-1.081, 2.208], loss: 11.684615, mean_absolute_error: 48.407314, mean_q: 64.382339\n",
      " 232600/700000: episode: 588, duration: 1.274s, episode steps: 250, steps per second: 196, episode reward: 242.073, mean reward: 0.968 [-17.676, 100.000], mean action: 1.096 [0.000, 3.000], mean observation: 0.137 [-1.073, 1.000], loss: 8.359378, mean_absolute_error: 48.185589, mean_q: 63.747231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 232707/700000: episode: 589, duration: 0.548s, episode steps: 107, steps per second: 195, episode reward: -90.706, mean reward: -0.848 [-100.000, 16.961], mean action: 1.654 [0.000, 3.000], mean observation: 0.036 [-1.753, 1.000], loss: 7.398994, mean_absolute_error: 47.992496, mean_q: 63.910412\n",
      " 233337/700000: episode: 590, duration: 3.594s, episode steps: 630, steps per second: 175, episode reward: 235.346, mean reward: 0.374 [-19.192, 100.000], mean action: 0.894 [0.000, 3.000], mean observation: 0.175 [-1.019, 1.000], loss: 10.445923, mean_absolute_error: 48.177933, mean_q: 63.802479\n",
      " 233568/700000: episode: 591, duration: 1.215s, episode steps: 231, steps per second: 190, episode reward: 246.895, mean reward: 1.069 [-11.204, 100.000], mean action: 1.446 [0.000, 3.000], mean observation: 0.136 [-0.757, 1.000], loss: 8.356945, mean_absolute_error: 48.281521, mean_q: 63.980934\n",
      " 233934/700000: episode: 592, duration: 2.102s, episode steps: 366, steps per second: 174, episode reward: 237.225, mean reward: 0.648 [-12.113, 100.000], mean action: 1.533 [0.000, 3.000], mean observation: 0.045 [-0.910, 1.000], loss: 10.268996, mean_absolute_error: 48.149071, mean_q: 63.994259\n",
      " 234063/700000: episode: 593, duration: 0.734s, episode steps: 129, steps per second: 176, episode reward: -242.858, mean reward: -1.883 [-100.000, 11.001], mean action: 1.783 [0.000, 3.000], mean observation: -0.110 [-1.075, 1.428], loss: 8.961349, mean_absolute_error: 47.720901, mean_q: 63.305580\n",
      " 234510/700000: episode: 594, duration: 2.514s, episode steps: 447, steps per second: 178, episode reward: 205.549, mean reward: 0.460 [-19.972, 100.000], mean action: 1.257 [0.000, 3.000], mean observation: 0.138 [-0.830, 1.000], loss: 11.827627, mean_absolute_error: 48.296165, mean_q: 63.983631\n",
      " 234656/700000: episode: 595, duration: 0.737s, episode steps: 146, steps per second: 198, episode reward: -45.942, mean reward: -0.315 [-100.000, 9.886], mean action: 1.863 [0.000, 3.000], mean observation: 0.103 [-1.040, 1.000], loss: 10.182743, mean_absolute_error: 47.379444, mean_q: 62.753586\n",
      " 234986/700000: episode: 596, duration: 1.726s, episode steps: 330, steps per second: 191, episode reward: 148.925, mean reward: 0.451 [-24.548, 100.000], mean action: 1.670 [0.000, 3.000], mean observation: 0.106 [-0.802, 1.000], loss: 13.317232, mean_absolute_error: 47.544529, mean_q: 62.979179\n",
      " 235190/700000: episode: 597, duration: 1.044s, episode steps: 204, steps per second: 195, episode reward: 247.893, mean reward: 1.215 [-9.172, 100.000], mean action: 1.480 [0.000, 3.000], mean observation: 0.060 [-0.762, 1.000], loss: 14.247178, mean_absolute_error: 47.659447, mean_q: 63.446156\n",
      " 235863/700000: episode: 598, duration: 3.618s, episode steps: 673, steps per second: 186, episode reward: 213.367, mean reward: 0.317 [-18.214, 100.000], mean action: 0.935 [0.000, 3.000], mean observation: 0.132 [-0.882, 1.000], loss: 10.122945, mean_absolute_error: 47.669445, mean_q: 63.282818\n",
      " 236136/700000: episode: 599, duration: 1.390s, episode steps: 273, steps per second: 196, episode reward: 221.387, mean reward: 0.811 [-12.776, 100.000], mean action: 0.832 [0.000, 3.000], mean observation: 0.136 [-1.140, 1.000], loss: 9.658054, mean_absolute_error: 47.603424, mean_q: 63.406944\n",
      " 236477/700000: episode: 600, duration: 1.769s, episode steps: 341, steps per second: 193, episode reward: 235.125, mean reward: 0.690 [-17.661, 100.000], mean action: 1.314 [0.000, 3.000], mean observation: 0.044 [-0.662, 1.000], loss: 10.212425, mean_absolute_error: 47.976131, mean_q: 63.559769\n",
      " 236817/700000: episode: 601, duration: 1.777s, episode steps: 340, steps per second: 191, episode reward: 201.565, mean reward: 0.593 [-7.402, 100.000], mean action: 1.544 [0.000, 3.000], mean observation: 0.052 [-0.679, 1.000], loss: 8.573897, mean_absolute_error: 47.751656, mean_q: 63.200062\n",
      " 237158/700000: episode: 602, duration: 1.769s, episode steps: 341, steps per second: 193, episode reward: 216.596, mean reward: 0.635 [-10.037, 100.000], mean action: 1.120 [0.000, 3.000], mean observation: 0.138 [-0.793, 1.000], loss: 9.044479, mean_absolute_error: 47.859783, mean_q: 63.307713\n",
      " 237439/700000: episode: 603, duration: 1.437s, episode steps: 281, steps per second: 196, episode reward: 212.010, mean reward: 0.754 [-19.578, 100.000], mean action: 1.174 [0.000, 3.000], mean observation: 0.129 [-0.754, 1.000], loss: 7.440574, mean_absolute_error: 48.080643, mean_q: 63.845329\n",
      " 237794/700000: episode: 604, duration: 1.802s, episode steps: 355, steps per second: 197, episode reward: 240.022, mean reward: 0.676 [-18.654, 100.000], mean action: 1.290 [0.000, 3.000], mean observation: 0.153 [-0.815, 1.015], loss: 9.594944, mean_absolute_error: 47.874065, mean_q: 63.416683\n",
      " 238038/700000: episode: 605, duration: 1.238s, episode steps: 244, steps per second: 197, episode reward: 237.213, mean reward: 0.972 [-19.245, 100.000], mean action: 0.947 [0.000, 3.000], mean observation: 0.106 [-1.012, 1.000], loss: 8.152098, mean_absolute_error: 47.552395, mean_q: 63.289013\n",
      " 238313/700000: episode: 606, duration: 1.399s, episode steps: 275, steps per second: 197, episode reward: 226.948, mean reward: 0.825 [-19.590, 100.000], mean action: 1.033 [0.000, 3.000], mean observation: 0.138 [-0.800, 1.000], loss: 7.417317, mean_absolute_error: 47.852726, mean_q: 63.741768\n",
      " 238639/700000: episode: 607, duration: 1.673s, episode steps: 326, steps per second: 195, episode reward: 220.935, mean reward: 0.678 [-3.929, 100.000], mean action: 1.215 [0.000, 3.000], mean observation: 0.072 [-1.155, 1.000], loss: 5.606357, mean_absolute_error: 48.010166, mean_q: 63.626110\n",
      " 239052/700000: episode: 608, duration: 2.233s, episode steps: 413, steps per second: 185, episode reward: 214.211, mean reward: 0.519 [-18.790, 100.000], mean action: 1.097 [0.000, 3.000], mean observation: 0.071 [-0.953, 1.000], loss: 5.394353, mean_absolute_error: 48.013134, mean_q: 63.918819\n",
      " 239325/700000: episode: 609, duration: 1.382s, episode steps: 273, steps per second: 198, episode reward: 209.782, mean reward: 0.768 [-2.503, 100.000], mean action: 1.125 [0.000, 3.000], mean observation: 0.086 [-0.759, 1.011], loss: 6.769542, mean_absolute_error: 47.975483, mean_q: 63.762035\n",
      " 239518/700000: episode: 610, duration: 0.961s, episode steps: 193, steps per second: 201, episode reward: 203.138, mean reward: 1.053 [-3.238, 100.000], mean action: 1.207 [0.000, 3.000], mean observation: 0.107 [-0.956, 1.000], loss: 9.823359, mean_absolute_error: 48.200794, mean_q: 63.937359\n",
      " 239710/700000: episode: 611, duration: 0.963s, episode steps: 192, steps per second: 199, episode reward: 211.338, mean reward: 1.101 [-3.008, 100.000], mean action: 1.146 [0.000, 3.000], mean observation: 0.113 [-1.015, 1.000], loss: 12.014061, mean_absolute_error: 48.491131, mean_q: 64.325569\n",
      " 239902/700000: episode: 612, duration: 0.976s, episode steps: 192, steps per second: 197, episode reward: -64.106, mean reward: -0.334 [-100.000, 18.409], mean action: 1.740 [0.000, 3.000], mean observation: -0.054 [-0.672, 1.379], loss: 7.489295, mean_absolute_error: 48.214737, mean_q: 63.931561\n",
      " 240140/700000: episode: 613, duration: 1.196s, episode steps: 238, steps per second: 199, episode reward: 261.937, mean reward: 1.101 [-17.824, 100.000], mean action: 1.454 [0.000, 3.000], mean observation: 0.136 [-1.291, 1.000], loss: 7.267506, mean_absolute_error: 48.426781, mean_q: 64.573311\n",
      " 240314/700000: episode: 614, duration: 0.870s, episode steps: 174, steps per second: 200, episode reward: 178.472, mean reward: 1.026 [-15.158, 100.000], mean action: 1.299 [0.000, 3.000], mean observation: 0.012 [-0.965, 1.000], loss: 9.409027, mean_absolute_error: 48.539310, mean_q: 64.649307\n",
      " 240877/700000: episode: 615, duration: 2.981s, episode steps: 563, steps per second: 189, episode reward: 202.606, mean reward: 0.360 [-20.097, 100.000], mean action: 1.032 [0.000, 3.000], mean observation: 0.223 [-1.388, 1.000], loss: 8.687662, mean_absolute_error: 48.740139, mean_q: 65.040100\n",
      " 241246/700000: episode: 616, duration: 1.883s, episode steps: 369, steps per second: 196, episode reward: 210.494, mean reward: 0.570 [-11.300, 100.000], mean action: 0.683 [0.000, 3.000], mean observation: 0.171 [-1.385, 1.000], loss: 10.510130, mean_absolute_error: 49.368744, mean_q: 65.689819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 241600/700000: episode: 617, duration: 1.822s, episode steps: 354, steps per second: 194, episode reward: 262.998, mean reward: 0.743 [-10.201, 100.000], mean action: 1.743 [0.000, 3.000], mean observation: 0.048 [-1.254, 1.004], loss: 11.533413, mean_absolute_error: 49.351299, mean_q: 65.932312\n",
      " 241878/700000: episode: 618, duration: 1.405s, episode steps: 278, steps per second: 198, episode reward: 228.899, mean reward: 0.823 [-22.967, 100.000], mean action: 1.209 [0.000, 3.000], mean observation: 0.127 [-0.727, 1.000], loss: 12.447282, mean_absolute_error: 49.661476, mean_q: 66.359100\n",
      " 242089/700000: episode: 619, duration: 1.059s, episode steps: 211, steps per second: 199, episode reward: 220.724, mean reward: 1.046 [-17.440, 100.000], mean action: 0.995 [0.000, 3.000], mean observation: 0.165 [-1.213, 1.000], loss: 8.782227, mean_absolute_error: 50.147751, mean_q: 66.978912\n",
      " 242438/700000: episode: 620, duration: 1.786s, episode steps: 349, steps per second: 195, episode reward: 238.211, mean reward: 0.683 [-17.772, 100.000], mean action: 1.066 [0.000, 3.000], mean observation: 0.168 [-0.806, 1.000], loss: 9.181976, mean_absolute_error: 49.795162, mean_q: 66.248894\n",
      " 243438/700000: episode: 621, duration: 5.481s, episode steps: 1000, steps per second: 182, episode reward: 88.376, mean reward: 0.088 [-19.432, 22.354], mean action: 1.021 [0.000, 3.000], mean observation: 0.159 [-1.136, 1.000], loss: 11.989742, mean_absolute_error: 49.967934, mean_q: 66.459084\n",
      " 243934/700000: episode: 622, duration: 2.624s, episode steps: 496, steps per second: 189, episode reward: 195.746, mean reward: 0.395 [-24.048, 100.000], mean action: 1.016 [0.000, 3.000], mean observation: 0.172 [-0.760, 1.000], loss: 10.563989, mean_absolute_error: 50.312443, mean_q: 67.168709\n",
      " 244205/700000: episode: 623, duration: 1.392s, episode steps: 271, steps per second: 195, episode reward: 229.199, mean reward: 0.846 [-5.242, 100.000], mean action: 1.572 [0.000, 3.000], mean observation: 0.127 [-0.918, 1.000], loss: 9.388631, mean_absolute_error: 50.159550, mean_q: 66.924629\n",
      " 244622/700000: episode: 624, duration: 2.183s, episode steps: 417, steps per second: 191, episode reward: 217.586, mean reward: 0.522 [-18.068, 100.000], mean action: 1.257 [0.000, 3.000], mean observation: 0.095 [-0.643, 1.000], loss: 10.640923, mean_absolute_error: 50.497730, mean_q: 67.284637\n",
      " 244892/700000: episode: 625, duration: 1.385s, episode steps: 270, steps per second: 195, episode reward: 208.989, mean reward: 0.774 [-17.753, 100.000], mean action: 0.793 [0.000, 3.000], mean observation: 0.135 [-1.151, 1.000], loss: 8.844281, mean_absolute_error: 50.572830, mean_q: 67.420357\n",
      " 245105/700000: episode: 626, duration: 1.091s, episode steps: 213, steps per second: 195, episode reward: -188.384, mean reward: -0.884 [-100.000, 32.695], mean action: 2.108 [0.000, 3.000], mean observation: 0.089 [-0.886, 1.808], loss: 18.095570, mean_absolute_error: 50.696644, mean_q: 67.462433\n",
      " 245379/700000: episode: 627, duration: 1.385s, episode steps: 274, steps per second: 198, episode reward: 243.712, mean reward: 0.889 [-18.183, 100.000], mean action: 1.088 [0.000, 3.000], mean observation: 0.122 [-0.738, 1.132], loss: 11.933007, mean_absolute_error: 50.446098, mean_q: 67.229912\n",
      " 245579/700000: episode: 628, duration: 1.009s, episode steps: 200, steps per second: 198, episode reward: 194.427, mean reward: 0.972 [-12.894, 100.000], mean action: 1.185 [0.000, 3.000], mean observation: 0.029 [-1.121, 1.000], loss: 9.532131, mean_absolute_error: 50.225819, mean_q: 67.115234\n",
      " 245839/700000: episode: 629, duration: 1.338s, episode steps: 260, steps per second: 194, episode reward: 215.993, mean reward: 0.831 [-9.423, 100.000], mean action: 1.038 [0.000, 3.000], mean observation: 0.019 [-1.489, 1.000], loss: 10.373095, mean_absolute_error: 50.342838, mean_q: 67.259407\n",
      " 246111/700000: episode: 630, duration: 1.389s, episode steps: 272, steps per second: 196, episode reward: 239.136, mean reward: 0.879 [-17.764, 100.000], mean action: 1.393 [0.000, 3.000], mean observation: 0.065 [-0.749, 1.000], loss: 10.456841, mean_absolute_error: 50.587502, mean_q: 67.398262\n",
      " 246397/700000: episode: 631, duration: 1.449s, episode steps: 286, steps per second: 197, episode reward: 215.718, mean reward: 0.754 [-17.688, 100.000], mean action: 0.906 [0.000, 3.000], mean observation: 0.146 [-0.709, 1.012], loss: 8.094942, mean_absolute_error: 50.939667, mean_q: 67.877502\n",
      " 246504/700000: episode: 632, duration: 0.542s, episode steps: 107, steps per second: 198, episode reward: -10.331, mean reward: -0.097 [-100.000, 12.863], mean action: 1.682 [0.000, 3.000], mean observation: -0.046 [-0.815, 1.151], loss: 10.942901, mean_absolute_error: 50.683311, mean_q: 67.646820\n",
      " 247006/700000: episode: 633, duration: 2.663s, episode steps: 502, steps per second: 189, episode reward: 232.767, mean reward: 0.464 [-10.510, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.111 [-0.663, 1.000], loss: 10.825584, mean_absolute_error: 50.613079, mean_q: 67.277107\n",
      " 247094/700000: episode: 634, duration: 0.438s, episode steps: 88, steps per second: 201, episode reward: -152.997, mean reward: -1.739 [-100.000, 5.459], mean action: 1.352 [0.000, 3.000], mean observation: 0.098 [-1.362, 1.072], loss: 7.253462, mean_absolute_error: 50.596592, mean_q: 67.490082\n",
      " 247362/700000: episode: 635, duration: 1.344s, episode steps: 268, steps per second: 199, episode reward: 245.896, mean reward: 0.918 [-11.530, 100.000], mean action: 0.985 [0.000, 3.000], mean observation: 0.071 [-1.409, 1.005], loss: 8.131983, mean_absolute_error: 50.003952, mean_q: 66.267334\n",
      " 247714/700000: episode: 636, duration: 1.815s, episode steps: 352, steps per second: 194, episode reward: 243.765, mean reward: 0.693 [-9.464, 100.000], mean action: 1.457 [0.000, 3.000], mean observation: 0.058 [-1.461, 1.017], loss: 8.160385, mean_absolute_error: 50.562046, mean_q: 67.452812\n",
      " 247926/700000: episode: 637, duration: 1.076s, episode steps: 212, steps per second: 197, episode reward: 212.452, mean reward: 1.002 [-3.502, 100.000], mean action: 1.137 [0.000, 3.000], mean observation: 0.072 [-0.739, 1.000], loss: 9.219313, mean_absolute_error: 51.499916, mean_q: 68.473244\n",
      " 248048/700000: episode: 638, duration: 0.617s, episode steps: 122, steps per second: 198, episode reward: -20.349, mean reward: -0.167 [-100.000, 15.259], mean action: 1.680 [0.000, 3.000], mean observation: -0.065 [-0.795, 1.505], loss: 13.563399, mean_absolute_error: 50.943169, mean_q: 67.704552\n",
      " 248361/700000: episode: 639, duration: 1.606s, episode steps: 313, steps per second: 195, episode reward: 186.254, mean reward: 0.595 [-10.901, 100.000], mean action: 0.805 [0.000, 3.000], mean observation: 0.153 [-0.725, 1.000], loss: 8.881396, mean_absolute_error: 51.012688, mean_q: 68.038414\n",
      " 248803/700000: episode: 640, duration: 2.349s, episode steps: 442, steps per second: 188, episode reward: 208.664, mean reward: 0.472 [-20.698, 100.000], mean action: 0.925 [0.000, 3.000], mean observation: 0.187 [-0.901, 1.000], loss: 12.191720, mean_absolute_error: 51.173935, mean_q: 68.095695\n",
      " 249076/700000: episode: 641, duration: 1.392s, episode steps: 273, steps per second: 196, episode reward: 217.281, mean reward: 0.796 [-19.208, 100.000], mean action: 1.234 [0.000, 3.000], mean observation: 0.115 [-0.795, 1.000], loss: 10.413637, mean_absolute_error: 50.791721, mean_q: 67.562210\n",
      " 249529/700000: episode: 642, duration: 2.382s, episode steps: 453, steps per second: 190, episode reward: 213.767, mean reward: 0.472 [-20.514, 100.000], mean action: 0.746 [0.000, 3.000], mean observation: 0.181 [-0.883, 1.000], loss: 10.793249, mean_absolute_error: 51.231289, mean_q: 67.990120\n",
      " 249865/700000: episode: 643, duration: 1.753s, episode steps: 336, steps per second: 192, episode reward: 209.243, mean reward: 0.623 [-19.259, 100.000], mean action: 0.961 [0.000, 3.000], mean observation: 0.122 [-0.781, 1.000], loss: 14.980697, mean_absolute_error: 51.315693, mean_q: 68.109146\n",
      " 250022/700000: episode: 644, duration: 0.783s, episode steps: 157, steps per second: 200, episode reward: -29.343, mean reward: -0.187 [-100.000, 13.054], mean action: 1.631 [0.000, 3.000], mean observation: -0.013 [-0.684, 1.000], loss: 13.724548, mean_absolute_error: 51.670185, mean_q: 68.426468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 250575/700000: episode: 645, duration: 2.861s, episode steps: 553, steps per second: 193, episode reward: 190.146, mean reward: 0.344 [-18.066, 100.000], mean action: 0.915 [0.000, 3.000], mean observation: 0.198 [-0.909, 1.000], loss: 11.175149, mean_absolute_error: 51.238365, mean_q: 68.046089\n",
      " 250984/700000: episode: 646, duration: 2.163s, episode steps: 409, steps per second: 189, episode reward: 224.186, mean reward: 0.548 [-18.856, 100.000], mean action: 1.071 [0.000, 3.000], mean observation: 0.137 [-0.852, 1.005], loss: 8.800639, mean_absolute_error: 51.311768, mean_q: 67.973839\n",
      " 251439/700000: episode: 647, duration: 2.367s, episode steps: 455, steps per second: 192, episode reward: 195.843, mean reward: 0.430 [-17.780, 100.000], mean action: 0.635 [0.000, 3.000], mean observation: 0.164 [-0.889, 1.000], loss: 10.187439, mean_absolute_error: 51.369514, mean_q: 68.051239\n",
      " 251793/700000: episode: 648, duration: 1.843s, episode steps: 354, steps per second: 192, episode reward: -22.941, mean reward: -0.065 [-100.000, 20.814], mean action: 1.624 [0.000, 3.000], mean observation: 0.157 [-1.021, 1.000], loss: 10.539702, mean_absolute_error: 51.546181, mean_q: 68.359367\n",
      " 252431/700000: episode: 649, duration: 3.412s, episode steps: 638, steps per second: 187, episode reward: 210.947, mean reward: 0.331 [-18.184, 100.000], mean action: 1.324 [0.000, 3.000], mean observation: 0.163 [-0.775, 1.000], loss: 9.547905, mean_absolute_error: 51.592270, mean_q: 68.695618\n",
      " 253431/700000: episode: 650, duration: 5.755s, episode steps: 1000, steps per second: 174, episode reward: 54.980, mean reward: 0.055 [-19.343, 13.156], mean action: 1.773 [0.000, 3.000], mean observation: 0.183 [-0.678, 1.000], loss: 9.727105, mean_absolute_error: 51.819344, mean_q: 68.827423\n",
      " 253771/700000: episode: 651, duration: 1.757s, episode steps: 340, steps per second: 194, episode reward: 176.065, mean reward: 0.518 [-17.350, 100.000], mean action: 0.947 [0.000, 3.000], mean observation: 0.125 [-0.713, 1.000], loss: 8.453906, mean_absolute_error: 51.954090, mean_q: 69.026665\n",
      " 254146/700000: episode: 652, duration: 2.054s, episode steps: 375, steps per second: 183, episode reward: 149.629, mean reward: 0.399 [-11.896, 100.000], mean action: 2.403 [0.000, 3.000], mean observation: 0.120 [-0.687, 1.000], loss: 5.922923, mean_absolute_error: 51.640511, mean_q: 68.530365\n",
      " 254514/700000: episode: 653, duration: 2.207s, episode steps: 368, steps per second: 167, episode reward: 233.339, mean reward: 0.634 [-17.683, 100.000], mean action: 1.005 [0.000, 3.000], mean observation: 0.137 [-0.925, 1.000], loss: 8.298758, mean_absolute_error: 51.546749, mean_q: 68.532845\n",
      " 254824/700000: episode: 654, duration: 1.595s, episode steps: 310, steps per second: 194, episode reward: 185.596, mean reward: 0.599 [-8.865, 100.000], mean action: 1.071 [0.000, 3.000], mean observation: 0.108 [-1.056, 1.000], loss: 10.088850, mean_absolute_error: 51.357460, mean_q: 68.306297\n",
      " 255039/700000: episode: 655, duration: 1.082s, episode steps: 215, steps per second: 199, episode reward: 227.405, mean reward: 1.058 [-3.427, 100.000], mean action: 1.367 [0.000, 3.000], mean observation: 0.091 [-0.673, 1.000], loss: 8.064930, mean_absolute_error: 51.834366, mean_q: 69.047348\n",
      " 255359/700000: episode: 656, duration: 1.670s, episode steps: 320, steps per second: 192, episode reward: 208.354, mean reward: 0.651 [-18.801, 100.000], mean action: 1.194 [0.000, 3.000], mean observation: 0.063 [-0.752, 1.000], loss: 8.817865, mean_absolute_error: 51.661140, mean_q: 68.650475\n",
      " 255861/700000: episode: 657, duration: 2.895s, episode steps: 502, steps per second: 173, episode reward: -211.862, mean reward: -0.422 [-100.000, 14.174], mean action: 1.667 [0.000, 3.000], mean observation: 0.042 [-1.489, 1.402], loss: 12.603084, mean_absolute_error: 51.490284, mean_q: 68.419304\n",
      " 256253/700000: episode: 658, duration: 2.121s, episode steps: 392, steps per second: 185, episode reward: 230.984, mean reward: 0.589 [-11.367, 100.000], mean action: 0.898 [0.000, 3.000], mean observation: 0.131 [-1.362, 1.000], loss: 11.760833, mean_absolute_error: 51.020279, mean_q: 67.837654\n",
      " 256932/700000: episode: 659, duration: 3.945s, episode steps: 679, steps per second: 172, episode reward: 211.287, mean reward: 0.311 [-20.772, 100.000], mean action: 0.935 [0.000, 3.000], mean observation: 0.188 [-0.712, 1.000], loss: 11.421035, mean_absolute_error: 51.269703, mean_q: 67.979378\n",
      " 257181/700000: episode: 660, duration: 1.363s, episode steps: 249, steps per second: 183, episode reward: 244.990, mean reward: 0.984 [-17.625, 100.000], mean action: 1.225 [0.000, 3.000], mean observation: 0.062 [-0.765, 1.000], loss: 7.675573, mean_absolute_error: 50.997623, mean_q: 67.991356\n",
      " 257456/700000: episode: 661, duration: 1.397s, episode steps: 275, steps per second: 197, episode reward: 253.824, mean reward: 0.923 [-18.120, 100.000], mean action: 1.302 [0.000, 3.000], mean observation: 0.109 [-0.666, 1.000], loss: 9.493803, mean_absolute_error: 51.099533, mean_q: 67.928299\n",
      " 257760/700000: episode: 662, duration: 1.559s, episode steps: 304, steps per second: 195, episode reward: 238.722, mean reward: 0.785 [-11.479, 100.000], mean action: 0.970 [0.000, 3.000], mean observation: 0.155 [-0.816, 1.000], loss: 7.795251, mean_absolute_error: 50.992321, mean_q: 67.681511\n",
      " 258191/700000: episode: 663, duration: 2.323s, episode steps: 431, steps per second: 186, episode reward: 251.313, mean reward: 0.583 [-17.467, 100.000], mean action: 1.032 [0.000, 3.000], mean observation: 0.183 [-0.776, 1.000], loss: 9.110473, mean_absolute_error: 50.401688, mean_q: 66.926071\n",
      " 258889/700000: episode: 664, duration: 3.769s, episode steps: 698, steps per second: 185, episode reward: 113.601, mean reward: 0.163 [-20.871, 100.000], mean action: 1.358 [0.000, 3.000], mean observation: -0.004 [-0.761, 1.000], loss: 13.622405, mean_absolute_error: 50.624680, mean_q: 67.213737\n",
      " 259079/700000: episode: 665, duration: 0.951s, episode steps: 190, steps per second: 200, episode reward: 258.922, mean reward: 1.363 [-3.603, 100.000], mean action: 1.195 [0.000, 3.000], mean observation: 0.112 [-0.708, 1.231], loss: 7.874602, mean_absolute_error: 50.663128, mean_q: 67.355705\n",
      " 259414/700000: episode: 666, duration: 1.707s, episode steps: 335, steps per second: 196, episode reward: 221.850, mean reward: 0.662 [-17.845, 100.000], mean action: 0.672 [0.000, 3.000], mean observation: 0.173 [-0.968, 1.000], loss: 11.445796, mean_absolute_error: 50.704590, mean_q: 67.251724\n",
      " 259630/700000: episode: 667, duration: 1.091s, episode steps: 216, steps per second: 198, episode reward: 236.653, mean reward: 1.096 [-8.118, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.078 [-0.910, 1.000], loss: 8.005782, mean_absolute_error: 50.742805, mean_q: 67.506836\n",
      " 259916/700000: episode: 668, duration: 1.474s, episode steps: 286, steps per second: 194, episode reward: 166.052, mean reward: 0.581 [-17.155, 100.000], mean action: 1.136 [0.000, 3.000], mean observation: 0.154 [-0.923, 1.462], loss: 18.234846, mean_absolute_error: 50.178963, mean_q: 66.505013\n",
      " 260304/700000: episode: 669, duration: 2.048s, episode steps: 388, steps per second: 189, episode reward: 236.678, mean reward: 0.610 [-17.334, 100.000], mean action: 1.214 [0.000, 3.000], mean observation: 0.134 [-0.777, 1.000], loss: 7.700863, mean_absolute_error: 50.681770, mean_q: 67.412323\n",
      " 260581/700000: episode: 670, duration: 1.424s, episode steps: 277, steps per second: 195, episode reward: 196.825, mean reward: 0.711 [-10.386, 100.000], mean action: 1.758 [0.000, 3.000], mean observation: 0.084 [-1.042, 1.000], loss: 11.101796, mean_absolute_error: 50.309589, mean_q: 66.907578\n",
      " 261040/700000: episode: 671, duration: 2.514s, episode steps: 459, steps per second: 183, episode reward: -51.351, mean reward: -0.112 [-100.000, 17.330], mean action: 1.867 [0.000, 3.000], mean observation: 0.041 [-1.151, 1.727], loss: 14.572231, mean_absolute_error: 49.634174, mean_q: 65.929710\n",
      " 261395/700000: episode: 672, duration: 1.842s, episode steps: 355, steps per second: 193, episode reward: 208.664, mean reward: 0.588 [-18.498, 100.000], mean action: 1.115 [0.000, 3.000], mean observation: 0.146 [-1.541, 1.000], loss: 9.851896, mean_absolute_error: 50.004898, mean_q: 66.572868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 262104/700000: episode: 673, duration: 3.800s, episode steps: 709, steps per second: 187, episode reward: 224.520, mean reward: 0.317 [-17.785, 100.000], mean action: 1.020 [0.000, 3.000], mean observation: 0.156 [-0.930, 1.206], loss: 8.569420, mean_absolute_error: 49.878571, mean_q: 66.432907\n",
      " 262476/700000: episode: 674, duration: 1.921s, episode steps: 372, steps per second: 194, episode reward: 246.123, mean reward: 0.662 [-19.842, 100.000], mean action: 1.204 [0.000, 3.000], mean observation: 0.098 [-0.774, 1.000], loss: 8.155675, mean_absolute_error: 49.861778, mean_q: 66.296021\n",
      " 262776/700000: episode: 675, duration: 1.541s, episode steps: 300, steps per second: 195, episode reward: -75.453, mean reward: -0.252 [-100.000, 10.697], mean action: 1.743 [0.000, 3.000], mean observation: -0.007 [-0.979, 3.300], loss: 9.936695, mean_absolute_error: 49.597092, mean_q: 65.857620\n",
      " 262906/700000: episode: 676, duration: 0.662s, episode steps: 130, steps per second: 196, episode reward: -53.208, mean reward: -0.409 [-100.000, 8.712], mean action: 1.708 [0.000, 3.000], mean observation: -0.004 [-0.782, 1.332], loss: 13.074202, mean_absolute_error: 49.611805, mean_q: 65.836632\n",
      " 263237/700000: episode: 677, duration: 1.774s, episode steps: 331, steps per second: 187, episode reward: 234.068, mean reward: 0.707 [-14.464, 100.000], mean action: 1.556 [0.000, 3.000], mean observation: 0.050 [-1.480, 1.000], loss: 11.011378, mean_absolute_error: 49.622143, mean_q: 65.829445\n",
      " 263779/700000: episode: 678, duration: 2.902s, episode steps: 542, steps per second: 187, episode reward: 228.116, mean reward: 0.421 [-17.652, 100.000], mean action: 1.044 [0.000, 3.000], mean observation: 0.129 [-1.498, 1.000], loss: 10.150111, mean_absolute_error: 49.327702, mean_q: 65.602402\n",
      " 263966/700000: episode: 679, duration: 0.959s, episode steps: 187, steps per second: 195, episode reward: 230.022, mean reward: 1.230 [-7.663, 100.000], mean action: 1.417 [0.000, 3.000], mean observation: 0.090 [-0.851, 1.000], loss: 9.287086, mean_absolute_error: 49.574200, mean_q: 65.634254\n",
      " 264461/700000: episode: 680, duration: 2.645s, episode steps: 495, steps per second: 187, episode reward: 224.067, mean reward: 0.453 [-23.573, 100.000], mean action: 1.341 [0.000, 3.000], mean observation: 0.069 [-0.740, 1.000], loss: 9.079435, mean_absolute_error: 49.352951, mean_q: 65.259583\n",
      " 264694/700000: episode: 681, duration: 1.178s, episode steps: 233, steps per second: 198, episode reward: 226.425, mean reward: 0.972 [-9.345, 100.000], mean action: 1.185 [0.000, 3.000], mean observation: 0.119 [-0.818, 1.000], loss: 8.765040, mean_absolute_error: 49.158924, mean_q: 65.303535\n",
      " 265004/700000: episode: 682, duration: 1.618s, episode steps: 310, steps per second: 192, episode reward: 198.244, mean reward: 0.639 [-10.759, 100.000], mean action: 1.174 [0.000, 3.000], mean observation: 0.035 [-0.745, 1.000], loss: 16.063618, mean_absolute_error: 49.069077, mean_q: 65.124718\n",
      " 265626/700000: episode: 683, duration: 3.300s, episode steps: 622, steps per second: 188, episode reward: 217.311, mean reward: 0.349 [-18.488, 100.000], mean action: 0.720 [0.000, 3.000], mean observation: 0.169 [-0.858, 1.000], loss: 8.714273, mean_absolute_error: 49.189106, mean_q: 65.311653\n",
      " 265874/700000: episode: 684, duration: 1.270s, episode steps: 248, steps per second: 195, episode reward: 235.684, mean reward: 0.950 [-8.115, 100.000], mean action: 1.044 [0.000, 3.000], mean observation: 0.146 [-0.886, 1.000], loss: 6.619955, mean_absolute_error: 48.894661, mean_q: 65.007599\n",
      " 266133/700000: episode: 685, duration: 1.308s, episode steps: 259, steps per second: 198, episode reward: 202.478, mean reward: 0.782 [-19.071, 100.000], mean action: 1.012 [0.000, 3.000], mean observation: 0.133 [-0.877, 1.000], loss: 6.674836, mean_absolute_error: 49.142677, mean_q: 65.285309\n",
      " 266454/700000: episode: 686, duration: 1.655s, episode steps: 321, steps per second: 194, episode reward: 223.703, mean reward: 0.697 [-17.921, 100.000], mean action: 1.193 [0.000, 3.000], mean observation: 0.074 [-0.690, 1.000], loss: 7.270726, mean_absolute_error: 49.996738, mean_q: 66.483696\n",
      " 266628/700000: episode: 687, duration: 0.872s, episode steps: 174, steps per second: 200, episode reward: 203.267, mean reward: 1.168 [-11.303, 100.000], mean action: 1.374 [0.000, 3.000], mean observation: 0.023 [-0.770, 1.000], loss: 14.219839, mean_absolute_error: 49.467865, mean_q: 65.799538\n",
      " 267366/700000: episode: 688, duration: 4.100s, episode steps: 738, steps per second: 180, episode reward: 236.369, mean reward: 0.320 [-17.858, 100.000], mean action: 0.976 [0.000, 3.000], mean observation: 0.149 [-1.209, 1.000], loss: 8.571572, mean_absolute_error: 50.159077, mean_q: 66.650604\n",
      " 267870/700000: episode: 689, duration: 2.663s, episode steps: 504, steps per second: 189, episode reward: 229.826, mean reward: 0.456 [-12.353, 100.000], mean action: 1.310 [0.000, 3.000], mean observation: 0.081 [-1.221, 1.006], loss: 9.692096, mean_absolute_error: 50.219944, mean_q: 66.774284\n",
      " 268175/700000: episode: 690, duration: 1.550s, episode steps: 305, steps per second: 197, episode reward: 228.821, mean reward: 0.750 [-9.340, 100.000], mean action: 1.059 [0.000, 3.000], mean observation: 0.121 [-0.761, 1.000], loss: 9.149964, mean_absolute_error: 50.603168, mean_q: 67.191216\n",
      " 268430/700000: episode: 691, duration: 1.292s, episode steps: 255, steps per second: 197, episode reward: 186.741, mean reward: 0.732 [-19.703, 100.000], mean action: 1.349 [0.000, 3.000], mean observation: 0.115 [-0.838, 1.000], loss: 13.206051, mean_absolute_error: 50.511135, mean_q: 66.920097\n",
      " 268685/700000: episode: 692, duration: 1.306s, episode steps: 255, steps per second: 195, episode reward: 223.891, mean reward: 0.878 [-9.933, 100.000], mean action: 1.176 [0.000, 3.000], mean observation: 0.096 [-0.768, 1.000], loss: 10.179458, mean_absolute_error: 50.347832, mean_q: 66.681297\n",
      " 269113/700000: episode: 693, duration: 2.220s, episode steps: 428, steps per second: 193, episode reward: 202.406, mean reward: 0.473 [-9.679, 100.000], mean action: 1.477 [0.000, 3.000], mean observation: 0.015 [-1.634, 1.000], loss: 9.369358, mean_absolute_error: 50.381088, mean_q: 66.694946\n",
      " 269600/700000: episode: 694, duration: 2.629s, episode steps: 487, steps per second: 185, episode reward: 208.826, mean reward: 0.429 [-6.098, 100.000], mean action: 1.589 [0.000, 3.000], mean observation: 0.063 [-1.578, 1.016], loss: 17.685209, mean_absolute_error: 50.451126, mean_q: 67.079376\n",
      " 269928/700000: episode: 695, duration: 1.685s, episode steps: 328, steps per second: 195, episode reward: 224.752, mean reward: 0.685 [-17.337, 100.000], mean action: 0.921 [0.000, 3.000], mean observation: 0.099 [-0.608, 1.024], loss: 7.887895, mean_absolute_error: 50.326138, mean_q: 66.945961\n",
      " 270300/700000: episode: 696, duration: 1.990s, episode steps: 372, steps per second: 187, episode reward: 233.824, mean reward: 0.629 [-4.060, 100.000], mean action: 1.575 [0.000, 3.000], mean observation: 0.036 [-1.160, 1.000], loss: 9.982352, mean_absolute_error: 50.401390, mean_q: 67.227417\n",
      " 270557/700000: episode: 697, duration: 1.294s, episode steps: 257, steps per second: 199, episode reward: 234.777, mean reward: 0.914 [-7.978, 100.000], mean action: 1.035 [0.000, 3.000], mean observation: 0.099 [-0.607, 1.014], loss: 11.033766, mean_absolute_error: 50.688763, mean_q: 67.499985\n",
      " 270756/700000: episode: 698, duration: 0.982s, episode steps: 199, steps per second: 203, episode reward: 244.841, mean reward: 1.230 [-8.303, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.089 [-0.737, 1.003], loss: 9.468826, mean_absolute_error: 49.962673, mean_q: 66.560211\n",
      " 271260/700000: episode: 699, duration: 2.736s, episode steps: 504, steps per second: 184, episode reward: 217.022, mean reward: 0.431 [-17.682, 100.000], mean action: 1.302 [0.000, 3.000], mean observation: 0.131 [-0.711, 1.000], loss: 7.230150, mean_absolute_error: 50.066807, mean_q: 66.738625\n",
      " 271480/700000: episode: 700, duration: 1.138s, episode steps: 220, steps per second: 193, episode reward: -61.358, mean reward: -0.279 [-100.000, 12.206], mean action: 1.886 [0.000, 3.000], mean observation: 0.032 [-1.600, 1.000], loss: 10.010962, mean_absolute_error: 49.767967, mean_q: 66.295677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 271720/700000: episode: 701, duration: 1.216s, episode steps: 240, steps per second: 197, episode reward: 199.291, mean reward: 0.830 [-17.444, 100.000], mean action: 1.050 [0.000, 3.000], mean observation: 0.084 [-0.766, 1.000], loss: 7.333984, mean_absolute_error: 49.699261, mean_q: 65.821381\n",
      " 272282/700000: episode: 702, duration: 2.902s, episode steps: 562, steps per second: 194, episode reward: 220.091, mean reward: 0.392 [-19.707, 100.000], mean action: 1.062 [0.000, 3.000], mean observation: 0.198 [-0.796, 1.308], loss: 8.479438, mean_absolute_error: 49.907898, mean_q: 66.111771\n",
      " 272811/700000: episode: 703, duration: 2.749s, episode steps: 529, steps per second: 192, episode reward: 209.918, mean reward: 0.397 [-17.539, 100.000], mean action: 0.915 [0.000, 3.000], mean observation: 0.150 [-1.172, 1.000], loss: 10.873364, mean_absolute_error: 49.908676, mean_q: 66.168861\n",
      " 273426/700000: episode: 704, duration: 3.201s, episode steps: 615, steps per second: 192, episode reward: 265.567, mean reward: 0.432 [-20.173, 100.000], mean action: 0.779 [0.000, 3.000], mean observation: 0.199 [-0.960, 1.000], loss: 9.649387, mean_absolute_error: 49.815182, mean_q: 66.074471\n",
      " 273546/700000: episode: 705, duration: 0.605s, episode steps: 120, steps per second: 198, episode reward: -17.962, mean reward: -0.150 [-100.000, 9.662], mean action: 1.917 [0.000, 3.000], mean observation: 0.093 [-0.749, 1.350], loss: 6.256834, mean_absolute_error: 49.386848, mean_q: 65.874252\n",
      " 273885/700000: episode: 706, duration: 1.770s, episode steps: 339, steps per second: 192, episode reward: -180.944, mean reward: -0.534 [-100.000, 5.450], mean action: 1.737 [0.000, 3.000], mean observation: 0.078 [-0.891, 1.299], loss: 11.905454, mean_absolute_error: 49.921368, mean_q: 66.591248\n",
      " 274110/700000: episode: 707, duration: 1.130s, episode steps: 225, steps per second: 199, episode reward: 207.709, mean reward: 0.923 [-4.884, 100.000], mean action: 1.271 [0.000, 3.000], mean observation: 0.091 [-0.878, 1.000], loss: 11.577991, mean_absolute_error: 49.972511, mean_q: 66.232704\n",
      " 274215/700000: episode: 708, duration: 0.529s, episode steps: 105, steps per second: 198, episode reward: -82.065, mean reward: -0.782 [-100.000, 11.268], mean action: 1.686 [0.000, 3.000], mean observation: 0.138 [-0.826, 1.538], loss: 10.103909, mean_absolute_error: 50.295364, mean_q: 66.730644\n",
      " 274498/700000: episode: 709, duration: 1.480s, episode steps: 283, steps per second: 191, episode reward: 206.523, mean reward: 0.730 [-9.563, 100.000], mean action: 1.562 [0.000, 3.000], mean observation: 0.043 [-0.765, 1.000], loss: 11.884764, mean_absolute_error: 49.465519, mean_q: 65.827797\n",
      " 274835/700000: episode: 710, duration: 1.750s, episode steps: 337, steps per second: 193, episode reward: 239.237, mean reward: 0.710 [-13.260, 100.000], mean action: 1.582 [0.000, 3.000], mean observation: 0.111 [-1.269, 1.000], loss: 7.279192, mean_absolute_error: 49.645660, mean_q: 66.074089\n",
      " 275032/700000: episode: 711, duration: 0.984s, episode steps: 197, steps per second: 200, episode reward: 210.291, mean reward: 1.067 [-10.698, 100.000], mean action: 1.360 [0.000, 3.000], mean observation: 0.086 [-1.039, 1.000], loss: 10.362471, mean_absolute_error: 49.359890, mean_q: 65.547203\n",
      " 275164/700000: episode: 712, duration: 0.667s, episode steps: 132, steps per second: 198, episode reward: -2.542, mean reward: -0.019 [-100.000, 19.514], mean action: 1.909 [0.000, 3.000], mean observation: 0.091 [-0.860, 1.292], loss: 8.762688, mean_absolute_error: 49.742172, mean_q: 66.211411\n",
      " 275388/700000: episode: 713, duration: 1.138s, episode steps: 224, steps per second: 197, episode reward: 255.763, mean reward: 1.142 [-7.972, 100.000], mean action: 1.254 [0.000, 3.000], mean observation: 0.070 [-0.817, 1.000], loss: 22.561842, mean_absolute_error: 49.673592, mean_q: 65.855919\n",
      " 275808/700000: episode: 714, duration: 2.162s, episode steps: 420, steps per second: 194, episode reward: 205.864, mean reward: 0.490 [-22.573, 100.000], mean action: 1.314 [0.000, 3.000], mean observation: 0.079 [-0.907, 1.000], loss: 8.775146, mean_absolute_error: 49.832390, mean_q: 66.049683\n",
      " 276018/700000: episode: 715, duration: 1.061s, episode steps: 210, steps per second: 198, episode reward: 216.193, mean reward: 1.029 [-8.585, 100.000], mean action: 1.214 [0.000, 3.000], mean observation: 0.108 [-0.753, 1.045], loss: 7.315178, mean_absolute_error: 49.320553, mean_q: 65.454567\n",
      " 276362/700000: episode: 716, duration: 1.795s, episode steps: 344, steps per second: 192, episode reward: 214.020, mean reward: 0.622 [-3.526, 100.000], mean action: 1.488 [0.000, 3.000], mean observation: 0.040 [-0.811, 1.000], loss: 6.072734, mean_absolute_error: 49.634975, mean_q: 66.023499\n",
      " 276637/700000: episode: 717, duration: 1.410s, episode steps: 275, steps per second: 195, episode reward: 227.046, mean reward: 0.826 [-8.464, 100.000], mean action: 1.422 [0.000, 3.000], mean observation: 0.045 [-0.721, 1.000], loss: 10.993929, mean_absolute_error: 49.921959, mean_q: 66.338203\n",
      " 276785/700000: episode: 718, duration: 0.752s, episode steps: 148, steps per second: 197, episode reward: -83.365, mean reward: -0.563 [-100.000, 9.318], mean action: 1.716 [0.000, 3.000], mean observation: 0.121 [-1.742, 1.000], loss: 7.173971, mean_absolute_error: 50.206993, mean_q: 66.951782\n",
      " 277039/700000: episode: 719, duration: 1.292s, episode steps: 254, steps per second: 197, episode reward: -259.552, mean reward: -1.022 [-100.000, 6.402], mean action: 1.831 [0.000, 3.000], mean observation: 0.044 [-1.750, 3.148], loss: 5.364828, mean_absolute_error: 49.668194, mean_q: 66.157959\n",
      " 277247/700000: episode: 720, duration: 1.056s, episode steps: 208, steps per second: 197, episode reward: 232.878, mean reward: 1.120 [-19.355, 100.000], mean action: 1.337 [0.000, 3.000], mean observation: 0.092 [-0.687, 1.173], loss: 16.864458, mean_absolute_error: 50.286602, mean_q: 66.819298\n",
      " 277641/700000: episode: 721, duration: 2.101s, episode steps: 394, steps per second: 188, episode reward: 262.335, mean reward: 0.666 [-18.199, 100.000], mean action: 1.157 [0.000, 3.000], mean observation: 0.138 [-0.824, 1.000], loss: 7.848165, mean_absolute_error: 50.164986, mean_q: 66.566978\n",
      " 277885/700000: episode: 722, duration: 1.236s, episode steps: 244, steps per second: 197, episode reward: 195.836, mean reward: 0.803 [-9.395, 100.000], mean action: 1.078 [0.000, 3.000], mean observation: 0.106 [-1.018, 1.000], loss: 8.262600, mean_absolute_error: 49.724476, mean_q: 66.218895\n",
      " 278825/700000: episode: 723, duration: 5.531s, episode steps: 940, steps per second: 170, episode reward: -309.872, mean reward: -0.330 [-100.000, 15.091], mean action: 1.923 [0.000, 3.000], mean observation: -0.003 [-1.053, 1.000], loss: 10.454930, mean_absolute_error: 50.114029, mean_q: 66.668175\n",
      " 279190/700000: episode: 724, duration: 1.928s, episode steps: 365, steps per second: 189, episode reward: 228.949, mean reward: 0.627 [-19.132, 100.000], mean action: 0.792 [0.000, 3.000], mean observation: 0.152 [-0.750, 1.000], loss: 8.412467, mean_absolute_error: 50.064674, mean_q: 66.831284\n",
      " 279495/700000: episode: 725, duration: 1.763s, episode steps: 305, steps per second: 173, episode reward: 185.148, mean reward: 0.607 [-17.971, 100.000], mean action: 1.292 [0.000, 3.000], mean observation: 0.091 [-0.689, 1.000], loss: 12.417223, mean_absolute_error: 49.861423, mean_q: 66.244774\n",
      " 279688/700000: episode: 726, duration: 1.008s, episode steps: 193, steps per second: 192, episode reward: 212.658, mean reward: 1.102 [-7.852, 100.000], mean action: 1.316 [0.000, 3.000], mean observation: 0.065 [-0.669, 1.000], loss: 7.144785, mean_absolute_error: 50.353638, mean_q: 67.023460\n",
      " 280036/700000: episode: 727, duration: 1.900s, episode steps: 348, steps per second: 183, episode reward: 260.400, mean reward: 0.748 [-10.497, 100.000], mean action: 1.095 [0.000, 3.000], mean observation: 0.158 [-0.688, 1.025], loss: 9.185818, mean_absolute_error: 50.168823, mean_q: 66.967819\n",
      " 280299/700000: episode: 728, duration: 1.337s, episode steps: 263, steps per second: 197, episode reward: 197.110, mean reward: 0.749 [-9.025, 100.000], mean action: 1.255 [0.000, 3.000], mean observation: 0.096 [-0.937, 1.000], loss: 14.398795, mean_absolute_error: 50.610184, mean_q: 67.350937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 280922/700000: episode: 729, duration: 3.291s, episode steps: 623, steps per second: 189, episode reward: 227.807, mean reward: 0.366 [-20.171, 100.000], mean action: 1.289 [0.000, 3.000], mean observation: 0.042 [-0.808, 1.006], loss: 9.455032, mean_absolute_error: 50.168423, mean_q: 66.709961\n",
      " 281082/700000: episode: 730, duration: 0.809s, episode steps: 160, steps per second: 198, episode reward: -30.043, mean reward: -0.188 [-100.000, 15.759], mean action: 1.494 [0.000, 3.000], mean observation: 0.025 [-0.933, 1.663], loss: 6.177604, mean_absolute_error: 49.973869, mean_q: 66.565437\n",
      " 281443/700000: episode: 731, duration: 1.912s, episode steps: 361, steps per second: 189, episode reward: 217.948, mean reward: 0.604 [-3.295, 100.000], mean action: 1.632 [0.000, 3.000], mean observation: -0.010 [-0.921, 1.000], loss: 8.155828, mean_absolute_error: 50.466591, mean_q: 67.162941\n",
      " 281674/700000: episode: 732, duration: 1.171s, episode steps: 231, steps per second: 197, episode reward: 228.519, mean reward: 0.989 [-7.015, 100.000], mean action: 1.238 [0.000, 3.000], mean observation: 0.092 [-0.662, 1.022], loss: 9.095948, mean_absolute_error: 50.135494, mean_q: 66.642059\n",
      " 282211/700000: episode: 733, duration: 2.990s, episode steps: 537, steps per second: 180, episode reward: -154.169, mean reward: -0.287 [-100.000, 8.452], mean action: 1.836 [0.000, 3.000], mean observation: -0.060 [-1.000, 1.000], loss: 11.447508, mean_absolute_error: 50.636799, mean_q: 67.446350\n",
      " 282540/700000: episode: 734, duration: 1.692s, episode steps: 329, steps per second: 194, episode reward: 215.077, mean reward: 0.654 [-17.542, 100.000], mean action: 1.067 [0.000, 3.000], mean observation: 0.124 [-0.734, 1.045], loss: 9.023295, mean_absolute_error: 50.017406, mean_q: 66.510178\n",
      " 282765/700000: episode: 735, duration: 1.144s, episode steps: 225, steps per second: 197, episode reward: 184.730, mean reward: 0.821 [-13.345, 100.000], mean action: 1.507 [0.000, 3.000], mean observation: 0.026 [-1.235, 1.000], loss: 11.693485, mean_absolute_error: 50.077473, mean_q: 66.358772\n",
      " 283099/700000: episode: 736, duration: 1.744s, episode steps: 334, steps per second: 191, episode reward: 203.942, mean reward: 0.611 [-17.231, 100.000], mean action: 1.323 [0.000, 3.000], mean observation: 0.098 [-0.765, 1.000], loss: 9.281741, mean_absolute_error: 50.341915, mean_q: 66.736526\n",
      " 283333/700000: episode: 737, duration: 1.182s, episode steps: 234, steps per second: 198, episode reward: 208.489, mean reward: 0.891 [-19.186, 100.000], mean action: 1.312 [0.000, 3.000], mean observation: 0.075 [-0.690, 1.000], loss: 11.655170, mean_absolute_error: 49.972908, mean_q: 66.567612\n",
      " 283582/700000: episode: 738, duration: 1.266s, episode steps: 249, steps per second: 197, episode reward: 223.973, mean reward: 0.899 [-7.542, 100.000], mean action: 1.414 [0.000, 3.000], mean observation: 0.087 [-0.713, 1.095], loss: 9.310337, mean_absolute_error: 50.379848, mean_q: 67.018295\n",
      " 283930/700000: episode: 739, duration: 1.814s, episode steps: 348, steps per second: 192, episode reward: 203.022, mean reward: 0.583 [-20.194, 100.000], mean action: 0.963 [0.000, 3.000], mean observation: 0.143 [-0.773, 1.000], loss: 6.985664, mean_absolute_error: 50.290150, mean_q: 66.500542\n",
      " 284372/700000: episode: 740, duration: 2.406s, episode steps: 442, steps per second: 184, episode reward: 211.625, mean reward: 0.479 [-18.226, 100.000], mean action: 1.170 [0.000, 3.000], mean observation: 0.025 [-0.657, 1.000], loss: 10.407017, mean_absolute_error: 50.300560, mean_q: 66.583633\n",
      " 284743/700000: episode: 741, duration: 1.990s, episode steps: 371, steps per second: 186, episode reward: 238.944, mean reward: 0.644 [-8.292, 100.000], mean action: 1.499 [0.000, 3.000], mean observation: 0.084 [-1.204, 1.000], loss: 8.242838, mean_absolute_error: 50.120667, mean_q: 66.686966\n",
      " 284981/700000: episode: 742, duration: 1.212s, episode steps: 238, steps per second: 196, episode reward: 183.933, mean reward: 0.773 [-10.668, 100.000], mean action: 1.319 [0.000, 3.000], mean observation: 0.046 [-0.616, 1.000], loss: 9.784972, mean_absolute_error: 49.858486, mean_q: 66.293373\n",
      " 285502/700000: episode: 743, duration: 2.784s, episode steps: 521, steps per second: 187, episode reward: 213.023, mean reward: 0.409 [-6.474, 100.000], mean action: 1.685 [0.000, 3.000], mean observation: 0.077 [-0.974, 1.374], loss: 10.086951, mean_absolute_error: 50.025028, mean_q: 66.565102\n",
      " 285964/700000: episode: 744, duration: 2.458s, episode steps: 462, steps per second: 188, episode reward: 199.360, mean reward: 0.432 [-18.128, 100.000], mean action: 2.022 [0.000, 3.000], mean observation: 0.135 [-0.625, 1.000], loss: 9.326542, mean_absolute_error: 49.988461, mean_q: 66.788979\n",
      " 286964/700000: episode: 745, duration: 5.775s, episode steps: 1000, steps per second: 173, episode reward: 53.415, mean reward: 0.053 [-19.174, 15.178], mean action: 1.327 [0.000, 3.000], mean observation: 0.171 [-0.677, 1.000], loss: 8.480103, mean_absolute_error: 50.057461, mean_q: 66.409813\n",
      " 287188/700000: episode: 746, duration: 1.132s, episode steps: 224, steps per second: 198, episode reward: 223.706, mean reward: 0.999 [-12.851, 100.000], mean action: 1.826 [0.000, 3.000], mean observation: 0.115 [-1.769, 1.000], loss: 11.076737, mean_absolute_error: 50.249901, mean_q: 66.843307\n",
      " 287521/700000: episode: 747, duration: 1.726s, episode steps: 333, steps per second: 193, episode reward: 210.808, mean reward: 0.633 [-9.547, 100.000], mean action: 1.141 [0.000, 3.000], mean observation: 0.126 [-0.863, 1.078], loss: 11.064178, mean_absolute_error: 50.462017, mean_q: 66.856270\n",
      " 287826/700000: episode: 748, duration: 1.565s, episode steps: 305, steps per second: 195, episode reward: 248.250, mean reward: 0.814 [-3.890, 100.000], mean action: 1.170 [0.000, 3.000], mean observation: 0.057 [-0.812, 1.000], loss: 11.901097, mean_absolute_error: 50.709152, mean_q: 67.314690\n",
      " 288036/700000: episode: 749, duration: 1.064s, episode steps: 210, steps per second: 197, episode reward: 198.277, mean reward: 0.944 [-3.312, 100.000], mean action: 1.343 [0.000, 3.000], mean observation: 0.101 [-0.884, 1.000], loss: 9.984010, mean_absolute_error: 50.157707, mean_q: 66.608772\n",
      " 288264/700000: episode: 750, duration: 1.143s, episode steps: 228, steps per second: 199, episode reward: 213.346, mean reward: 0.936 [-17.654, 100.000], mean action: 0.921 [0.000, 3.000], mean observation: 0.126 [-0.946, 1.000], loss: 6.798251, mean_absolute_error: 50.317425, mean_q: 66.976341\n",
      " 288452/700000: episode: 751, duration: 0.952s, episode steps: 188, steps per second: 197, episode reward: -30.230, mean reward: -0.161 [-100.000, 26.821], mean action: 1.644 [0.000, 3.000], mean observation: 0.064 [-0.734, 1.000], loss: 7.741956, mean_absolute_error: 50.201328, mean_q: 66.686287\n",
      " 288699/700000: episode: 752, duration: 1.239s, episode steps: 247, steps per second: 199, episode reward: 238.414, mean reward: 0.965 [-10.986, 100.000], mean action: 1.040 [0.000, 3.000], mean observation: 0.168 [-0.882, 1.000], loss: 13.095595, mean_absolute_error: 50.265099, mean_q: 66.764175\n",
      " 289008/700000: episode: 753, duration: 1.564s, episode steps: 309, steps per second: 198, episode reward: 257.279, mean reward: 0.833 [-17.353, 100.000], mean action: 0.987 [0.000, 3.000], mean observation: 0.163 [-1.026, 1.000], loss: 6.602066, mean_absolute_error: 50.457424, mean_q: 66.996132\n",
      " 289185/700000: episode: 754, duration: 0.876s, episode steps: 177, steps per second: 202, episode reward: 224.727, mean reward: 1.270 [-8.264, 100.000], mean action: 0.881 [0.000, 3.000], mean observation: 0.100 [-0.985, 1.000], loss: 12.229150, mean_absolute_error: 50.250717, mean_q: 66.317390\n",
      " 289444/700000: episode: 755, duration: 1.324s, episode steps: 259, steps per second: 196, episode reward: 182.898, mean reward: 0.706 [-12.715, 100.000], mean action: 1.317 [0.000, 3.000], mean observation: 0.093 [-0.864, 1.000], loss: 9.100213, mean_absolute_error: 50.178459, mean_q: 66.400459\n",
      " 289771/700000: episode: 756, duration: 1.700s, episode steps: 327, steps per second: 192, episode reward: 227.967, mean reward: 0.697 [-10.411, 100.000], mean action: 1.242 [0.000, 3.000], mean observation: 0.123 [-0.741, 1.000], loss: 6.435852, mean_absolute_error: 50.227375, mean_q: 66.785995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 290178/700000: episode: 757, duration: 2.173s, episode steps: 407, steps per second: 187, episode reward: 234.484, mean reward: 0.576 [-16.923, 100.000], mean action: 1.263 [0.000, 3.000], mean observation: 0.184 [-0.871, 1.000], loss: 6.951761, mean_absolute_error: 49.974777, mean_q: 66.322723\n",
      " 290388/700000: episode: 758, duration: 1.056s, episode steps: 210, steps per second: 199, episode reward: 212.433, mean reward: 1.012 [-9.833, 100.000], mean action: 1.371 [0.000, 3.000], mean observation: 0.071 [-0.671, 1.000], loss: 8.657633, mean_absolute_error: 50.168545, mean_q: 66.678253\n",
      " 290613/700000: episode: 759, duration: 1.118s, episode steps: 225, steps per second: 201, episode reward: 219.707, mean reward: 0.976 [-3.074, 100.000], mean action: 0.956 [0.000, 3.000], mean observation: 0.102 [-0.823, 1.000], loss: 8.771860, mean_absolute_error: 50.400356, mean_q: 67.175339\n",
      " 291045/700000: episode: 760, duration: 2.301s, episode steps: 432, steps per second: 188, episode reward: 239.522, mean reward: 0.554 [-19.328, 100.000], mean action: 1.433 [0.000, 3.000], mean observation: 0.058 [-1.012, 1.000], loss: 11.051722, mean_absolute_error: 50.286839, mean_q: 66.879242\n",
      " 291286/700000: episode: 761, duration: 1.221s, episode steps: 241, steps per second: 197, episode reward: 211.669, mean reward: 0.878 [-4.050, 100.000], mean action: 0.959 [0.000, 3.000], mean observation: 0.122 [-0.854, 1.000], loss: 10.211958, mean_absolute_error: 50.579586, mean_q: 67.086792\n",
      " 291645/700000: episode: 762, duration: 1.866s, episode steps: 359, steps per second: 192, episode reward: 232.386, mean reward: 0.647 [-10.409, 100.000], mean action: 1.393 [0.000, 3.000], mean observation: 0.074 [-0.953, 1.000], loss: 10.051586, mean_absolute_error: 50.246449, mean_q: 66.814148\n",
      " 291827/700000: episode: 763, duration: 0.911s, episode steps: 182, steps per second: 200, episode reward: 193.690, mean reward: 1.064 [-9.364, 100.000], mean action: 1.286 [0.000, 3.000], mean observation: 0.032 [-0.736, 1.000], loss: 11.571862, mean_absolute_error: 50.109634, mean_q: 66.539925\n",
      " 292043/700000: episode: 764, duration: 1.091s, episode steps: 216, steps per second: 198, episode reward: 216.036, mean reward: 1.000 [-8.616, 100.000], mean action: 1.269 [0.000, 3.000], mean observation: 0.083 [-0.926, 1.000], loss: 6.170406, mean_absolute_error: 49.957798, mean_q: 66.482483\n",
      " 292738/700000: episode: 765, duration: 3.834s, episode steps: 695, steps per second: 181, episode reward: 167.273, mean reward: 0.241 [-19.117, 100.000], mean action: 1.308 [0.000, 3.000], mean observation: 0.150 [-0.687, 1.000], loss: 7.230350, mean_absolute_error: 49.931698, mean_q: 66.367020\n",
      " 293068/700000: episode: 766, duration: 1.704s, episode steps: 330, steps per second: 194, episode reward: 196.273, mean reward: 0.595 [-8.091, 100.000], mean action: 1.039 [0.000, 3.000], mean observation: 0.148 [-1.140, 1.000], loss: 8.899923, mean_absolute_error: 50.069054, mean_q: 66.516121\n",
      " 293409/700000: episode: 767, duration: 1.775s, episode steps: 341, steps per second: 192, episode reward: 237.306, mean reward: 0.696 [-17.505, 100.000], mean action: 1.279 [0.000, 3.000], mean observation: 0.104 [-1.035, 1.000], loss: 7.180811, mean_absolute_error: 50.053062, mean_q: 66.390732\n",
      " 293585/700000: episode: 768, duration: 0.901s, episode steps: 176, steps per second: 195, episode reward: 208.045, mean reward: 1.182 [-6.239, 100.000], mean action: 2.006 [0.000, 3.000], mean observation: 0.153 [-0.825, 1.000], loss: 7.553551, mean_absolute_error: 49.798244, mean_q: 66.121346\n",
      " 293860/700000: episode: 769, duration: 1.417s, episode steps: 275, steps per second: 194, episode reward: 240.802, mean reward: 0.876 [-17.458, 100.000], mean action: 1.247 [0.000, 3.000], mean observation: 0.135 [-0.878, 1.000], loss: 10.177984, mean_absolute_error: 49.774780, mean_q: 66.249702\n",
      " 294495/700000: episode: 770, duration: 3.524s, episode steps: 635, steps per second: 180, episode reward: 167.670, mean reward: 0.264 [-20.800, 100.000], mean action: 2.002 [0.000, 3.000], mean observation: 0.162 [-0.705, 1.000], loss: 8.353353, mean_absolute_error: 49.493355, mean_q: 65.784317\n",
      " 294848/700000: episode: 771, duration: 1.846s, episode steps: 353, steps per second: 191, episode reward: 236.917, mean reward: 0.671 [-17.332, 100.000], mean action: 1.136 [0.000, 3.000], mean observation: 0.090 [-0.776, 1.000], loss: 5.314893, mean_absolute_error: 49.854046, mean_q: 66.429634\n",
      " 295142/700000: episode: 772, duration: 1.544s, episode steps: 294, steps per second: 190, episode reward: 255.547, mean reward: 0.869 [-10.349, 100.000], mean action: 1.486 [0.000, 3.000], mean observation: 0.097 [-0.816, 1.000], loss: 7.240750, mean_absolute_error: 49.863728, mean_q: 66.155640\n",
      " 295488/700000: episode: 773, duration: 1.789s, episode steps: 346, steps per second: 193, episode reward: 223.766, mean reward: 0.647 [-10.171, 100.000], mean action: 1.480 [0.000, 3.000], mean observation: 0.063 [-1.596, 1.000], loss: 12.257972, mean_absolute_error: 49.817951, mean_q: 66.209145\n",
      " 295574/700000: episode: 774, duration: 0.439s, episode steps: 86, steps per second: 196, episode reward: -98.917, mean reward: -1.150 [-100.000, 25.863], mean action: 1.872 [0.000, 3.000], mean observation: -0.137 [-1.015, 4.116], loss: 18.020428, mean_absolute_error: 49.926571, mean_q: 66.276749\n",
      " 295956/700000: episode: 775, duration: 2.733s, episode steps: 382, steps per second: 140, episode reward: 239.574, mean reward: 0.627 [-19.777, 100.000], mean action: 0.995 [0.000, 3.000], mean observation: 0.139 [-0.775, 1.000], loss: 11.386681, mean_absolute_error: 49.168526, mean_q: 65.363182\n",
      " 296185/700000: episode: 776, duration: 1.428s, episode steps: 229, steps per second: 160, episode reward: 210.997, mean reward: 0.921 [-19.931, 100.000], mean action: 1.424 [0.000, 3.000], mean observation: 0.095 [-1.029, 1.000], loss: 8.949533, mean_absolute_error: 48.855572, mean_q: 64.942253\n",
      " 296522/700000: episode: 777, duration: 2.502s, episode steps: 337, steps per second: 135, episode reward: 214.078, mean reward: 0.635 [-9.497, 100.000], mean action: 1.401 [0.000, 3.000], mean observation: 0.037 [-0.721, 1.000], loss: 8.063894, mean_absolute_error: 48.669964, mean_q: 64.502502\n",
      " 296769/700000: episode: 778, duration: 1.785s, episode steps: 247, steps per second: 138, episode reward: 215.750, mean reward: 0.873 [-3.590, 100.000], mean action: 1.538 [0.000, 3.000], mean observation: 0.026 [-0.581, 1.000], loss: 8.400724, mean_absolute_error: 48.537376, mean_q: 64.614388\n",
      " 297177/700000: episode: 779, duration: 2.430s, episode steps: 408, steps per second: 168, episode reward: 218.366, mean reward: 0.535 [-12.842, 100.000], mean action: 0.843 [0.000, 3.000], mean observation: 0.093 [-0.726, 1.000], loss: 12.121578, mean_absolute_error: 48.902977, mean_q: 65.127724\n",
      " 297596/700000: episode: 780, duration: 2.166s, episode steps: 419, steps per second: 193, episode reward: 215.364, mean reward: 0.514 [-20.985, 100.000], mean action: 1.043 [0.000, 3.000], mean observation: 0.089 [-0.830, 1.000], loss: 9.766957, mean_absolute_error: 48.612526, mean_q: 64.668526\n",
      " 298035/700000: episode: 781, duration: 2.319s, episode steps: 439, steps per second: 189, episode reward: 231.197, mean reward: 0.527 [-17.816, 100.000], mean action: 0.945 [0.000, 3.000], mean observation: 0.156 [-0.801, 1.000], loss: 9.291623, mean_absolute_error: 48.833199, mean_q: 64.984268\n",
      " 298322/700000: episode: 782, duration: 1.494s, episode steps: 287, steps per second: 192, episode reward: 236.840, mean reward: 0.825 [-10.911, 100.000], mean action: 1.056 [0.000, 3.000], mean observation: 0.078 [-0.837, 1.000], loss: 9.336524, mean_absolute_error: 48.290001, mean_q: 64.251541\n",
      " 298579/700000: episode: 783, duration: 1.317s, episode steps: 257, steps per second: 195, episode reward: 198.395, mean reward: 0.772 [-19.204, 100.000], mean action: 1.307 [0.000, 3.000], mean observation: 0.132 [-0.873, 1.000], loss: 9.317520, mean_absolute_error: 48.870987, mean_q: 64.949280\n",
      " 298792/700000: episode: 784, duration: 1.068s, episode steps: 213, steps per second: 199, episode reward: 219.204, mean reward: 1.029 [-4.120, 100.000], mean action: 1.254 [0.000, 3.000], mean observation: 0.093 [-0.923, 1.000], loss: 9.071897, mean_absolute_error: 48.612785, mean_q: 64.737152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 299167/700000: episode: 785, duration: 1.968s, episode steps: 375, steps per second: 191, episode reward: 200.520, mean reward: 0.535 [-19.195, 100.000], mean action: 1.027 [0.000, 3.000], mean observation: 0.094 [-0.814, 1.000], loss: 13.713289, mean_absolute_error: 48.901302, mean_q: 64.936279\n",
      " 299648/700000: episode: 786, duration: 2.535s, episode steps: 481, steps per second: 190, episode reward: 237.099, mean reward: 0.493 [-18.253, 100.000], mean action: 1.195 [0.000, 3.000], mean observation: 0.075 [-0.861, 1.000], loss: 7.381178, mean_absolute_error: 48.679119, mean_q: 64.786720\n",
      " 300036/700000: episode: 787, duration: 2.018s, episode steps: 388, steps per second: 192, episode reward: 231.338, mean reward: 0.596 [-17.738, 100.000], mean action: 1.098 [0.000, 3.000], mean observation: 0.148 [-0.823, 1.000], loss: 5.760766, mean_absolute_error: 49.024410, mean_q: 65.132523\n",
      " 300611/700000: episode: 788, duration: 2.981s, episode steps: 575, steps per second: 193, episode reward: 237.116, mean reward: 0.412 [-22.902, 100.000], mean action: 0.772 [0.000, 3.000], mean observation: 0.157 [-1.235, 1.000], loss: 8.059599, mean_absolute_error: 49.193230, mean_q: 65.336639\n",
      " 301376/700000: episode: 789, duration: 4.517s, episode steps: 765, steps per second: 169, episode reward: 170.543, mean reward: 0.223 [-20.014, 100.000], mean action: 0.878 [0.000, 3.000], mean observation: 0.215 [-1.308, 1.000], loss: 6.709764, mean_absolute_error: 49.515636, mean_q: 65.687073\n",
      " 301991/700000: episode: 790, duration: 3.817s, episode steps: 615, steps per second: 161, episode reward: 171.192, mean reward: 0.278 [-18.528, 100.000], mean action: 0.907 [0.000, 3.000], mean observation: 0.118 [-0.767, 1.000], loss: 12.316094, mean_absolute_error: 49.298782, mean_q: 65.274109\n",
      " 302455/700000: episode: 791, duration: 2.487s, episode steps: 464, steps per second: 187, episode reward: 230.022, mean reward: 0.496 [-19.148, 100.000], mean action: 1.399 [0.000, 3.000], mean observation: 0.053 [-0.818, 1.017], loss: 10.157042, mean_absolute_error: 49.201836, mean_q: 65.077538\n",
      " 302736/700000: episode: 792, duration: 1.497s, episode steps: 281, steps per second: 188, episode reward: 214.647, mean reward: 0.764 [-9.388, 100.000], mean action: 1.256 [0.000, 3.000], mean observation: 0.045 [-1.010, 1.000], loss: 5.216086, mean_absolute_error: 49.435215, mean_q: 65.325089\n",
      " 302967/700000: episode: 793, duration: 1.175s, episode steps: 231, steps per second: 197, episode reward: 240.690, mean reward: 1.042 [-14.080, 100.000], mean action: 1.316 [0.000, 3.000], mean observation: 0.149 [-1.075, 1.314], loss: 6.864712, mean_absolute_error: 48.742786, mean_q: 64.486610\n",
      " 303419/700000: episode: 794, duration: 2.393s, episode steps: 452, steps per second: 189, episode reward: 232.437, mean reward: 0.514 [-17.498, 100.000], mean action: 1.164 [0.000, 3.000], mean observation: 0.084 [-0.770, 1.000], loss: 7.145501, mean_absolute_error: 49.061127, mean_q: 65.146362\n",
      " 303649/700000: episode: 795, duration: 1.159s, episode steps: 230, steps per second: 198, episode reward: 192.602, mean reward: 0.837 [-9.152, 100.000], mean action: 1.109 [0.000, 3.000], mean observation: 0.114 [-0.836, 1.000], loss: 8.965970, mean_absolute_error: 48.784042, mean_q: 65.002258\n",
      " 303955/700000: episode: 796, duration: 1.581s, episode steps: 306, steps per second: 193, episode reward: 230.445, mean reward: 0.753 [-4.917, 100.000], mean action: 1.503 [0.000, 3.000], mean observation: 0.050 [-0.959, 1.047], loss: 9.480335, mean_absolute_error: 49.083096, mean_q: 64.995468\n",
      " 304461/700000: episode: 797, duration: 2.684s, episode steps: 506, steps per second: 189, episode reward: 228.683, mean reward: 0.452 [-19.301, 100.000], mean action: 0.984 [0.000, 3.000], mean observation: 0.107 [-0.782, 1.000], loss: 8.138060, mean_absolute_error: 49.048904, mean_q: 65.297318\n",
      " 304617/700000: episode: 798, duration: 0.796s, episode steps: 156, steps per second: 196, episode reward: 17.449, mean reward: 0.112 [-100.000, 14.965], mean action: 1.788 [0.000, 3.000], mean observation: 0.099 [-1.054, 1.000], loss: 12.521391, mean_absolute_error: 48.859299, mean_q: 64.705444\n",
      " 305389/700000: episode: 799, duration: 4.279s, episode steps: 772, steps per second: 180, episode reward: 150.417, mean reward: 0.195 [-18.809, 100.000], mean action: 1.108 [0.000, 3.000], mean observation: 0.179 [-0.993, 1.003], loss: 8.927885, mean_absolute_error: 49.341171, mean_q: 65.823235\n",
      " 305623/700000: episode: 800, duration: 1.172s, episode steps: 234, steps per second: 200, episode reward: 257.795, mean reward: 1.102 [-10.791, 100.000], mean action: 0.919 [0.000, 3.000], mean observation: 0.057 [-0.863, 1.000], loss: 18.311050, mean_absolute_error: 49.262428, mean_q: 65.610802\n",
      " 305850/700000: episode: 801, duration: 1.157s, episode steps: 227, steps per second: 196, episode reward: 236.050, mean reward: 1.040 [-11.396, 100.000], mean action: 1.304 [0.000, 3.000], mean observation: 0.112 [-0.825, 1.000], loss: 11.693236, mean_absolute_error: 49.439617, mean_q: 66.030022\n",
      " 306850/700000: episode: 802, duration: 6.183s, episode steps: 1000, steps per second: 162, episode reward: -56.792, mean reward: -0.057 [-18.582, 20.896], mean action: 1.854 [0.000, 3.000], mean observation: 0.015 [-0.948, 1.011], loss: 10.313295, mean_absolute_error: 49.081306, mean_q: 65.311630\n",
      " 307213/700000: episode: 803, duration: 1.869s, episode steps: 363, steps per second: 194, episode reward: 206.242, mean reward: 0.568 [-19.041, 100.000], mean action: 0.964 [0.000, 3.000], mean observation: 0.141 [-0.871, 1.290], loss: 10.419038, mean_absolute_error: 48.657078, mean_q: 64.484711\n",
      " 307431/700000: episode: 804, duration: 1.112s, episode steps: 218, steps per second: 196, episode reward: 206.426, mean reward: 0.947 [-10.338, 100.000], mean action: 1.436 [0.000, 3.000], mean observation: 0.086 [-0.805, 1.000], loss: 8.276628, mean_absolute_error: 48.688316, mean_q: 64.656227\n",
      " 307921/700000: episode: 805, duration: 2.527s, episode steps: 490, steps per second: 194, episode reward: 229.905, mean reward: 0.469 [-17.340, 100.000], mean action: 1.165 [0.000, 3.000], mean observation: 0.079 [-1.023, 1.000], loss: 12.624471, mean_absolute_error: 48.402058, mean_q: 64.203590\n",
      " 308260/700000: episode: 806, duration: 1.768s, episode steps: 339, steps per second: 192, episode reward: 196.581, mean reward: 0.580 [-10.078, 100.000], mean action: 1.204 [0.000, 3.000], mean observation: 0.105 [-1.192, 1.000], loss: 6.783062, mean_absolute_error: 48.658138, mean_q: 64.834969\n",
      " 308486/700000: episode: 807, duration: 1.140s, episode steps: 226, steps per second: 198, episode reward: 224.037, mean reward: 0.991 [-17.929, 100.000], mean action: 1.226 [0.000, 3.000], mean observation: 0.103 [-0.914, 1.000], loss: 8.848380, mean_absolute_error: 48.669498, mean_q: 64.696075\n",
      " 308729/700000: episode: 808, duration: 1.240s, episode steps: 243, steps per second: 196, episode reward: 218.086, mean reward: 0.897 [-8.409, 100.000], mean action: 1.218 [0.000, 3.000], mean observation: 0.080 [-1.060, 1.000], loss: 8.497721, mean_absolute_error: 48.541843, mean_q: 64.413467\n",
      " 308960/700000: episode: 809, duration: 1.168s, episode steps: 231, steps per second: 198, episode reward: 229.880, mean reward: 0.995 [-4.851, 100.000], mean action: 1.398 [0.000, 3.000], mean observation: 0.050 [-0.912, 1.000], loss: 7.057932, mean_absolute_error: 48.873623, mean_q: 64.900909\n",
      " 309157/700000: episode: 810, duration: 0.990s, episode steps: 197, steps per second: 199, episode reward: 194.515, mean reward: 0.987 [-3.584, 100.000], mean action: 1.320 [0.000, 3.000], mean observation: 0.065 [-0.809, 1.000], loss: 8.198450, mean_absolute_error: 48.413944, mean_q: 64.434586\n",
      " 309470/700000: episode: 811, duration: 1.601s, episode steps: 313, steps per second: 196, episode reward: 233.924, mean reward: 0.747 [-17.435, 100.000], mean action: 1.089 [0.000, 3.000], mean observation: 0.146 [-0.808, 1.000], loss: 6.332058, mean_absolute_error: 47.976398, mean_q: 63.824780\n",
      " 309773/700000: episode: 812, duration: 1.551s, episode steps: 303, steps per second: 195, episode reward: 227.051, mean reward: 0.749 [-17.782, 100.000], mean action: 1.046 [0.000, 3.000], mean observation: 0.117 [-1.015, 1.000], loss: 10.695957, mean_absolute_error: 48.771240, mean_q: 64.811897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 310076/700000: episode: 813, duration: 1.557s, episode steps: 303, steps per second: 195, episode reward: 230.178, mean reward: 0.760 [-20.343, 100.000], mean action: 1.165 [0.000, 3.000], mean observation: 0.067 [-1.101, 1.000], loss: 9.716264, mean_absolute_error: 48.571899, mean_q: 64.726166\n",
      " 310439/700000: episode: 814, duration: 2.027s, episode steps: 363, steps per second: 179, episode reward: 241.256, mean reward: 0.665 [-17.296, 100.000], mean action: 0.871 [0.000, 3.000], mean observation: 0.164 [-1.015, 1.000], loss: 10.537494, mean_absolute_error: 48.606281, mean_q: 65.022949\n",
      " 310888/700000: episode: 815, duration: 2.358s, episode steps: 449, steps per second: 190, episode reward: 252.659, mean reward: 0.563 [-19.405, 100.000], mean action: 1.147 [0.000, 3.000], mean observation: 0.148 [-0.646, 1.268], loss: 7.016394, mean_absolute_error: 48.818336, mean_q: 64.991425\n",
      " 311152/700000: episode: 816, duration: 1.339s, episode steps: 264, steps per second: 197, episode reward: 233.870, mean reward: 0.886 [-8.228, 100.000], mean action: 1.201 [0.000, 3.000], mean observation: 0.115 [-0.895, 1.000], loss: 7.298538, mean_absolute_error: 48.481045, mean_q: 64.544579\n",
      " 311392/700000: episode: 817, duration: 1.216s, episode steps: 240, steps per second: 197, episode reward: 239.884, mean reward: 1.000 [-17.794, 100.000], mean action: 1.183 [0.000, 3.000], mean observation: 0.110 [-0.679, 1.020], loss: 6.573365, mean_absolute_error: 48.568596, mean_q: 64.863647\n",
      " 311968/700000: episode: 818, duration: 3.082s, episode steps: 576, steps per second: 187, episode reward: 217.341, mean reward: 0.377 [-19.660, 100.000], mean action: 1.024 [0.000, 3.000], mean observation: 0.131 [-0.709, 1.200], loss: 7.220393, mean_absolute_error: 49.069763, mean_q: 65.474899\n",
      " 312108/700000: episode: 819, duration: 0.716s, episode steps: 140, steps per second: 195, episode reward: -46.930, mean reward: -0.335 [-100.000, 13.166], mean action: 2.086 [0.000, 3.000], mean observation: 0.056 [-1.068, 1.000], loss: 6.979038, mean_absolute_error: 48.720871, mean_q: 65.265488\n",
      " 312609/700000: episode: 820, duration: 2.634s, episode steps: 501, steps per second: 190, episode reward: 216.848, mean reward: 0.433 [-18.031, 100.000], mean action: 0.890 [0.000, 3.000], mean observation: 0.152 [-1.119, 1.000], loss: 8.146976, mean_absolute_error: 48.850544, mean_q: 65.274391\n",
      " 313004/700000: episode: 821, duration: 2.091s, episode steps: 395, steps per second: 189, episode reward: 239.392, mean reward: 0.606 [-11.805, 100.000], mean action: 1.127 [0.000, 3.000], mean observation: 0.122 [-0.982, 1.000], loss: 11.413549, mean_absolute_error: 49.012108, mean_q: 65.404274\n",
      " 313147/700000: episode: 822, duration: 0.718s, episode steps: 143, steps per second: 199, episode reward: -65.580, mean reward: -0.459 [-100.000, 18.310], mean action: 1.573 [0.000, 3.000], mean observation: 0.134 [-1.740, 1.000], loss: 5.830280, mean_absolute_error: 48.951378, mean_q: 65.076035\n",
      " 313503/700000: episode: 823, duration: 1.834s, episode steps: 356, steps per second: 194, episode reward: 228.787, mean reward: 0.643 [-18.164, 100.000], mean action: 0.669 [0.000, 3.000], mean observation: 0.160 [-0.906, 1.000], loss: 8.774340, mean_absolute_error: 48.913189, mean_q: 65.278137\n",
      " 313849/700000: episode: 824, duration: 1.807s, episode steps: 346, steps per second: 191, episode reward: 215.070, mean reward: 0.622 [-9.328, 100.000], mean action: 1.084 [0.000, 3.000], mean observation: 0.068 [-0.879, 1.003], loss: 8.369565, mean_absolute_error: 48.777519, mean_q: 65.172981\n",
      " 314253/700000: episode: 825, duration: 2.138s, episode steps: 404, steps per second: 189, episode reward: 180.425, mean reward: 0.447 [-21.153, 100.000], mean action: 1.198 [0.000, 3.000], mean observation: 0.063 [-0.918, 1.000], loss: 8.208168, mean_absolute_error: 49.529278, mean_q: 66.199928\n",
      " 314834/700000: episode: 826, duration: 3.236s, episode steps: 581, steps per second: 180, episode reward: 227.053, mean reward: 0.391 [-20.063, 100.000], mean action: 0.924 [0.000, 3.000], mean observation: 0.102 [-0.688, 1.000], loss: 10.859602, mean_absolute_error: 49.380882, mean_q: 66.109230\n",
      " 315113/700000: episode: 827, duration: 1.443s, episode steps: 279, steps per second: 193, episode reward: 242.465, mean reward: 0.869 [-3.178, 100.000], mean action: 1.462 [0.000, 3.000], mean observation: 0.042 [-0.643, 1.000], loss: 10.465242, mean_absolute_error: 49.317493, mean_q: 65.915428\n",
      " 315456/700000: episode: 828, duration: 1.749s, episode steps: 343, steps per second: 196, episode reward: 204.136, mean reward: 0.595 [-17.492, 100.000], mean action: 0.942 [0.000, 3.000], mean observation: 0.142 [-0.945, 1.111], loss: 7.703495, mean_absolute_error: 49.281570, mean_q: 65.951172\n",
      " 315997/700000: episode: 829, duration: 3.052s, episode steps: 541, steps per second: 177, episode reward: 196.801, mean reward: 0.364 [-18.496, 100.000], mean action: 1.453 [0.000, 3.000], mean observation: 0.107 [-0.785, 1.012], loss: 9.341231, mean_absolute_error: 49.037670, mean_q: 65.488358\n",
      " 316930/700000: episode: 830, duration: 4.846s, episode steps: 933, steps per second: 193, episode reward: 206.790, mean reward: 0.222 [-19.946, 100.000], mean action: 0.717 [0.000, 3.000], mean observation: 0.185 [-0.940, 1.000], loss: 9.712036, mean_absolute_error: 48.938084, mean_q: 65.212135\n",
      " 317219/700000: episode: 831, duration: 1.493s, episode steps: 289, steps per second: 194, episode reward: 216.485, mean reward: 0.749 [-8.245, 100.000], mean action: 1.183 [0.000, 3.000], mean observation: 0.050 [-0.852, 1.000], loss: 9.567913, mean_absolute_error: 48.863956, mean_q: 65.155899\n",
      " 317524/700000: episode: 832, duration: 1.554s, episode steps: 305, steps per second: 196, episode reward: 244.196, mean reward: 0.801 [-17.338, 100.000], mean action: 0.957 [0.000, 3.000], mean observation: 0.161 [-0.786, 1.133], loss: 7.087222, mean_absolute_error: 48.876717, mean_q: 64.929787\n",
      " 317625/700000: episode: 833, duration: 0.515s, episode steps: 101, steps per second: 196, episode reward: -44.351, mean reward: -0.439 [-100.000, 17.653], mean action: 1.683 [0.000, 3.000], mean observation: 0.043 [-1.687, 1.000], loss: 5.072889, mean_absolute_error: 49.103981, mean_q: 65.331573\n",
      " 317986/700000: episode: 834, duration: 1.896s, episode steps: 361, steps per second: 190, episode reward: 158.441, mean reward: 0.439 [-11.769, 100.000], mean action: 2.091 [0.000, 3.000], mean observation: 0.088 [-0.686, 1.000], loss: 11.782660, mean_absolute_error: 48.730701, mean_q: 64.494705\n",
      " 318203/700000: episode: 835, duration: 1.092s, episode steps: 217, steps per second: 199, episode reward: 211.652, mean reward: 0.975 [-5.806, 100.000], mean action: 1.157 [0.000, 3.000], mean observation: 0.059 [-0.854, 1.000], loss: 7.603734, mean_absolute_error: 48.840916, mean_q: 64.968269\n",
      " 318506/700000: episode: 836, duration: 1.561s, episode steps: 303, steps per second: 194, episode reward: 248.824, mean reward: 0.821 [-19.201, 100.000], mean action: 1.908 [0.000, 3.000], mean observation: 0.114 [-0.846, 1.207], loss: 9.163095, mean_absolute_error: 48.613510, mean_q: 64.498306\n",
      " 318750/700000: episode: 837, duration: 1.248s, episode steps: 244, steps per second: 196, episode reward: 215.823, mean reward: 0.885 [-20.411, 100.000], mean action: 1.537 [0.000, 3.000], mean observation: 0.098 [-0.861, 1.000], loss: 9.132048, mean_absolute_error: 49.152473, mean_q: 65.368210\n",
      " 318879/700000: episode: 838, duration: 0.643s, episode steps: 129, steps per second: 201, episode reward: -50.632, mean reward: -0.392 [-100.000, 16.130], mean action: 1.504 [0.000, 3.000], mean observation: 0.000 [-1.009, 1.274], loss: 9.998157, mean_absolute_error: 48.986618, mean_q: 65.084969\n",
      " 319397/700000: episode: 839, duration: 2.692s, episode steps: 518, steps per second: 192, episode reward: 232.760, mean reward: 0.449 [-20.382, 100.000], mean action: 1.486 [0.000, 3.000], mean observation: 0.109 [-0.940, 1.000], loss: 8.949002, mean_absolute_error: 49.049316, mean_q: 65.124863\n",
      " 319698/700000: episode: 840, duration: 1.519s, episode steps: 301, steps per second: 198, episode reward: 243.997, mean reward: 0.811 [-19.571, 100.000], mean action: 1.183 [0.000, 3.000], mean observation: 0.132 [-0.637, 1.000], loss: 11.245605, mean_absolute_error: 48.952267, mean_q: 64.921555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 320017/700000: episode: 841, duration: 1.632s, episode steps: 319, steps per second: 195, episode reward: 241.117, mean reward: 0.756 [-11.458, 100.000], mean action: 1.038 [0.000, 3.000], mean observation: 0.125 [-1.437, 1.000], loss: 10.206387, mean_absolute_error: 48.625488, mean_q: 64.451653\n",
      " 320210/700000: episode: 842, duration: 0.984s, episode steps: 193, steps per second: 196, episode reward: 255.224, mean reward: 1.322 [-8.431, 100.000], mean action: 1.513 [0.000, 3.000], mean observation: 0.023 [-1.435, 1.000], loss: 9.118364, mean_absolute_error: 48.732327, mean_q: 64.858345\n",
      " 320448/700000: episode: 843, duration: 1.208s, episode steps: 238, steps per second: 197, episode reward: 223.441, mean reward: 0.939 [-6.413, 100.000], mean action: 1.319 [0.000, 3.000], mean observation: 0.049 [-0.972, 1.000], loss: 7.310735, mean_absolute_error: 48.676144, mean_q: 64.556831\n",
      " 321184/700000: episode: 844, duration: 3.802s, episode steps: 736, steps per second: 194, episode reward: 217.264, mean reward: 0.295 [-19.564, 100.000], mean action: 0.723 [0.000, 3.000], mean observation: 0.152 [-1.240, 1.000], loss: 9.935503, mean_absolute_error: 48.947414, mean_q: 64.700760\n",
      " 321462/700000: episode: 845, duration: 1.406s, episode steps: 278, steps per second: 198, episode reward: 210.847, mean reward: 0.758 [-19.687, 100.000], mean action: 0.914 [0.000, 3.000], mean observation: 0.045 [-0.982, 1.000], loss: 8.799067, mean_absolute_error: 48.768440, mean_q: 64.744308\n",
      " 321937/700000: episode: 846, duration: 2.426s, episode steps: 475, steps per second: 196, episode reward: 218.854, mean reward: 0.461 [-22.475, 100.000], mean action: 0.796 [0.000, 3.000], mean observation: 0.176 [-0.718, 1.000], loss: 10.386231, mean_absolute_error: 48.547840, mean_q: 64.153473\n",
      " 322296/700000: episode: 847, duration: 1.847s, episode steps: 359, steps per second: 194, episode reward: 189.485, mean reward: 0.528 [-18.212, 100.000], mean action: 0.880 [0.000, 3.000], mean observation: 0.113 [-0.834, 1.000], loss: 8.331261, mean_absolute_error: 48.450798, mean_q: 64.203224\n",
      " 322617/700000: episode: 848, duration: 1.640s, episode steps: 321, steps per second: 196, episode reward: 212.601, mean reward: 0.662 [-8.560, 100.000], mean action: 0.919 [0.000, 3.000], mean observation: 0.138 [-0.877, 1.000], loss: 8.694625, mean_absolute_error: 48.604919, mean_q: 64.572708\n",
      " 322847/700000: episode: 849, duration: 1.178s, episode steps: 230, steps per second: 195, episode reward: 225.582, mean reward: 0.981 [-17.386, 100.000], mean action: 1.461 [0.000, 3.000], mean observation: 0.066 [-0.920, 1.000], loss: 10.818540, mean_absolute_error: 48.471581, mean_q: 64.321754\n",
      " 323136/700000: episode: 850, duration: 1.453s, episode steps: 289, steps per second: 199, episode reward: 182.817, mean reward: 0.633 [-17.836, 100.000], mean action: 0.938 [0.000, 3.000], mean observation: 0.124 [-0.882, 1.000], loss: 7.838695, mean_absolute_error: 48.560715, mean_q: 64.160614\n",
      " 324136/700000: episode: 851, duration: 6.012s, episode steps: 1000, steps per second: 166, episode reward: 83.379, mean reward: 0.083 [-19.406, 22.873], mean action: 1.901 [0.000, 3.000], mean observation: 0.180 [-0.939, 1.000], loss: 7.612477, mean_absolute_error: 47.938290, mean_q: 63.472454\n",
      " 324542/700000: episode: 852, duration: 2.516s, episode steps: 406, steps per second: 161, episode reward: 213.333, mean reward: 0.525 [-17.843, 100.000], mean action: 1.333 [0.000, 3.000], mean observation: 0.066 [-1.463, 1.000], loss: 9.471308, mean_absolute_error: 47.818249, mean_q: 63.426670\n",
      " 324758/700000: episode: 853, duration: 1.143s, episode steps: 216, steps per second: 189, episode reward: 219.182, mean reward: 1.015 [-17.336, 100.000], mean action: 1.259 [0.000, 3.000], mean observation: 0.068 [-0.821, 1.000], loss: 7.117794, mean_absolute_error: 47.657669, mean_q: 63.056705\n",
      " 324993/700000: episode: 854, duration: 1.193s, episode steps: 235, steps per second: 197, episode reward: 215.124, mean reward: 0.915 [-10.103, 100.000], mean action: 1.315 [0.000, 3.000], mean observation: 0.028 [-0.931, 1.000], loss: 12.944230, mean_absolute_error: 47.532154, mean_q: 62.943478\n",
      " 325355/700000: episode: 855, duration: 1.937s, episode steps: 362, steps per second: 187, episode reward: 226.138, mean reward: 0.625 [-18.273, 100.000], mean action: 1.530 [0.000, 3.000], mean observation: 0.146 [-0.734, 1.000], loss: 14.261910, mean_absolute_error: 47.609207, mean_q: 63.119972\n",
      " 325889/700000: episode: 856, duration: 2.800s, episode steps: 534, steps per second: 191, episode reward: 255.872, mean reward: 0.479 [-20.892, 100.000], mean action: 1.150 [0.000, 3.000], mean observation: 0.107 [-1.838, 1.000], loss: 8.146518, mean_absolute_error: 47.299603, mean_q: 62.738678\n",
      " 326051/700000: episode: 857, duration: 0.813s, episode steps: 162, steps per second: 199, episode reward: 219.818, mean reward: 1.357 [-9.493, 100.000], mean action: 1.340 [0.000, 3.000], mean observation: 0.071 [-1.065, 1.000], loss: 12.781432, mean_absolute_error: 46.658424, mean_q: 61.602177\n",
      " 326133/700000: episode: 858, duration: 0.414s, episode steps: 82, steps per second: 198, episode reward: -102.912, mean reward: -1.255 [-100.000, 19.502], mean action: 1.537 [0.000, 3.000], mean observation: -0.040 [-2.149, 1.000], loss: 8.344065, mean_absolute_error: 46.847988, mean_q: 62.357307\n",
      " 326455/700000: episode: 859, duration: 1.700s, episode steps: 322, steps per second: 189, episode reward: 188.893, mean reward: 0.587 [-23.537, 100.000], mean action: 1.248 [0.000, 3.000], mean observation: 0.106 [-1.050, 1.000], loss: 13.877462, mean_absolute_error: 47.490482, mean_q: 63.169571\n",
      " 326692/700000: episode: 860, duration: 1.208s, episode steps: 237, steps per second: 196, episode reward: 243.560, mean reward: 1.028 [-18.099, 100.000], mean action: 1.156 [0.000, 3.000], mean observation: 0.106 [-0.777, 1.000], loss: 6.607927, mean_absolute_error: 47.635944, mean_q: 63.205929\n",
      " 326945/700000: episode: 861, duration: 1.274s, episode steps: 253, steps per second: 199, episode reward: 225.135, mean reward: 0.890 [-10.411, 100.000], mean action: 1.004 [0.000, 3.000], mean observation: 0.092 [-0.722, 1.315], loss: 5.847545, mean_absolute_error: 47.496525, mean_q: 63.192585\n",
      " 327200/700000: episode: 862, duration: 1.292s, episode steps: 255, steps per second: 197, episode reward: 233.222, mean reward: 0.915 [-17.455, 100.000], mean action: 1.216 [0.000, 3.000], mean observation: 0.079 [-0.868, 1.000], loss: 6.370272, mean_absolute_error: 47.739689, mean_q: 63.334259\n",
      " 327427/700000: episode: 863, duration: 1.165s, episode steps: 227, steps per second: 195, episode reward: 242.188, mean reward: 1.067 [-17.377, 100.000], mean action: 1.366 [0.000, 3.000], mean observation: 0.058 [-1.075, 1.000], loss: 5.210710, mean_absolute_error: 47.670559, mean_q: 63.395184\n",
      " 327714/700000: episode: 864, duration: 1.462s, episode steps: 287, steps per second: 196, episode reward: 212.294, mean reward: 0.740 [-19.382, 100.000], mean action: 0.808 [0.000, 3.000], mean observation: 0.129 [-0.968, 1.000], loss: 9.337041, mean_absolute_error: 47.509922, mean_q: 63.139793\n",
      " 328094/700000: episode: 865, duration: 2.009s, episode steps: 380, steps per second: 189, episode reward: 213.230, mean reward: 0.561 [-19.311, 100.000], mean action: 1.124 [0.000, 3.000], mean observation: 0.121 [-0.912, 1.000], loss: 7.141997, mean_absolute_error: 47.924210, mean_q: 63.640022\n",
      " 328257/700000: episode: 866, duration: 0.823s, episode steps: 163, steps per second: 198, episode reward: 7.376, mean reward: 0.045 [-100.000, 20.146], mean action: 1.902 [0.000, 3.000], mean observation: 0.105 [-1.069, 1.999], loss: 9.704713, mean_absolute_error: 47.740326, mean_q: 63.055370\n",
      " 328760/700000: episode: 867, duration: 2.723s, episode steps: 503, steps per second: 185, episode reward: 146.959, mean reward: 0.292 [-18.682, 100.000], mean action: 2.083 [0.000, 3.000], mean observation: 0.138 [-1.006, 1.000], loss: 9.959025, mean_absolute_error: 48.169071, mean_q: 63.850819\n",
      " 328959/700000: episode: 868, duration: 0.996s, episode steps: 199, steps per second: 200, episode reward: 213.805, mean reward: 1.074 [-8.322, 100.000], mean action: 1.060 [0.000, 3.000], mean observation: 0.095 [-0.916, 1.000], loss: 7.249447, mean_absolute_error: 48.045082, mean_q: 63.710453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 329166/700000: episode: 869, duration: 1.055s, episode steps: 207, steps per second: 196, episode reward: 237.143, mean reward: 1.146 [-8.140, 100.000], mean action: 1.145 [0.000, 3.000], mean observation: 0.016 [-1.086, 1.366], loss: 7.415489, mean_absolute_error: 48.426056, mean_q: 64.095230\n",
      " 329407/700000: episode: 870, duration: 1.219s, episode steps: 241, steps per second: 198, episode reward: 240.398, mean reward: 0.998 [-7.206, 100.000], mean action: 1.473 [0.000, 3.000], mean observation: 0.027 [-0.837, 1.000], loss: 7.390568, mean_absolute_error: 48.346375, mean_q: 64.165878\n",
      " 329739/700000: episode: 871, duration: 1.698s, episode steps: 332, steps per second: 196, episode reward: 226.203, mean reward: 0.681 [-17.795, 100.000], mean action: 1.117 [0.000, 3.000], mean observation: 0.140 [-0.990, 1.000], loss: 7.091640, mean_absolute_error: 48.672291, mean_q: 64.688148\n",
      " 329985/700000: episode: 872, duration: 1.244s, episode steps: 246, steps per second: 198, episode reward: 260.028, mean reward: 1.057 [-8.417, 100.000], mean action: 1.110 [0.000, 3.000], mean observation: 0.053 [-0.892, 1.000], loss: 7.278505, mean_absolute_error: 48.258629, mean_q: 63.820408\n",
      " 330202/700000: episode: 873, duration: 1.097s, episode steps: 217, steps per second: 198, episode reward: 177.483, mean reward: 0.818 [-13.917, 100.000], mean action: 1.106 [0.000, 3.000], mean observation: 0.097 [-0.914, 1.401], loss: 8.559197, mean_absolute_error: 48.742638, mean_q: 64.703537\n",
      " 330573/700000: episode: 874, duration: 1.944s, episode steps: 371, steps per second: 191, episode reward: 157.683, mean reward: 0.425 [-12.223, 100.000], mean action: 1.612 [0.000, 3.000], mean observation: 0.110 [-1.009, 1.390], loss: 11.467652, mean_absolute_error: 48.607849, mean_q: 64.305733\n",
      " 330749/700000: episode: 875, duration: 0.883s, episode steps: 176, steps per second: 199, episode reward: 233.163, mean reward: 1.325 [-11.766, 100.000], mean action: 1.062 [0.000, 3.000], mean observation: 0.093 [-1.040, 1.000], loss: 13.276190, mean_absolute_error: 48.099476, mean_q: 63.818214\n",
      " 331270/700000: episode: 876, duration: 2.697s, episode steps: 521, steps per second: 193, episode reward: 246.548, mean reward: 0.473 [-18.249, 100.000], mean action: 0.578 [0.000, 3.000], mean observation: 0.164 [-1.055, 1.563], loss: 10.114256, mean_absolute_error: 48.357334, mean_q: 64.134460\n",
      " 331430/700000: episode: 877, duration: 0.798s, episode steps: 160, steps per second: 200, episode reward: -22.207, mean reward: -0.139 [-100.000, 19.540], mean action: 1.544 [0.000, 3.000], mean observation: -0.008 [-0.911, 1.000], loss: 10.878652, mean_absolute_error: 48.976082, mean_q: 65.162010\n",
      " 331702/700000: episode: 878, duration: 1.374s, episode steps: 272, steps per second: 198, episode reward: 248.171, mean reward: 0.912 [-11.211, 100.000], mean action: 1.037 [0.000, 3.000], mean observation: 0.150 [-0.713, 1.236], loss: 9.273796, mean_absolute_error: 48.426441, mean_q: 64.285973\n",
      " 332022/700000: episode: 879, duration: 1.630s, episode steps: 320, steps per second: 196, episode reward: 252.524, mean reward: 0.789 [-10.600, 100.000], mean action: 1.500 [0.000, 3.000], mean observation: 0.036 [-0.886, 1.054], loss: 9.189442, mean_absolute_error: 48.754280, mean_q: 64.811340\n",
      " 332261/700000: episode: 880, duration: 1.191s, episode steps: 239, steps per second: 201, episode reward: 199.353, mean reward: 0.834 [-17.893, 100.000], mean action: 1.042 [0.000, 3.000], mean observation: 0.083 [-0.910, 1.000], loss: 8.453063, mean_absolute_error: 48.585304, mean_q: 64.316856\n",
      " 332572/700000: episode: 881, duration: 1.593s, episode steps: 311, steps per second: 195, episode reward: 215.964, mean reward: 0.694 [-20.735, 100.000], mean action: 0.974 [0.000, 3.000], mean observation: 0.148 [-0.923, 1.000], loss: 7.248008, mean_absolute_error: 48.996338, mean_q: 64.974617\n",
      " 332858/700000: episode: 882, duration: 1.449s, episode steps: 286, steps per second: 197, episode reward: 216.653, mean reward: 0.758 [-10.429, 100.000], mean action: 1.136 [0.000, 3.000], mean observation: 0.126 [-0.939, 1.000], loss: 6.966230, mean_absolute_error: 48.099678, mean_q: 63.609619\n",
      " 333182/700000: episode: 883, duration: 1.743s, episode steps: 324, steps per second: 186, episode reward: 96.402, mean reward: 0.298 [-23.400, 100.000], mean action: 1.713 [0.000, 3.000], mean observation: 0.013 [-0.982, 1.000], loss: 8.509642, mean_absolute_error: 48.305355, mean_q: 64.017082\n",
      " 333394/700000: episode: 884, duration: 1.061s, episode steps: 212, steps per second: 200, episode reward: 238.814, mean reward: 1.126 [-4.035, 100.000], mean action: 1.061 [0.000, 3.000], mean observation: 0.067 [-0.971, 1.000], loss: 9.843849, mean_absolute_error: 48.234577, mean_q: 64.053535\n",
      " 333657/700000: episode: 885, duration: 1.336s, episode steps: 263, steps per second: 197, episode reward: 216.765, mean reward: 0.824 [-8.349, 100.000], mean action: 1.171 [0.000, 3.000], mean observation: 0.074 [-0.850, 1.000], loss: 9.687512, mean_absolute_error: 48.664692, mean_q: 64.507675\n",
      " 333988/700000: episode: 886, duration: 1.709s, episode steps: 331, steps per second: 194, episode reward: 185.281, mean reward: 0.560 [-17.855, 100.000], mean action: 1.275 [0.000, 3.000], mean observation: 0.139 [-0.715, 1.000], loss: 9.396407, mean_absolute_error: 48.555973, mean_q: 64.415123\n",
      " 334276/700000: episode: 887, duration: 1.477s, episode steps: 288, steps per second: 195, episode reward: 242.176, mean reward: 0.841 [-19.455, 100.000], mean action: 0.750 [0.000, 3.000], mean observation: 0.136 [-0.988, 1.000], loss: 8.454823, mean_absolute_error: 48.706261, mean_q: 64.700958\n",
      " 334811/700000: episode: 888, duration: 2.951s, episode steps: 535, steps per second: 181, episode reward: 198.036, mean reward: 0.370 [-17.745, 100.000], mean action: 2.129 [0.000, 3.000], mean observation: 0.163 [-1.004, 1.000], loss: 8.712036, mean_absolute_error: 48.788002, mean_q: 64.786598\n",
      " 335587/700000: episode: 889, duration: 4.219s, episode steps: 776, steps per second: 184, episode reward: 160.218, mean reward: 0.206 [-19.267, 100.000], mean action: 0.978 [0.000, 3.000], mean observation: 0.204 [-0.936, 1.000], loss: 7.866027, mean_absolute_error: 48.715740, mean_q: 64.769737\n",
      " 335875/700000: episode: 890, duration: 1.500s, episode steps: 288, steps per second: 192, episode reward: 232.925, mean reward: 0.809 [-17.716, 100.000], mean action: 1.049 [0.000, 3.000], mean observation: 0.127 [-0.908, 1.000], loss: 10.000564, mean_absolute_error: 48.753525, mean_q: 64.728546\n",
      " 336006/700000: episode: 891, duration: 0.687s, episode steps: 131, steps per second: 191, episode reward: -11.500, mean reward: -0.088 [-100.000, 23.721], mean action: 1.740 [0.000, 3.000], mean observation: -0.003 [-1.396, 1.000], loss: 11.049108, mean_absolute_error: 48.319687, mean_q: 64.075142\n",
      " 336292/700000: episode: 892, duration: 1.474s, episode steps: 286, steps per second: 194, episode reward: 219.685, mean reward: 0.768 [-10.554, 100.000], mean action: 1.115 [0.000, 3.000], mean observation: 0.130 [-1.142, 1.000], loss: 12.660779, mean_absolute_error: 48.294662, mean_q: 64.070412\n",
      " 336703/700000: episode: 893, duration: 2.124s, episode steps: 411, steps per second: 193, episode reward: 234.945, mean reward: 0.572 [-20.032, 100.000], mean action: 1.097 [0.000, 3.000], mean observation: 0.156 [-0.776, 1.178], loss: 7.506548, mean_absolute_error: 48.257576, mean_q: 64.113670\n",
      " 337050/700000: episode: 894, duration: 1.823s, episode steps: 347, steps per second: 190, episode reward: -231.450, mean reward: -0.667 [-100.000, 20.816], mean action: 1.608 [0.000, 3.000], mean observation: 0.051 [-0.993, 3.680], loss: 7.022116, mean_absolute_error: 48.313942, mean_q: 64.226379\n",
      " 337382/700000: episode: 895, duration: 1.721s, episode steps: 332, steps per second: 193, episode reward: 231.454, mean reward: 0.697 [-17.462, 100.000], mean action: 0.771 [0.000, 3.000], mean observation: 0.120 [-1.011, 1.000], loss: 5.802477, mean_absolute_error: 48.153641, mean_q: 64.023048\n",
      " 337625/700000: episode: 896, duration: 1.228s, episode steps: 243, steps per second: 198, episode reward: 206.387, mean reward: 0.849 [-15.008, 100.000], mean action: 1.453 [0.000, 3.000], mean observation: 0.088 [-0.920, 1.000], loss: 9.161002, mean_absolute_error: 48.307587, mean_q: 64.081940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 338024/700000: episode: 897, duration: 2.067s, episode steps: 399, steps per second: 193, episode reward: 195.192, mean reward: 0.489 [-14.091, 100.000], mean action: 0.937 [0.000, 3.000], mean observation: 0.110 [-0.960, 1.000], loss: 11.440192, mean_absolute_error: 48.331543, mean_q: 63.952724\n",
      " 338786/700000: episode: 898, duration: 4.010s, episode steps: 762, steps per second: 190, episode reward: 234.358, mean reward: 0.308 [-20.494, 100.000], mean action: 0.591 [0.000, 3.000], mean observation: 0.211 [-0.940, 1.000], loss: 13.541261, mean_absolute_error: 48.461845, mean_q: 64.377945\n",
      " 339786/700000: episode: 899, duration: 5.578s, episode steps: 1000, steps per second: 179, episode reward: 60.998, mean reward: 0.061 [-19.683, 22.367], mean action: 1.107 [0.000, 3.000], mean observation: 0.211 [-0.998, 1.000], loss: 8.763931, mean_absolute_error: 48.089725, mean_q: 63.954979\n",
      " 340267/700000: episode: 900, duration: 2.436s, episode steps: 481, steps per second: 197, episode reward: 205.106, mean reward: 0.426 [-19.650, 100.000], mean action: 0.821 [0.000, 3.000], mean observation: 0.188 [-0.999, 1.000], loss: 8.860556, mean_absolute_error: 48.249004, mean_q: 64.268295\n",
      " 340870/700000: episode: 901, duration: 3.168s, episode steps: 603, steps per second: 190, episode reward: -160.623, mean reward: -0.266 [-100.000, 21.467], mean action: 0.955 [0.000, 3.000], mean observation: 0.195 [-0.896, 1.059], loss: 7.280336, mean_absolute_error: 48.360821, mean_q: 64.497864\n",
      " 341281/700000: episode: 902, duration: 2.189s, episode steps: 411, steps per second: 188, episode reward: 198.703, mean reward: 0.483 [-20.160, 100.000], mean action: 1.180 [0.000, 3.000], mean observation: 0.110 [-0.933, 1.000], loss: 9.371366, mean_absolute_error: 48.484451, mean_q: 64.510353\n",
      " 341469/700000: episode: 903, duration: 0.951s, episode steps: 188, steps per second: 198, episode reward: -248.268, mean reward: -1.321 [-100.000, 4.493], mean action: 1.548 [0.000, 3.000], mean observation: -0.176 [-1.089, 0.927], loss: 9.663102, mean_absolute_error: 48.086327, mean_q: 63.836838\n",
      " 341648/700000: episode: 904, duration: 0.898s, episode steps: 179, steps per second: 199, episode reward: 219.099, mean reward: 1.224 [-3.220, 100.000], mean action: 1.151 [0.000, 3.000], mean observation: 0.070 [-1.057, 1.000], loss: 14.569411, mean_absolute_error: 48.380405, mean_q: 64.182594\n",
      " 342294/700000: episode: 905, duration: 3.496s, episode steps: 646, steps per second: 185, episode reward: -212.274, mean reward: -0.329 [-100.000, 35.780], mean action: 1.706 [0.000, 3.000], mean observation: 0.119 [-1.505, 1.001], loss: 11.162566, mean_absolute_error: 48.219730, mean_q: 64.174919\n",
      " 342425/700000: episode: 906, duration: 0.662s, episode steps: 131, steps per second: 198, episode reward: -126.296, mean reward: -0.964 [-100.000, 18.372], mean action: 1.931 [0.000, 3.000], mean observation: 0.016 [-1.798, 1.010], loss: 10.846710, mean_absolute_error: 48.292030, mean_q: 64.545235\n",
      " 342661/700000: episode: 907, duration: 1.196s, episode steps: 236, steps per second: 197, episode reward: 241.297, mean reward: 1.022 [-9.392, 100.000], mean action: 1.042 [0.000, 3.000], mean observation: 0.103 [-1.043, 1.000], loss: 9.372500, mean_absolute_error: 48.562939, mean_q: 64.463150\n",
      " 342917/700000: episode: 908, duration: 1.290s, episode steps: 256, steps per second: 198, episode reward: 263.761, mean reward: 1.030 [-6.506, 100.000], mean action: 1.027 [0.000, 3.000], mean observation: 0.077 [-1.014, 1.000], loss: 10.411718, mean_absolute_error: 48.853516, mean_q: 65.505325\n",
      " 343242/700000: episode: 909, duration: 1.692s, episode steps: 325, steps per second: 192, episode reward: 233.767, mean reward: 0.719 [-17.626, 100.000], mean action: 0.945 [0.000, 3.000], mean observation: 0.117 [-0.893, 1.100], loss: 9.091814, mean_absolute_error: 49.049305, mean_q: 65.495132\n",
      " 343669/700000: episode: 910, duration: 2.191s, episode steps: 427, steps per second: 195, episode reward: 257.885, mean reward: 0.604 [-23.692, 100.000], mean action: 0.803 [0.000, 3.000], mean observation: 0.099 [-0.965, 1.000], loss: 7.737482, mean_absolute_error: 48.881832, mean_q: 65.298660\n",
      " 343879/700000: episode: 911, duration: 1.069s, episode steps: 210, steps per second: 197, episode reward: 271.936, mean reward: 1.295 [-8.481, 100.000], mean action: 1.310 [0.000, 3.000], mean observation: 0.015 [-0.886, 1.000], loss: 15.389405, mean_absolute_error: 49.003784, mean_q: 65.193024\n",
      " 344187/700000: episode: 912, duration: 1.602s, episode steps: 308, steps per second: 192, episode reward: 222.244, mean reward: 0.722 [-19.113, 100.000], mean action: 1.578 [0.000, 3.000], mean observation: 0.125 [-0.960, 1.000], loss: 7.658636, mean_absolute_error: 49.195850, mean_q: 65.685265\n",
      " 344668/700000: episode: 913, duration: 2.554s, episode steps: 481, steps per second: 188, episode reward: 211.418, mean reward: 0.440 [-19.173, 100.000], mean action: 0.811 [0.000, 3.000], mean observation: 0.181 [-0.825, 1.000], loss: 7.259983, mean_absolute_error: 49.299480, mean_q: 65.651512\n",
      " 344989/700000: episode: 914, duration: 1.609s, episode steps: 321, steps per second: 199, episode reward: 243.006, mean reward: 0.757 [-18.651, 100.000], mean action: 0.944 [0.000, 3.000], mean observation: 0.153 [-1.238, 1.049], loss: 9.829573, mean_absolute_error: 49.428959, mean_q: 65.843346\n",
      " 345389/700000: episode: 915, duration: 2.169s, episode steps: 400, steps per second: 184, episode reward: 210.622, mean reward: 0.527 [-10.032, 100.000], mean action: 1.552 [0.000, 3.000], mean observation: 0.089 [-1.000, 1.000], loss: 7.006705, mean_absolute_error: 49.155716, mean_q: 65.414436\n",
      " 345712/700000: episode: 916, duration: 1.647s, episode steps: 323, steps per second: 196, episode reward: 218.908, mean reward: 0.678 [-17.415, 100.000], mean action: 0.916 [0.000, 3.000], mean observation: 0.176 [-1.206, 1.000], loss: 6.017810, mean_absolute_error: 49.270287, mean_q: 65.442009\n",
      " 346103/700000: episode: 917, duration: 2.118s, episode steps: 391, steps per second: 185, episode reward: 259.171, mean reward: 0.663 [-20.048, 100.000], mean action: 1.174 [0.000, 3.000], mean observation: 0.186 [-1.395, 1.000], loss: 9.542420, mean_absolute_error: 49.642712, mean_q: 65.947960\n",
      " 346509/700000: episode: 918, duration: 2.370s, episode steps: 406, steps per second: 171, episode reward: 215.524, mean reward: 0.531 [-10.970, 100.000], mean action: 1.126 [0.000, 3.000], mean observation: 0.164 [-0.869, 1.157], loss: 7.702754, mean_absolute_error: 50.049809, mean_q: 66.443092\n",
      " 346754/700000: episode: 919, duration: 1.284s, episode steps: 245, steps per second: 191, episode reward: 204.365, mean reward: 0.834 [-3.312, 100.000], mean action: 1.176 [0.000, 3.000], mean observation: 0.110 [-0.886, 1.000], loss: 9.784337, mean_absolute_error: 49.954960, mean_q: 66.174484\n",
      " 347248/700000: episode: 920, duration: 2.874s, episode steps: 494, steps per second: 172, episode reward: 222.575, mean reward: 0.451 [-19.565, 100.000], mean action: 0.917 [0.000, 3.000], mean observation: 0.156 [-0.865, 1.000], loss: 9.623601, mean_absolute_error: 49.754414, mean_q: 65.958572\n",
      " 347500/700000: episode: 921, duration: 1.487s, episode steps: 252, steps per second: 169, episode reward: 199.720, mean reward: 0.793 [-9.247, 100.000], mean action: 1.377 [0.000, 3.000], mean observation: 0.053 [-1.336, 1.000], loss: 7.614751, mean_absolute_error: 49.969967, mean_q: 66.193291\n",
      " 347712/700000: episode: 922, duration: 1.066s, episode steps: 212, steps per second: 199, episode reward: 205.824, mean reward: 0.971 [-3.270, 100.000], mean action: 1.156 [0.000, 3.000], mean observation: 0.084 [-0.814, 1.000], loss: 4.666950, mean_absolute_error: 49.848640, mean_q: 65.978920\n",
      " 348128/700000: episode: 923, duration: 2.245s, episode steps: 416, steps per second: 185, episode reward: 173.268, mean reward: 0.417 [-20.434, 100.000], mean action: 1.250 [0.000, 3.000], mean observation: 0.105 [-1.379, 1.000], loss: 11.351409, mean_absolute_error: 50.062603, mean_q: 66.276779\n",
      " 348390/700000: episode: 924, duration: 1.325s, episode steps: 262, steps per second: 198, episode reward: 264.191, mean reward: 1.008 [-3.280, 100.000], mean action: 1.252 [0.000, 3.000], mean observation: 0.063 [-0.768, 1.000], loss: 8.394406, mean_absolute_error: 50.490108, mean_q: 67.193123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 348760/700000: episode: 925, duration: 1.909s, episode steps: 370, steps per second: 194, episode reward: 205.046, mean reward: 0.554 [-24.240, 100.000], mean action: 0.781 [0.000, 3.000], mean observation: 0.157 [-0.904, 1.000], loss: 11.217494, mean_absolute_error: 50.194088, mean_q: 66.417557\n",
      " 349197/700000: episode: 926, duration: 2.333s, episode steps: 437, steps per second: 187, episode reward: 220.751, mean reward: 0.505 [-19.211, 100.000], mean action: 0.744 [0.000, 3.000], mean observation: 0.179 [-0.998, 1.000], loss: 9.160974, mean_absolute_error: 50.299030, mean_q: 66.694687\n",
      " 349456/700000: episode: 927, duration: 1.307s, episode steps: 259, steps per second: 198, episode reward: 232.333, mean reward: 0.897 [-10.417, 100.000], mean action: 1.595 [0.000, 3.000], mean observation: 0.099 [-0.923, 1.000], loss: 7.386410, mean_absolute_error: 50.440975, mean_q: 67.050125\n",
      " 349537/700000: episode: 928, duration: 0.408s, episode steps: 81, steps per second: 199, episode reward: -7.680, mean reward: -0.095 [-100.000, 21.155], mean action: 1.432 [0.000, 3.000], mean observation: -0.002 [-1.920, 1.000], loss: 7.920201, mean_absolute_error: 50.203003, mean_q: 66.177826\n",
      " 349812/700000: episode: 929, duration: 1.424s, episode steps: 275, steps per second: 193, episode reward: 250.356, mean reward: 0.910 [-17.431, 100.000], mean action: 1.262 [0.000, 3.000], mean observation: 0.116 [-0.776, 1.000], loss: 7.557237, mean_absolute_error: 50.109787, mean_q: 66.746902\n",
      " 350070/700000: episode: 930, duration: 1.289s, episode steps: 258, steps per second: 200, episode reward: 245.115, mean reward: 0.950 [-8.150, 100.000], mean action: 0.647 [0.000, 3.000], mean observation: 0.158 [-1.040, 1.000], loss: 9.180100, mean_absolute_error: 50.180134, mean_q: 66.458138\n",
      " 350381/700000: episode: 931, duration: 1.633s, episode steps: 311, steps per second: 190, episode reward: 260.289, mean reward: 0.837 [-20.229, 100.000], mean action: 1.305 [0.000, 3.000], mean observation: 0.087 [-0.750, 1.108], loss: 8.172048, mean_absolute_error: 49.942493, mean_q: 66.079720\n",
      " 350536/700000: episode: 932, duration: 0.774s, episode steps: 155, steps per second: 200, episode reward: 5.654, mean reward: 0.036 [-100.000, 18.347], mean action: 1.665 [0.000, 3.000], mean observation: 0.063 [-1.085, 1.000], loss: 7.231447, mean_absolute_error: 50.475285, mean_q: 66.754288\n",
      " 351081/700000: episode: 933, duration: 2.814s, episode steps: 545, steps per second: 194, episode reward: 242.728, mean reward: 0.445 [-20.997, 100.000], mean action: 0.688 [0.000, 3.000], mean observation: 0.151 [-0.870, 1.010], loss: 6.377829, mean_absolute_error: 50.206703, mean_q: 66.818436\n",
      " 351314/700000: episode: 934, duration: 1.174s, episode steps: 233, steps per second: 199, episode reward: 220.340, mean reward: 0.946 [-10.361, 100.000], mean action: 0.854 [0.000, 3.000], mean observation: 0.125 [-1.108, 1.000], loss: 9.052774, mean_absolute_error: 50.019142, mean_q: 66.395897\n",
      " 351456/700000: episode: 935, duration: 0.713s, episode steps: 142, steps per second: 199, episode reward: 4.659, mean reward: 0.033 [-100.000, 18.464], mean action: 1.620 [0.000, 3.000], mean observation: 0.061 [-1.291, 1.000], loss: 10.203869, mean_absolute_error: 50.527081, mean_q: 67.232933\n",
      " 351653/700000: episode: 936, duration: 0.998s, episode steps: 197, steps per second: 197, episode reward: 208.105, mean reward: 1.056 [-3.162, 100.000], mean action: 1.437 [0.000, 3.000], mean observation: 0.084 [-0.894, 1.000], loss: 19.122961, mean_absolute_error: 49.862854, mean_q: 65.840759\n",
      " 352501/700000: episode: 937, duration: 4.821s, episode steps: 848, steps per second: 176, episode reward: 137.721, mean reward: 0.162 [-19.013, 100.000], mean action: 2.037 [0.000, 3.000], mean observation: 0.072 [-0.928, 1.077], loss: 8.662633, mean_absolute_error: 49.664715, mean_q: 65.837624\n",
      " 352739/700000: episode: 938, duration: 1.210s, episode steps: 238, steps per second: 197, episode reward: 224.542, mean reward: 0.943 [-10.293, 100.000], mean action: 1.349 [0.000, 3.000], mean observation: 0.098 [-1.096, 1.000], loss: 9.664322, mean_absolute_error: 49.670689, mean_q: 65.839340\n",
      " 352813/700000: episode: 939, duration: 0.382s, episode steps: 74, steps per second: 194, episode reward: -75.105, mean reward: -1.015 [-100.000, 11.609], mean action: 1.649 [0.000, 3.000], mean observation: 0.100 [-3.022, 1.000], loss: 10.803502, mean_absolute_error: 50.259705, mean_q: 66.900391\n",
      " 353098/700000: episode: 940, duration: 1.437s, episode steps: 285, steps per second: 198, episode reward: 215.535, mean reward: 0.756 [-8.742, 100.000], mean action: 1.218 [0.000, 3.000], mean observation: 0.046 [-0.991, 1.000], loss: 12.325838, mean_absolute_error: 49.548809, mean_q: 65.778465\n",
      " 353298/700000: episode: 941, duration: 1.012s, episode steps: 200, steps per second: 198, episode reward: 212.938, mean reward: 1.065 [-3.344, 100.000], mean action: 1.390 [0.000, 3.000], mean observation: 0.091 [-0.989, 1.000], loss: 6.455322, mean_absolute_error: 49.794205, mean_q: 66.353424\n",
      " 353613/700000: episode: 942, duration: 1.619s, episode steps: 315, steps per second: 195, episode reward: 234.901, mean reward: 0.746 [-10.971, 100.000], mean action: 1.298 [0.000, 3.000], mean observation: 0.085 [-0.745, 1.000], loss: 6.644744, mean_absolute_error: 50.050831, mean_q: 66.446747\n",
      " 354412/700000: episode: 943, duration: 4.318s, episode steps: 799, steps per second: 185, episode reward: 235.604, mean reward: 0.295 [-18.575, 100.000], mean action: 0.717 [0.000, 3.000], mean observation: 0.188 [-0.766, 1.000], loss: 9.620947, mean_absolute_error: 50.134708, mean_q: 66.293198\n",
      " 354504/700000: episode: 944, duration: 0.468s, episode steps: 92, steps per second: 197, episode reward: -82.231, mean reward: -0.894 [-100.000, 9.708], mean action: 1.543 [0.000, 3.000], mean observation: 0.022 [-2.254, 1.000], loss: 7.328391, mean_absolute_error: 49.990971, mean_q: 66.143005\n",
      " 354609/700000: episode: 945, duration: 0.524s, episode steps: 105, steps per second: 200, episode reward: -49.696, mean reward: -0.473 [-100.000, 16.474], mean action: 1.181 [0.000, 3.000], mean observation: 0.125 [-2.828, 1.000], loss: 7.310788, mean_absolute_error: 50.008411, mean_q: 66.398338\n",
      " 354855/700000: episode: 946, duration: 1.263s, episode steps: 246, steps per second: 195, episode reward: 268.990, mean reward: 1.093 [-18.333, 100.000], mean action: 1.073 [0.000, 3.000], mean observation: 0.106 [-0.840, 1.233], loss: 7.788912, mean_absolute_error: 50.336685, mean_q: 66.735779\n",
      " 355063/700000: episode: 947, duration: 1.063s, episode steps: 208, steps per second: 196, episode reward: 234.196, mean reward: 1.126 [-3.566, 100.000], mean action: 1.577 [0.000, 3.000], mean observation: 0.026 [-0.762, 1.000], loss: 9.542380, mean_absolute_error: 50.480427, mean_q: 66.785110\n",
      " 355256/700000: episode: 948, duration: 0.976s, episode steps: 193, steps per second: 198, episode reward: 268.754, mean reward: 1.393 [-8.773, 100.000], mean action: 1.316 [0.000, 3.000], mean observation: 0.019 [-0.747, 1.000], loss: 8.482853, mean_absolute_error: 50.058170, mean_q: 66.142303\n",
      " 355451/700000: episode: 949, duration: 0.981s, episode steps: 195, steps per second: 199, episode reward: 194.541, mean reward: 0.998 [-9.713, 100.000], mean action: 1.436 [0.000, 3.000], mean observation: 0.073 [-0.837, 1.000], loss: 10.010511, mean_absolute_error: 50.127533, mean_q: 66.586075\n",
      " 355697/700000: episode: 950, duration: 1.235s, episode steps: 246, steps per second: 199, episode reward: 216.877, mean reward: 0.882 [-18.277, 100.000], mean action: 0.996 [0.000, 3.000], mean observation: 0.111 [-0.941, 1.000], loss: 5.285902, mean_absolute_error: 50.171524, mean_q: 66.569954\n",
      " 356017/700000: episode: 951, duration: 1.650s, episode steps: 320, steps per second: 194, episode reward: 202.835, mean reward: 0.634 [-10.092, 100.000], mean action: 1.016 [0.000, 3.000], mean observation: 0.104 [-0.866, 1.000], loss: 6.794744, mean_absolute_error: 50.851139, mean_q: 67.522415\n",
      " 356306/700000: episode: 952, duration: 1.504s, episode steps: 289, steps per second: 192, episode reward: 219.138, mean reward: 0.758 [-18.025, 100.000], mean action: 0.872 [0.000, 3.000], mean observation: 0.124 [-0.940, 1.039], loss: 8.784887, mean_absolute_error: 51.051304, mean_q: 67.617455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 356551/700000: episode: 953, duration: 1.250s, episode steps: 245, steps per second: 196, episode reward: 233.256, mean reward: 0.952 [-12.198, 100.000], mean action: 0.967 [0.000, 3.000], mean observation: 0.093 [-0.889, 1.030], loss: 7.049412, mean_absolute_error: 50.500408, mean_q: 66.997917\n",
      " 356819/700000: episode: 954, duration: 1.394s, episode steps: 268, steps per second: 192, episode reward: 230.726, mean reward: 0.861 [-10.523, 100.000], mean action: 1.175 [0.000, 3.000], mean observation: 0.058 [-0.873, 1.000], loss: 9.667807, mean_absolute_error: 50.880554, mean_q: 67.365829\n",
      " 356939/700000: episode: 955, duration: 0.605s, episode steps: 120, steps per second: 198, episode reward: -31.273, mean reward: -0.261 [-100.000, 10.139], mean action: 1.583 [0.000, 3.000], mean observation: 0.033 [-1.264, 1.000], loss: 3.332143, mean_absolute_error: 51.100529, mean_q: 67.741936\n",
      " 357199/700000: episode: 956, duration: 1.304s, episode steps: 260, steps per second: 199, episode reward: 225.020, mean reward: 0.865 [-17.870, 100.000], mean action: 1.019 [0.000, 3.000], mean observation: 0.089 [-1.003, 1.000], loss: 7.331415, mean_absolute_error: 50.926411, mean_q: 67.750183\n",
      " 357428/700000: episode: 957, duration: 1.152s, episode steps: 229, steps per second: 199, episode reward: 201.158, mean reward: 0.878 [-11.156, 100.000], mean action: 1.153 [0.000, 3.000], mean observation: 0.096 [-0.897, 1.000], loss: 9.363481, mean_absolute_error: 51.313301, mean_q: 68.116631\n",
      " 357789/700000: episode: 958, duration: 1.932s, episode steps: 361, steps per second: 187, episode reward: 247.512, mean reward: 0.686 [-18.954, 100.000], mean action: 1.266 [0.000, 3.000], mean observation: 0.073 [-0.826, 1.000], loss: 10.064006, mean_absolute_error: 51.161118, mean_q: 68.015648\n",
      " 358089/700000: episode: 959, duration: 1.537s, episode steps: 300, steps per second: 195, episode reward: 186.878, mean reward: 0.623 [-17.201, 100.000], mean action: 1.227 [0.000, 3.000], mean observation: 0.191 [-0.995, 1.000], loss: 8.909661, mean_absolute_error: 51.891766, mean_q: 68.653526\n",
      " 358287/700000: episode: 960, duration: 0.987s, episode steps: 198, steps per second: 201, episode reward: 208.465, mean reward: 1.053 [-20.461, 100.000], mean action: 1.187 [0.000, 3.000], mean observation: 0.184 [-0.972, 1.000], loss: 12.840943, mean_absolute_error: 51.672073, mean_q: 68.686226\n",
      " 358392/700000: episode: 961, duration: 0.529s, episode steps: 105, steps per second: 198, episode reward: 19.180, mean reward: 0.183 [-100.000, 20.655], mean action: 1.638 [0.000, 3.000], mean observation: -0.051 [-0.872, 1.000], loss: 8.411977, mean_absolute_error: 51.126713, mean_q: 67.997406\n",
      " 358644/700000: episode: 962, duration: 1.280s, episode steps: 252, steps per second: 197, episode reward: 177.614, mean reward: 0.705 [-9.875, 100.000], mean action: 1.111 [0.000, 3.000], mean observation: 0.179 [-0.981, 1.000], loss: 10.299561, mean_absolute_error: 51.839657, mean_q: 68.920784\n",
      " 358882/700000: episode: 963, duration: 1.197s, episode steps: 238, steps per second: 199, episode reward: 240.524, mean reward: 1.011 [-17.370, 100.000], mean action: 1.437 [0.000, 3.000], mean observation: 0.126 [-1.067, 1.000], loss: 13.348255, mean_absolute_error: 52.483185, mean_q: 69.431213\n",
      " 359227/700000: episode: 964, duration: 1.743s, episode steps: 345, steps per second: 198, episode reward: 203.354, mean reward: 0.589 [-10.427, 100.000], mean action: 0.901 [0.000, 3.000], mean observation: 0.020 [-1.040, 1.000], loss: 11.401999, mean_absolute_error: 52.218830, mean_q: 69.461166\n",
      " 359475/700000: episode: 965, duration: 1.239s, episode steps: 248, steps per second: 200, episode reward: 249.731, mean reward: 1.007 [-9.906, 100.000], mean action: 1.044 [0.000, 3.000], mean observation: 0.046 [-0.811, 1.000], loss: 6.314294, mean_absolute_error: 52.075829, mean_q: 69.398827\n",
      " 359761/700000: episode: 966, duration: 1.469s, episode steps: 286, steps per second: 195, episode reward: 238.861, mean reward: 0.835 [-12.445, 100.000], mean action: 1.031 [0.000, 3.000], mean observation: 0.154 [-1.186, 1.000], loss: 7.617577, mean_absolute_error: 52.141640, mean_q: 69.678337\n",
      " 359981/700000: episode: 967, duration: 1.111s, episode steps: 220, steps per second: 198, episode reward: 252.874, mean reward: 1.149 [-8.707, 100.000], mean action: 1.277 [0.000, 3.000], mean observation: 0.041 [-0.828, 1.000], loss: 7.600935, mean_absolute_error: 52.425236, mean_q: 69.638115\n",
      " 360423/700000: episode: 968, duration: 2.329s, episode steps: 442, steps per second: 190, episode reward: 246.284, mean reward: 0.557 [-18.269, 100.000], mean action: 1.129 [0.000, 3.000], mean observation: 0.108 [-0.732, 1.000], loss: 8.179564, mean_absolute_error: 51.879642, mean_q: 69.095444\n",
      " 360830/700000: episode: 969, duration: 2.112s, episode steps: 407, steps per second: 193, episode reward: 225.809, mean reward: 0.555 [-8.911, 100.000], mean action: 1.160 [0.000, 3.000], mean observation: 0.166 [-0.863, 1.000], loss: 8.619061, mean_absolute_error: 51.881393, mean_q: 69.017197\n",
      " 361089/700000: episode: 970, duration: 1.335s, episode steps: 259, steps per second: 194, episode reward: 203.839, mean reward: 0.787 [-21.085, 100.000], mean action: 1.444 [0.000, 3.000], mean observation: 0.126 [-0.813, 1.000], loss: 8.970473, mean_absolute_error: 51.728661, mean_q: 68.853539\n",
      " 361301/700000: episode: 971, duration: 1.076s, episode steps: 212, steps per second: 197, episode reward: 238.560, mean reward: 1.125 [-9.466, 100.000], mean action: 1.269 [0.000, 3.000], mean observation: 0.106 [-0.841, 1.000], loss: 10.104344, mean_absolute_error: 52.178009, mean_q: 69.096817\n",
      " 361579/700000: episode: 972, duration: 1.416s, episode steps: 278, steps per second: 196, episode reward: 219.457, mean reward: 0.789 [-23.696, 100.000], mean action: 1.183 [0.000, 3.000], mean observation: 0.112 [-1.141, 1.000], loss: 7.171854, mean_absolute_error: 51.989899, mean_q: 69.223610\n",
      " 361836/700000: episode: 973, duration: 1.322s, episode steps: 257, steps per second: 194, episode reward: -236.677, mean reward: -0.921 [-100.000, 13.754], mean action: 1.611 [0.000, 3.000], mean observation: -0.022 [-1.987, 1.000], loss: 8.046700, mean_absolute_error: 51.758087, mean_q: 68.707726\n",
      " 362248/700000: episode: 974, duration: 2.118s, episode steps: 412, steps per second: 195, episode reward: 194.771, mean reward: 0.473 [-19.812, 100.000], mean action: 0.791 [0.000, 3.000], mean observation: 0.177 [-0.818, 1.000], loss: 8.698127, mean_absolute_error: 51.939758, mean_q: 69.146713\n",
      " 362546/700000: episode: 975, duration: 1.554s, episode steps: 298, steps per second: 192, episode reward: 221.498, mean reward: 0.743 [-10.703, 100.000], mean action: 1.463 [0.000, 3.000], mean observation: 0.079 [-0.824, 1.000], loss: 7.407254, mean_absolute_error: 51.835159, mean_q: 68.734566\n",
      " 362986/700000: episode: 976, duration: 2.370s, episode steps: 440, steps per second: 186, episode reward: 204.766, mean reward: 0.465 [-15.832, 100.000], mean action: 1.339 [0.000, 3.000], mean observation: 0.008 [-1.230, 1.451], loss: 8.530869, mean_absolute_error: 51.818420, mean_q: 69.004265\n",
      " 363287/700000: episode: 977, duration: 1.542s, episode steps: 301, steps per second: 195, episode reward: 219.062, mean reward: 0.728 [-10.647, 100.000], mean action: 1.289 [0.000, 3.000], mean observation: 0.073 [-0.527, 1.000], loss: 7.107735, mean_absolute_error: 52.101601, mean_q: 69.339043\n",
      " 363559/700000: episode: 978, duration: 1.384s, episode steps: 272, steps per second: 197, episode reward: 230.511, mean reward: 0.847 [-18.479, 100.000], mean action: 1.074 [0.000, 3.000], mean observation: 0.091 [-0.955, 1.000], loss: 9.482889, mean_absolute_error: 51.727386, mean_q: 68.885643\n",
      " 363685/700000: episode: 979, duration: 0.635s, episode steps: 126, steps per second: 198, episode reward: -22.233, mean reward: -0.176 [-100.000, 17.419], mean action: 1.484 [0.000, 3.000], mean observation: -0.089 [-0.932, 1.380], loss: 5.281820, mean_absolute_error: 51.679386, mean_q: 69.201302\n",
      " 363975/700000: episode: 980, duration: 1.466s, episode steps: 290, steps per second: 198, episode reward: 203.350, mean reward: 0.701 [-11.025, 100.000], mean action: 1.141 [0.000, 3.000], mean observation: 0.116 [-1.145, 1.000], loss: 8.331628, mean_absolute_error: 51.846573, mean_q: 68.985535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 364227/700000: episode: 981, duration: 1.274s, episode steps: 252, steps per second: 198, episode reward: 189.003, mean reward: 0.750 [-14.509, 100.000], mean action: 1.194 [0.000, 3.000], mean observation: 0.148 [-0.660, 1.114], loss: 11.203276, mean_absolute_error: 51.028629, mean_q: 67.592674\n",
      " 364713/700000: episode: 982, duration: 2.566s, episode steps: 486, steps per second: 189, episode reward: 199.637, mean reward: 0.411 [-17.695, 100.000], mean action: 1.134 [0.000, 3.000], mean observation: 0.132 [-0.706, 1.000], loss: 8.551657, mean_absolute_error: 51.012749, mean_q: 67.977013\n",
      " 365015/700000: episode: 983, duration: 1.573s, episode steps: 302, steps per second: 192, episode reward: 201.357, mean reward: 0.667 [-10.953, 100.000], mean action: 1.245 [0.000, 3.000], mean observation: 0.070 [-0.589, 1.000], loss: 8.655317, mean_absolute_error: 50.947617, mean_q: 67.464424\n",
      " 365246/700000: episode: 984, duration: 1.184s, episode steps: 231, steps per second: 195, episode reward: 230.114, mean reward: 0.996 [-6.680, 100.000], mean action: 1.303 [0.000, 3.000], mean observation: 0.070 [-0.662, 1.000], loss: 11.630155, mean_absolute_error: 50.694370, mean_q: 66.887741\n",
      " 365598/700000: episode: 985, duration: 1.846s, episode steps: 352, steps per second: 191, episode reward: 248.827, mean reward: 0.707 [-17.806, 100.000], mean action: 1.023 [0.000, 3.000], mean observation: 0.087 [-0.888, 1.000], loss: 8.205755, mean_absolute_error: 50.772182, mean_q: 67.326103\n",
      " 365834/700000: episode: 986, duration: 1.202s, episode steps: 236, steps per second: 196, episode reward: 223.508, mean reward: 0.947 [-9.397, 100.000], mean action: 1.305 [0.000, 3.000], mean observation: 0.068 [-1.037, 1.000], loss: 8.758989, mean_absolute_error: 50.902134, mean_q: 67.713257\n",
      " 366068/700000: episode: 987, duration: 1.197s, episode steps: 234, steps per second: 195, episode reward: 235.465, mean reward: 1.006 [-9.760, 100.000], mean action: 1.385 [0.000, 3.000], mean observation: 0.021 [-0.732, 1.000], loss: 8.797386, mean_absolute_error: 50.735531, mean_q: 67.464729\n",
      " 366742/700000: episode: 988, duration: 3.507s, episode steps: 674, steps per second: 192, episode reward: 207.020, mean reward: 0.307 [-20.213, 100.000], mean action: 0.734 [0.000, 3.000], mean observation: 0.233 [-1.053, 1.353], loss: 8.215296, mean_absolute_error: 50.601208, mean_q: 67.476768\n",
      " 366982/700000: episode: 989, duration: 1.217s, episode steps: 240, steps per second: 197, episode reward: 229.991, mean reward: 0.958 [-8.631, 100.000], mean action: 1.471 [0.000, 3.000], mean observation: 0.076 [-0.877, 1.000], loss: 8.503428, mean_absolute_error: 50.757229, mean_q: 67.476471\n",
      " 367314/700000: episode: 990, duration: 1.724s, episode steps: 332, steps per second: 193, episode reward: 199.547, mean reward: 0.601 [-17.425, 100.000], mean action: 1.057 [0.000, 3.000], mean observation: 0.096 [-0.956, 1.000], loss: 8.869305, mean_absolute_error: 50.422653, mean_q: 67.391243\n",
      " 367489/700000: episode: 991, duration: 0.880s, episode steps: 175, steps per second: 199, episode reward: 197.304, mean reward: 1.127 [-18.646, 100.000], mean action: 1.217 [0.000, 3.000], mean observation: 0.065 [-1.019, 1.000], loss: 13.352603, mean_absolute_error: 50.326462, mean_q: 66.965439\n",
      " 367661/700000: episode: 992, duration: 0.989s, episode steps: 172, steps per second: 174, episode reward: 225.973, mean reward: 1.314 [-5.763, 100.000], mean action: 1.186 [0.000, 3.000], mean observation: 0.073 [-0.853, 1.115], loss: 7.440178, mean_absolute_error: 50.374352, mean_q: 67.183830\n",
      " 367949/700000: episode: 993, duration: 1.513s, episode steps: 288, steps per second: 190, episode reward: 177.295, mean reward: 0.616 [-18.161, 100.000], mean action: 1.201 [0.000, 3.000], mean observation: 0.091 [-0.660, 1.000], loss: 6.606512, mean_absolute_error: 50.739269, mean_q: 67.636444\n",
      " 368085/700000: episode: 994, duration: 0.685s, episode steps: 136, steps per second: 198, episode reward: -186.176, mean reward: -1.369 [-100.000, 31.853], mean action: 1.676 [0.000, 3.000], mean observation: 0.142 [-0.830, 1.771], loss: 6.151210, mean_absolute_error: 50.667557, mean_q: 67.567810\n",
      " 368319/700000: episode: 995, duration: 1.194s, episode steps: 234, steps per second: 196, episode reward: 203.660, mean reward: 0.870 [-6.780, 100.000], mean action: 1.479 [0.000, 3.000], mean observation: 0.140 [-0.874, 1.026], loss: 6.608123, mean_absolute_error: 50.222755, mean_q: 66.887642\n",
      " 368534/700000: episode: 996, duration: 1.087s, episode steps: 215, steps per second: 198, episode reward: 258.568, mean reward: 1.203 [-8.281, 100.000], mean action: 1.460 [0.000, 3.000], mean observation: 0.107 [-0.829, 1.084], loss: 5.943927, mean_absolute_error: 50.486240, mean_q: 67.059296\n",
      " 368743/700000: episode: 997, duration: 1.050s, episode steps: 209, steps per second: 199, episode reward: 228.173, mean reward: 1.092 [-5.947, 100.000], mean action: 1.354 [0.000, 3.000], mean observation: 0.090 [-0.958, 1.091], loss: 6.398653, mean_absolute_error: 50.001831, mean_q: 66.406555\n",
      " 368857/700000: episode: 998, duration: 0.573s, episode steps: 114, steps per second: 199, episode reward: -161.010, mean reward: -1.412 [-100.000, 8.693], mean action: 1.728 [0.000, 3.000], mean observation: -0.023 [-0.911, 1.151], loss: 9.522512, mean_absolute_error: 50.090721, mean_q: 66.586578\n",
      " 369076/700000: episode: 999, duration: 1.176s, episode steps: 219, steps per second: 186, episode reward: -38.544, mean reward: -0.176 [-100.000, 15.181], mean action: 1.726 [0.000, 3.000], mean observation: 0.012 [-0.898, 1.000], loss: 11.456944, mean_absolute_error: 49.670021, mean_q: 66.260498\n",
      " 369381/700000: episode: 1000, duration: 1.760s, episode steps: 305, steps per second: 173, episode reward: 217.156, mean reward: 0.712 [-17.598, 100.000], mean action: 1.334 [0.000, 3.000], mean observation: 0.098 [-0.883, 1.008], loss: 9.512210, mean_absolute_error: 49.894749, mean_q: 66.089561\n",
      " 369782/700000: episode: 1001, duration: 2.124s, episode steps: 401, steps per second: 189, episode reward: 221.122, mean reward: 0.551 [-17.440, 100.000], mean action: 0.868 [0.000, 3.000], mean observation: 0.171 [-0.869, 1.319], loss: 6.598941, mean_absolute_error: 49.575504, mean_q: 65.852859\n",
      " 369983/700000: episode: 1002, duration: 1.472s, episode steps: 201, steps per second: 137, episode reward: 222.380, mean reward: 1.106 [-9.997, 100.000], mean action: 1.065 [0.000, 3.000], mean observation: 0.103 [-0.842, 1.087], loss: 6.689992, mean_absolute_error: 49.307739, mean_q: 65.210815\n",
      " 370370/700000: episode: 1003, duration: 2.985s, episode steps: 387, steps per second: 130, episode reward: 244.344, mean reward: 0.631 [-19.036, 100.000], mean action: 0.948 [0.000, 3.000], mean observation: 0.164 [-0.686, 1.000], loss: 6.910947, mean_absolute_error: 49.637604, mean_q: 65.755508\n",
      " 370621/700000: episode: 1004, duration: 1.406s, episode steps: 251, steps per second: 179, episode reward: 203.636, mean reward: 0.811 [-3.673, 100.000], mean action: 1.299 [0.000, 3.000], mean observation: 0.099 [-0.744, 1.145], loss: 8.634801, mean_absolute_error: 49.803455, mean_q: 65.831535\n",
      " 370907/700000: episode: 1005, duration: 1.605s, episode steps: 286, steps per second: 178, episode reward: 253.830, mean reward: 0.888 [-9.102, 100.000], mean action: 1.122 [0.000, 3.000], mean observation: 0.090 [-0.754, 1.000], loss: 13.730123, mean_absolute_error: 49.827980, mean_q: 65.986473\n",
      " 371427/700000: episode: 1006, duration: 2.801s, episode steps: 520, steps per second: 186, episode reward: 224.590, mean reward: 0.432 [-19.375, 100.000], mean action: 0.981 [0.000, 3.000], mean observation: 0.182 [-0.841, 1.010], loss: 8.160348, mean_absolute_error: 49.424438, mean_q: 65.575241\n",
      " 371759/700000: episode: 1007, duration: 1.847s, episode steps: 332, steps per second: 180, episode reward: 235.279, mean reward: 0.709 [-20.041, 100.000], mean action: 1.416 [0.000, 3.000], mean observation: 0.061 [-0.922, 1.000], loss: 9.648774, mean_absolute_error: 49.828941, mean_q: 66.215271\n",
      " 371993/700000: episode: 1008, duration: 1.225s, episode steps: 234, steps per second: 191, episode reward: 203.153, mean reward: 0.868 [-9.903, 100.000], mean action: 1.128 [0.000, 3.000], mean observation: 0.100 [-1.000, 1.000], loss: 11.662524, mean_absolute_error: 49.347416, mean_q: 65.348000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 372203/700000: episode: 1009, duration: 1.080s, episode steps: 210, steps per second: 194, episode reward: 183.917, mean reward: 0.876 [-8.060, 100.000], mean action: 1.467 [0.000, 3.000], mean observation: 0.163 [-1.050, 1.000], loss: 7.915995, mean_absolute_error: 50.139359, mean_q: 66.675522\n",
      " 372476/700000: episode: 1010, duration: 1.395s, episode steps: 273, steps per second: 196, episode reward: 235.282, mean reward: 0.862 [-8.922, 100.000], mean action: 0.967 [0.000, 3.000], mean observation: 0.112 [-1.068, 1.009], loss: 8.304250, mean_absolute_error: 49.995129, mean_q: 66.511253\n",
      " 372646/700000: episode: 1011, duration: 0.863s, episode steps: 170, steps per second: 197, episode reward: 237.999, mean reward: 1.400 [-9.388, 100.000], mean action: 1.147 [0.000, 3.000], mean observation: 0.015 [-1.082, 1.000], loss: 12.031823, mean_absolute_error: 49.974018, mean_q: 65.950600\n",
      " 372843/700000: episode: 1012, duration: 1.017s, episode steps: 197, steps per second: 194, episode reward: 246.417, mean reward: 1.251 [-10.857, 100.000], mean action: 1.325 [0.000, 3.000], mean observation: 0.024 [-1.074, 1.000], loss: 10.085668, mean_absolute_error: 50.037441, mean_q: 66.324760\n",
      " 373035/700000: episode: 1013, duration: 0.965s, episode steps: 192, steps per second: 199, episode reward: 203.199, mean reward: 1.058 [-12.059, 100.000], mean action: 0.979 [0.000, 3.000], mean observation: 0.098 [-0.965, 1.000], loss: 12.269803, mean_absolute_error: 50.080280, mean_q: 66.547218\n",
      " 373331/700000: episode: 1014, duration: 1.723s, episode steps: 296, steps per second: 172, episode reward: -93.574, mean reward: -0.316 [-100.000, 16.753], mean action: 1.723 [0.000, 3.000], mean observation: -0.009 [-1.012, 1.000], loss: 12.464196, mean_absolute_error: 49.825161, mean_q: 66.228539\n",
      " 373422/700000: episode: 1015, duration: 0.518s, episode steps: 91, steps per second: 176, episode reward: -3.416, mean reward: -0.038 [-100.000, 15.805], mean action: 1.626 [0.000, 3.000], mean observation: -0.079 [-1.017, 1.000], loss: 5.953549, mean_absolute_error: 50.096016, mean_q: 66.458412\n",
      " 373549/700000: episode: 1016, duration: 0.720s, episode steps: 127, steps per second: 176, episode reward: -47.342, mean reward: -0.373 [-100.000, 12.210], mean action: 1.685 [0.000, 3.000], mean observation: -0.069 [-0.778, 1.363], loss: 9.529568, mean_absolute_error: 50.643959, mean_q: 67.113075\n",
      " 373747/700000: episode: 1017, duration: 1.015s, episode steps: 198, steps per second: 195, episode reward: 236.064, mean reward: 1.192 [-9.773, 100.000], mean action: 1.192 [0.000, 3.000], mean observation: 0.068 [-0.944, 1.000], loss: 9.103732, mean_absolute_error: 50.485291, mean_q: 66.922539\n",
      " 373987/700000: episode: 1018, duration: 1.194s, episode steps: 240, steps per second: 201, episode reward: 203.529, mean reward: 0.848 [-17.498, 100.000], mean action: 0.808 [0.000, 3.000], mean observation: 0.128 [-0.935, 1.030], loss: 9.399929, mean_absolute_error: 50.609806, mean_q: 66.930420\n",
      " 374160/700000: episode: 1019, duration: 0.861s, episode steps: 173, steps per second: 201, episode reward: 213.646, mean reward: 1.235 [-12.244, 100.000], mean action: 1.185 [0.000, 3.000], mean observation: 0.086 [-1.123, 1.000], loss: 12.145767, mean_absolute_error: 50.625587, mean_q: 67.124763\n",
      " 374426/700000: episode: 1020, duration: 1.349s, episode steps: 266, steps per second: 197, episode reward: 255.528, mean reward: 0.961 [-17.953, 100.000], mean action: 1.064 [0.000, 3.000], mean observation: 0.063 [-0.948, 1.000], loss: 10.307593, mean_absolute_error: 50.763260, mean_q: 67.193695\n",
      " 374607/700000: episode: 1021, duration: 0.911s, episode steps: 181, steps per second: 199, episode reward: 218.293, mean reward: 1.206 [-9.592, 100.000], mean action: 1.315 [0.000, 3.000], mean observation: 0.063 [-0.959, 1.000], loss: 5.720837, mean_absolute_error: 50.389652, mean_q: 66.524582\n",
      " 374696/700000: episode: 1022, duration: 0.451s, episode steps: 89, steps per second: 197, episode reward: -9.052, mean reward: -0.102 [-100.000, 23.313], mean action: 1.506 [0.000, 3.000], mean observation: -0.046 [-1.015, 1.152], loss: 7.657914, mean_absolute_error: 50.385155, mean_q: 66.664421\n",
      " 374950/700000: episode: 1023, duration: 1.276s, episode steps: 254, steps per second: 199, episode reward: 220.185, mean reward: 0.867 [-17.549, 100.000], mean action: 0.917 [0.000, 3.000], mean observation: 0.133 [-0.817, 1.000], loss: 7.688440, mean_absolute_error: 50.710293, mean_q: 67.282753\n",
      " 375264/700000: episode: 1024, duration: 1.597s, episode steps: 314, steps per second: 197, episode reward: 259.020, mean reward: 0.825 [-7.454, 100.000], mean action: 0.860 [0.000, 3.000], mean observation: 0.093 [-1.195, 1.000], loss: 8.487726, mean_absolute_error: 50.565971, mean_q: 67.066963\n",
      " 375455/700000: episode: 1025, duration: 0.969s, episode steps: 191, steps per second: 197, episode reward: 256.339, mean reward: 1.342 [-8.883, 100.000], mean action: 1.660 [0.000, 3.000], mean observation: 0.118 [-0.754, 1.111], loss: 6.471208, mean_absolute_error: 50.697247, mean_q: 67.435120\n",
      " 375693/700000: episode: 1026, duration: 1.193s, episode steps: 238, steps per second: 199, episode reward: 263.149, mean reward: 1.106 [-9.572, 100.000], mean action: 1.282 [0.000, 3.000], mean observation: 0.113 [-0.851, 1.015], loss: 10.459746, mean_absolute_error: 51.055206, mean_q: 67.806137\n",
      " 376032/700000: episode: 1027, duration: 1.721s, episode steps: 339, steps per second: 197, episode reward: 220.590, mean reward: 0.651 [-19.028, 100.000], mean action: 0.670 [0.000, 3.000], mean observation: 0.168 [-0.891, 1.000], loss: 9.345545, mean_absolute_error: 51.110439, mean_q: 67.768196\n",
      " 376308/700000: episode: 1028, duration: 1.406s, episode steps: 276, steps per second: 196, episode reward: 239.686, mean reward: 0.868 [-20.038, 100.000], mean action: 1.014 [0.000, 3.000], mean observation: 0.098 [-0.880, 1.000], loss: 7.537639, mean_absolute_error: 51.127182, mean_q: 68.013824\n",
      " 376636/700000: episode: 1029, duration: 1.769s, episode steps: 328, steps per second: 185, episode reward: 210.233, mean reward: 0.641 [-11.173, 100.000], mean action: 1.405 [0.000, 3.000], mean observation: 0.071 [-0.837, 1.000], loss: 7.703442, mean_absolute_error: 51.714546, mean_q: 68.657402\n",
      " 376989/700000: episode: 1030, duration: 1.831s, episode steps: 353, steps per second: 193, episode reward: 226.985, mean reward: 0.643 [-10.334, 100.000], mean action: 0.918 [0.000, 3.000], mean observation: 0.113 [-0.998, 1.000], loss: 6.897498, mean_absolute_error: 51.584995, mean_q: 68.474922\n",
      " 377273/700000: episode: 1031, duration: 1.419s, episode steps: 284, steps per second: 200, episode reward: 252.471, mean reward: 0.889 [-17.897, 100.000], mean action: 1.141 [0.000, 3.000], mean observation: 0.124 [-0.799, 1.012], loss: 5.766004, mean_absolute_error: 51.003113, mean_q: 67.959419\n",
      " 377423/700000: episode: 1032, duration: 0.753s, episode steps: 150, steps per second: 199, episode reward: -26.484, mean reward: -0.177 [-100.000, 7.271], mean action: 1.460 [0.000, 3.000], mean observation: 0.107 [-0.721, 1.205], loss: 11.713068, mean_absolute_error: 51.358074, mean_q: 68.034348\n",
      " 377732/700000: episode: 1033, duration: 1.602s, episode steps: 309, steps per second: 193, episode reward: -170.556, mean reward: -0.552 [-100.000, 25.472], mean action: 1.738 [0.000, 3.000], mean observation: 0.012 [-1.154, 1.000], loss: 11.286154, mean_absolute_error: 51.316311, mean_q: 68.386047\n",
      " 377926/700000: episode: 1034, duration: 0.957s, episode steps: 194, steps per second: 203, episode reward: 225.713, mean reward: 1.163 [-17.644, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.096 [-0.957, 1.000], loss: 13.618775, mean_absolute_error: 51.723984, mean_q: 68.563744\n",
      " 378262/700000: episode: 1035, duration: 1.755s, episode steps: 336, steps per second: 191, episode reward: 233.279, mean reward: 0.694 [-19.520, 100.000], mean action: 1.238 [0.000, 3.000], mean observation: 0.050 [-0.937, 1.000], loss: 10.097960, mean_absolute_error: 51.381512, mean_q: 68.091415\n",
      " 378472/700000: episode: 1036, duration: 1.062s, episode steps: 210, steps per second: 198, episode reward: 238.971, mean reward: 1.138 [-17.915, 100.000], mean action: 1.348 [0.000, 3.000], mean observation: 0.088 [-0.724, 1.000], loss: 7.301164, mean_absolute_error: 50.949409, mean_q: 67.991211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 378753/700000: episode: 1037, duration: 1.463s, episode steps: 281, steps per second: 192, episode reward: 231.222, mean reward: 0.823 [-17.480, 100.000], mean action: 0.890 [0.000, 3.000], mean observation: 0.107 [-0.768, 1.000], loss: 10.075521, mean_absolute_error: 51.687511, mean_q: 68.310867\n",
      " 379165/700000: episode: 1038, duration: 2.136s, episode steps: 412, steps per second: 193, episode reward: 219.482, mean reward: 0.533 [-18.375, 100.000], mean action: 0.995 [0.000, 3.000], mean observation: 0.103 [-0.887, 1.000], loss: 6.205854, mean_absolute_error: 51.118923, mean_q: 67.855858\n",
      " 379647/700000: episode: 1039, duration: 2.515s, episode steps: 482, steps per second: 192, episode reward: 228.961, mean reward: 0.475 [-20.233, 100.000], mean action: 0.612 [0.000, 3.000], mean observation: 0.182 [-0.801, 1.000], loss: 10.308460, mean_absolute_error: 50.629124, mean_q: 67.307671\n",
      " 380126/700000: episode: 1040, duration: 2.752s, episode steps: 479, steps per second: 174, episode reward: 198.861, mean reward: 0.415 [-8.940, 100.000], mean action: 1.637 [0.000, 3.000], mean observation: 0.022 [-0.893, 1.000], loss: 9.197633, mean_absolute_error: 50.834038, mean_q: 67.849617\n",
      " 380347/700000: episode: 1041, duration: 1.113s, episode steps: 221, steps per second: 199, episode reward: 227.139, mean reward: 1.028 [-4.311, 100.000], mean action: 1.149 [0.000, 3.000], mean observation: 0.098 [-1.249, 1.000], loss: 7.503535, mean_absolute_error: 50.827137, mean_q: 67.231270\n",
      " 380616/700000: episode: 1042, duration: 1.385s, episode steps: 269, steps per second: 194, episode reward: -142.856, mean reward: -0.531 [-100.000, 19.902], mean action: 1.647 [0.000, 3.000], mean observation: 0.174 [-0.978, 1.000], loss: 8.940142, mean_absolute_error: 50.652328, mean_q: 67.415123\n",
      " 381013/700000: episode: 1043, duration: 2.015s, episode steps: 397, steps per second: 197, episode reward: 199.829, mean reward: 0.503 [-17.854, 100.000], mean action: 0.680 [0.000, 3.000], mean observation: 0.157 [-0.737, 1.000], loss: 8.349730, mean_absolute_error: 50.742786, mean_q: 67.550804\n",
      " 381342/700000: episode: 1044, duration: 1.708s, episode steps: 329, steps per second: 193, episode reward: 233.885, mean reward: 0.711 [-17.312, 100.000], mean action: 0.903 [0.000, 3.000], mean observation: 0.158 [-0.894, 1.000], loss: 9.336094, mean_absolute_error: 50.915047, mean_q: 67.866653\n",
      " 382011/700000: episode: 1045, duration: 3.668s, episode steps: 669, steps per second: 182, episode reward: 215.029, mean reward: 0.321 [-18.437, 100.000], mean action: 0.858 [0.000, 3.000], mean observation: 0.140 [-0.742, 1.000], loss: 10.280415, mean_absolute_error: 50.739033, mean_q: 67.340797\n",
      " 382431/700000: episode: 1046, duration: 2.142s, episode steps: 420, steps per second: 196, episode reward: 237.272, mean reward: 0.565 [-17.356, 100.000], mean action: 0.879 [0.000, 3.000], mean observation: 0.107 [-0.861, 1.000], loss: 8.954812, mean_absolute_error: 50.283855, mean_q: 66.749077\n",
      " 382632/700000: episode: 1047, duration: 1.026s, episode steps: 201, steps per second: 196, episode reward: 170.111, mean reward: 0.846 [-11.874, 100.000], mean action: 1.846 [0.000, 3.000], mean observation: 0.108 [-0.958, 1.000], loss: 8.412476, mean_absolute_error: 50.519623, mean_q: 67.281433\n",
      " 383014/700000: episode: 1048, duration: 1.995s, episode steps: 382, steps per second: 192, episode reward: 229.121, mean reward: 0.600 [-17.998, 100.000], mean action: 1.610 [0.000, 3.000], mean observation: 0.108 [-0.729, 1.000], loss: 7.812715, mean_absolute_error: 50.585876, mean_q: 67.160065\n",
      " 383341/700000: episode: 1049, duration: 1.702s, episode steps: 327, steps per second: 192, episode reward: 206.070, mean reward: 0.630 [-13.509, 100.000], mean action: 1.284 [0.000, 3.000], mean observation: 0.117 [-0.745, 1.001], loss: 10.436890, mean_absolute_error: 50.387787, mean_q: 66.915070\n",
      " 383643/700000: episode: 1050, duration: 1.543s, episode steps: 302, steps per second: 196, episode reward: 213.377, mean reward: 0.707 [-8.888, 100.000], mean action: 1.255 [0.000, 3.000], mean observation: 0.084 [-1.298, 1.004], loss: 7.259633, mean_absolute_error: 50.424274, mean_q: 66.833260\n",
      " 383866/700000: episode: 1051, duration: 1.129s, episode steps: 223, steps per second: 197, episode reward: 253.310, mean reward: 1.136 [-3.262, 100.000], mean action: 1.345 [0.000, 3.000], mean observation: 0.041 [-0.923, 1.000], loss: 7.316084, mean_absolute_error: 50.199760, mean_q: 66.519081\n",
      " 384244/700000: episode: 1052, duration: 1.948s, episode steps: 378, steps per second: 194, episode reward: 217.739, mean reward: 0.576 [-17.404, 100.000], mean action: 1.243 [0.000, 3.000], mean observation: 0.088 [-0.714, 1.000], loss: 10.316009, mean_absolute_error: 50.042259, mean_q: 66.344353\n",
      " 385052/700000: episode: 1053, duration: 4.301s, episode steps: 808, steps per second: 188, episode reward: 169.178, mean reward: 0.209 [-21.197, 100.000], mean action: 0.668 [0.000, 3.000], mean observation: 0.198 [-1.247, 1.018], loss: 9.326147, mean_absolute_error: 50.057701, mean_q: 66.407097\n",
      " 385325/700000: episode: 1054, duration: 1.404s, episode steps: 273, steps per second: 194, episode reward: 225.911, mean reward: 0.828 [-18.363, 100.000], mean action: 1.601 [0.000, 3.000], mean observation: 0.022 [-1.167, 1.000], loss: 8.236240, mean_absolute_error: 49.857307, mean_q: 66.058113\n",
      " 385803/700000: episode: 1055, duration: 2.579s, episode steps: 478, steps per second: 185, episode reward: 158.792, mean reward: 0.332 [-17.588, 100.000], mean action: 1.803 [0.000, 3.000], mean observation: 0.071 [-1.251, 1.000], loss: 6.970160, mean_absolute_error: 50.144943, mean_q: 66.435188\n",
      " 386057/700000: episode: 1056, duration: 1.276s, episode steps: 254, steps per second: 199, episode reward: 229.576, mean reward: 0.904 [-20.734, 100.000], mean action: 1.181 [0.000, 3.000], mean observation: 0.089 [-0.924, 1.000], loss: 9.120980, mean_absolute_error: 49.937046, mean_q: 66.327446\n",
      " 386379/700000: episode: 1057, duration: 1.685s, episode steps: 322, steps per second: 191, episode reward: 234.426, mean reward: 0.728 [-3.226, 100.000], mean action: 1.512 [0.000, 3.000], mean observation: 0.064 [-1.414, 1.000], loss: 11.306761, mean_absolute_error: 50.556885, mean_q: 67.028275\n",
      " 386619/700000: episode: 1058, duration: 1.236s, episode steps: 240, steps per second: 194, episode reward: 221.038, mean reward: 0.921 [-9.422, 100.000], mean action: 1.092 [0.000, 3.000], mean observation: 0.104 [-0.918, 1.012], loss: 6.116338, mean_absolute_error: 49.728878, mean_q: 65.834366\n",
      " 386951/700000: episode: 1059, duration: 1.734s, episode steps: 332, steps per second: 191, episode reward: -71.042, mean reward: -0.214 [-100.000, 16.393], mean action: 1.708 [0.000, 3.000], mean observation: 0.022 [-1.446, 1.000], loss: 7.622156, mean_absolute_error: 49.949837, mean_q: 66.545731\n",
      " 387307/700000: episode: 1060, duration: 1.778s, episode steps: 356, steps per second: 200, episode reward: 209.759, mean reward: 0.589 [-17.512, 100.000], mean action: 0.725 [0.000, 3.000], mean observation: 0.174 [-1.224, 1.000], loss: 13.071042, mean_absolute_error: 49.752708, mean_q: 66.053879\n",
      " 387421/700000: episode: 1061, duration: 0.580s, episode steps: 114, steps per second: 197, episode reward: -154.112, mean reward: -1.352 [-100.000, 57.367], mean action: 1.737 [0.000, 3.000], mean observation: 0.156 [-0.933, 1.722], loss: 14.459846, mean_absolute_error: 50.403572, mean_q: 67.054230\n",
      " 387607/700000: episode: 1062, duration: 0.933s, episode steps: 186, steps per second: 199, episode reward: 238.421, mean reward: 1.282 [-8.071, 100.000], mean action: 1.065 [0.000, 3.000], mean observation: 0.100 [-0.964, 1.000], loss: 9.207931, mean_absolute_error: 49.985523, mean_q: 66.449516\n",
      " 387874/700000: episode: 1063, duration: 1.351s, episode steps: 267, steps per second: 198, episode reward: 234.098, mean reward: 0.877 [-17.493, 100.000], mean action: 0.978 [0.000, 3.000], mean observation: 0.112 [-0.808, 1.009], loss: 12.567924, mean_absolute_error: 50.083729, mean_q: 66.488777\n",
      " 388152/700000: episode: 1064, duration: 1.427s, episode steps: 278, steps per second: 195, episode reward: 210.732, mean reward: 0.758 [-9.268, 100.000], mean action: 1.112 [0.000, 3.000], mean observation: 0.122 [-0.966, 1.000], loss: 10.964095, mean_absolute_error: 49.778656, mean_q: 66.164650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 388481/700000: episode: 1065, duration: 1.688s, episode steps: 329, steps per second: 195, episode reward: 218.194, mean reward: 0.663 [-20.419, 100.000], mean action: 0.827 [0.000, 3.000], mean observation: 0.165 [-0.927, 1.000], loss: 7.661958, mean_absolute_error: 50.362392, mean_q: 66.952904\n",
      " 388763/700000: episode: 1066, duration: 1.448s, episode steps: 282, steps per second: 195, episode reward: 255.186, mean reward: 0.905 [-8.427, 100.000], mean action: 1.128 [0.000, 3.000], mean observation: 0.096 [-0.889, 1.015], loss: 8.362241, mean_absolute_error: 49.894592, mean_q: 66.394882\n",
      " 388950/700000: episode: 1067, duration: 0.944s, episode steps: 187, steps per second: 198, episode reward: 234.553, mean reward: 1.254 [-9.497, 100.000], mean action: 1.369 [0.000, 3.000], mean observation: 0.091 [-0.794, 1.000], loss: 4.384242, mean_absolute_error: 49.994568, mean_q: 66.239990\n",
      " 389262/700000: episode: 1068, duration: 1.644s, episode steps: 312, steps per second: 190, episode reward: 253.836, mean reward: 0.814 [-17.334, 100.000], mean action: 1.535 [0.000, 3.000], mean observation: 0.035 [-0.775, 1.000], loss: 13.087007, mean_absolute_error: 49.770832, mean_q: 65.980721\n",
      " 389485/700000: episode: 1069, duration: 1.141s, episode steps: 223, steps per second: 195, episode reward: 245.727, mean reward: 1.102 [-17.336, 100.000], mean action: 1.332 [0.000, 3.000], mean observation: 0.118 [-1.331, 1.000], loss: 7.401210, mean_absolute_error: 49.560528, mean_q: 66.014839\n",
      " 389975/700000: episode: 1070, duration: 2.507s, episode steps: 490, steps per second: 195, episode reward: 222.748, mean reward: 0.455 [-19.608, 100.000], mean action: 0.639 [0.000, 3.000], mean observation: 0.174 [-0.924, 1.036], loss: 9.097031, mean_absolute_error: 49.614170, mean_q: 65.992065\n",
      " 390299/700000: episode: 1071, duration: 1.656s, episode steps: 324, steps per second: 196, episode reward: 244.511, mean reward: 0.755 [-18.067, 100.000], mean action: 1.037 [0.000, 3.000], mean observation: 0.124 [-0.770, 1.002], loss: 8.818634, mean_absolute_error: 49.499420, mean_q: 65.937103\n",
      " 390507/700000: episode: 1072, duration: 1.060s, episode steps: 208, steps per second: 196, episode reward: 219.163, mean reward: 1.054 [-10.709, 100.000], mean action: 1.423 [0.000, 3.000], mean observation: 0.012 [-0.897, 1.000], loss: 8.810604, mean_absolute_error: 49.436485, mean_q: 65.724602\n",
      " 390787/700000: episode: 1073, duration: 1.421s, episode steps: 280, steps per second: 197, episode reward: 236.306, mean reward: 0.844 [-9.439, 100.000], mean action: 1.382 [0.000, 3.000], mean observation: 0.071 [-0.767, 1.000], loss: 6.728467, mean_absolute_error: 49.489643, mean_q: 66.084290\n",
      " 391086/700000: episode: 1074, duration: 1.564s, episode steps: 299, steps per second: 191, episode reward: 197.734, mean reward: 0.661 [-17.352, 100.000], mean action: 1.251 [0.000, 3.000], mean observation: 0.134 [-0.897, 1.000], loss: 10.497014, mean_absolute_error: 50.282177, mean_q: 66.778015\n",
      " 391410/700000: episode: 1075, duration: 1.651s, episode steps: 324, steps per second: 196, episode reward: 238.903, mean reward: 0.737 [-8.766, 100.000], mean action: 1.056 [0.000, 3.000], mean observation: 0.039 [-0.912, 1.000], loss: 7.220446, mean_absolute_error: 49.542114, mean_q: 65.978996\n",
      " 391587/700000: episode: 1076, duration: 0.897s, episode steps: 177, steps per second: 197, episode reward: 250.049, mean reward: 1.413 [-6.502, 100.000], mean action: 1.503 [0.000, 3.000], mean observation: 0.058 [-0.777, 1.000], loss: 11.832600, mean_absolute_error: 50.034081, mean_q: 66.297409\n",
      " 391796/700000: episode: 1077, duration: 1.095s, episode steps: 209, steps per second: 191, episode reward: 227.002, mean reward: 1.086 [-10.406, 100.000], mean action: 1.124 [0.000, 3.000], mean observation: 0.115 [-0.904, 1.000], loss: 7.725659, mean_absolute_error: 49.961369, mean_q: 66.049896\n",
      " 392173/700000: episode: 1078, duration: 2.163s, episode steps: 377, steps per second: 174, episode reward: 233.071, mean reward: 0.618 [-10.732, 100.000], mean action: 1.432 [0.000, 3.000], mean observation: 0.087 [-0.782, 1.000], loss: 10.445809, mean_absolute_error: 49.532425, mean_q: 65.721321\n",
      " 392471/700000: episode: 1079, duration: 1.560s, episode steps: 298, steps per second: 191, episode reward: 234.086, mean reward: 0.786 [-12.315, 100.000], mean action: 1.218 [0.000, 3.000], mean observation: 0.100 [-0.774, 1.000], loss: 8.723643, mean_absolute_error: 49.572113, mean_q: 65.892967\n",
      " 392592/700000: episode: 1080, duration: 0.639s, episode steps: 121, steps per second: 189, episode reward: -5.872, mean reward: -0.049 [-100.000, 16.141], mean action: 1.760 [0.000, 3.000], mean observation: -0.040 [-0.824, 1.000], loss: 8.616789, mean_absolute_error: 49.681274, mean_q: 65.908989\n",
      " 392991/700000: episode: 1081, duration: 2.319s, episode steps: 399, steps per second: 172, episode reward: 171.015, mean reward: 0.429 [-20.193, 100.000], mean action: 2.276 [0.000, 3.000], mean observation: 0.132 [-0.958, 1.000], loss: 8.124159, mean_absolute_error: 49.694244, mean_q: 65.956100\n",
      " 393238/700000: episode: 1082, duration: 1.341s, episode steps: 247, steps per second: 184, episode reward: 235.480, mean reward: 0.953 [-18.216, 100.000], mean action: 0.798 [0.000, 3.000], mean observation: 0.143 [-1.259, 1.000], loss: 6.568126, mean_absolute_error: 49.572880, mean_q: 65.872200\n",
      " 393782/700000: episode: 1083, duration: 3.056s, episode steps: 544, steps per second: 178, episode reward: 196.780, mean reward: 0.362 [-17.500, 100.000], mean action: 1.097 [0.000, 3.000], mean observation: 0.180 [-0.836, 1.000], loss: 8.593292, mean_absolute_error: 49.667885, mean_q: 65.812012\n",
      " 394094/700000: episode: 1084, duration: 1.649s, episode steps: 312, steps per second: 189, episode reward: 242.134, mean reward: 0.776 [-3.807, 100.000], mean action: 1.176 [0.000, 3.000], mean observation: 0.077 [-0.623, 1.000], loss: 10.086166, mean_absolute_error: 49.770359, mean_q: 65.866135\n",
      " 394917/700000: episode: 1085, duration: 4.488s, episode steps: 823, steps per second: 183, episode reward: 219.339, mean reward: 0.267 [-17.719, 100.000], mean action: 0.762 [0.000, 3.000], mean observation: 0.139 [-0.627, 1.081], loss: 9.509380, mean_absolute_error: 49.357040, mean_q: 65.355606\n",
      " 395762/700000: episode: 1086, duration: 4.592s, episode steps: 845, steps per second: 184, episode reward: 138.490, mean reward: 0.164 [-19.108, 100.000], mean action: 1.975 [0.000, 3.000], mean observation: 0.178 [-0.727, 1.000], loss: 7.605234, mean_absolute_error: 49.584080, mean_q: 65.734451\n",
      " 396155/700000: episode: 1087, duration: 2.012s, episode steps: 393, steps per second: 195, episode reward: 248.262, mean reward: 0.632 [-17.892, 100.000], mean action: 1.010 [0.000, 3.000], mean observation: 0.096 [-0.793, 1.000], loss: 7.604613, mean_absolute_error: 49.627510, mean_q: 65.685608\n",
      " 396506/700000: episode: 1088, duration: 1.810s, episode steps: 351, steps per second: 194, episode reward: 248.385, mean reward: 0.708 [-17.749, 100.000], mean action: 1.231 [0.000, 3.000], mean observation: 0.129 [-0.690, 1.214], loss: 7.822631, mean_absolute_error: 49.309361, mean_q: 65.631943\n",
      " 396729/700000: episode: 1089, duration: 1.131s, episode steps: 223, steps per second: 197, episode reward: 232.262, mean reward: 1.042 [-17.332, 100.000], mean action: 1.148 [0.000, 3.000], mean observation: 0.092 [-0.918, 1.028], loss: 10.506792, mean_absolute_error: 49.521248, mean_q: 65.762573\n",
      " 397112/700000: episode: 1090, duration: 1.998s, episode steps: 383, steps per second: 192, episode reward: 222.246, mean reward: 0.580 [-9.651, 100.000], mean action: 1.444 [0.000, 3.000], mean observation: 0.033 [-0.867, 1.000], loss: 11.288411, mean_absolute_error: 49.647858, mean_q: 66.004929\n",
      " 397309/700000: episode: 1091, duration: 0.987s, episode steps: 197, steps per second: 200, episode reward: 227.776, mean reward: 1.156 [-18.895, 100.000], mean action: 1.208 [0.000, 3.000], mean observation: 0.095 [-0.872, 1.000], loss: 10.072502, mean_absolute_error: 49.898258, mean_q: 66.022820\n",
      " 397667/700000: episode: 1092, duration: 1.920s, episode steps: 358, steps per second: 186, episode reward: 243.114, mean reward: 0.679 [-5.259, 100.000], mean action: 1.754 [0.000, 3.000], mean observation: 0.043 [-1.380, 1.000], loss: 8.319520, mean_absolute_error: 49.825104, mean_q: 66.208076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 397910/700000: episode: 1093, duration: 1.212s, episode steps: 243, steps per second: 200, episode reward: 230.415, mean reward: 0.948 [-10.843, 100.000], mean action: 0.914 [0.000, 3.000], mean observation: 0.088 [-1.055, 1.000], loss: 6.923079, mean_absolute_error: 49.971489, mean_q: 66.519676\n",
      " 398180/700000: episode: 1094, duration: 1.441s, episode steps: 270, steps per second: 187, episode reward: 211.337, mean reward: 0.783 [-9.956, 100.000], mean action: 1.285 [0.000, 3.000], mean observation: 0.118 [-0.907, 1.000], loss: 6.446816, mean_absolute_error: 50.410225, mean_q: 66.807770\n",
      " 398459/700000: episode: 1095, duration: 1.419s, episode steps: 279, steps per second: 197, episode reward: 248.524, mean reward: 0.891 [-10.334, 100.000], mean action: 1.434 [0.000, 3.000], mean observation: 0.046 [-0.780, 1.000], loss: 6.297441, mean_absolute_error: 49.825127, mean_q: 66.386253\n",
      " 398672/700000: episode: 1096, duration: 1.063s, episode steps: 213, steps per second: 200, episode reward: 204.285, mean reward: 0.959 [-8.643, 100.000], mean action: 0.892 [0.000, 3.000], mean observation: 0.084 [-0.933, 1.258], loss: 8.487482, mean_absolute_error: 50.485195, mean_q: 66.973511\n",
      " 398813/700000: episode: 1097, duration: 0.699s, episode steps: 141, steps per second: 202, episode reward: 240.290, mean reward: 1.704 [-3.045, 100.000], mean action: 1.170 [0.000, 3.000], mean observation: 0.058 [-1.017, 1.000], loss: 7.372428, mean_absolute_error: 50.123138, mean_q: 66.518150\n",
      " 399017/700000: episode: 1098, duration: 1.021s, episode steps: 204, steps per second: 200, episode reward: 198.939, mean reward: 0.975 [-17.451, 100.000], mean action: 0.868 [0.000, 3.000], mean observation: 0.125 [-0.998, 1.000], loss: 11.007407, mean_absolute_error: 50.308514, mean_q: 66.905525\n",
      " 399513/700000: episode: 1099, duration: 2.614s, episode steps: 496, steps per second: 190, episode reward: 254.072, mean reward: 0.512 [-19.457, 100.000], mean action: 0.929 [0.000, 3.000], mean observation: 0.138 [-1.015, 1.006], loss: 7.695212, mean_absolute_error: 50.186310, mean_q: 66.524178\n",
      " 399754/700000: episode: 1100, duration: 1.217s, episode steps: 241, steps per second: 198, episode reward: -107.040, mean reward: -0.444 [-100.000, 11.825], mean action: 1.411 [0.000, 3.000], mean observation: 0.140 [-1.108, 1.000], loss: 8.924788, mean_absolute_error: 50.653107, mean_q: 67.299911\n",
      " 400097/700000: episode: 1101, duration: 1.730s, episode steps: 343, steps per second: 198, episode reward: 208.513, mean reward: 0.608 [-17.333, 100.000], mean action: 0.676 [0.000, 3.000], mean observation: 0.161 [-1.198, 1.000], loss: 7.670599, mean_absolute_error: 50.549641, mean_q: 67.015366\n",
      " 400754/700000: episode: 1102, duration: 3.339s, episode steps: 657, steps per second: 197, episode reward: 228.291, mean reward: 0.347 [-19.865, 100.000], mean action: 0.469 [0.000, 3.000], mean observation: 0.197 [-1.021, 1.030], loss: 8.255394, mean_absolute_error: 50.257362, mean_q: 66.790695\n",
      " 400965/700000: episode: 1103, duration: 1.078s, episode steps: 211, steps per second: 196, episode reward: 245.197, mean reward: 1.162 [-19.684, 100.000], mean action: 1.237 [0.000, 3.000], mean observation: 0.064 [-0.782, 1.270], loss: 5.574015, mean_absolute_error: 50.462132, mean_q: 66.929626\n",
      " 401313/700000: episode: 1104, duration: 1.771s, episode steps: 348, steps per second: 197, episode reward: 236.019, mean reward: 0.678 [-10.463, 100.000], mean action: 0.922 [0.000, 3.000], mean observation: 0.169 [-0.897, 1.000], loss: 8.304058, mean_absolute_error: 50.593842, mean_q: 67.549309\n",
      " 401600/700000: episode: 1105, duration: 1.484s, episode steps: 287, steps per second: 193, episode reward: 277.493, mean reward: 0.967 [-18.789, 100.000], mean action: 1.206 [0.000, 3.000], mean observation: 0.149 [-0.958, 1.000], loss: 9.808748, mean_absolute_error: 50.589027, mean_q: 67.472961\n",
      " 401901/700000: episode: 1106, duration: 1.546s, episode steps: 301, steps per second: 195, episode reward: 239.900, mean reward: 0.797 [-19.707, 100.000], mean action: 1.355 [0.000, 3.000], mean observation: 0.114 [-0.831, 1.000], loss: 10.631353, mean_absolute_error: 50.580456, mean_q: 67.631371\n",
      " 402143/700000: episode: 1107, duration: 1.212s, episode steps: 242, steps per second: 200, episode reward: 208.789, mean reward: 0.863 [-19.156, 100.000], mean action: 1.021 [0.000, 3.000], mean observation: 0.130 [-1.127, 1.000], loss: 7.150590, mean_absolute_error: 50.230629, mean_q: 66.952896\n",
      " 402713/700000: episode: 1108, duration: 2.976s, episode steps: 570, steps per second: 192, episode reward: 217.219, mean reward: 0.381 [-19.545, 100.000], mean action: 1.119 [0.000, 3.000], mean observation: 0.151 [-0.791, 1.000], loss: 11.144372, mean_absolute_error: 50.340637, mean_q: 67.143288\n",
      " 403108/700000: episode: 1109, duration: 2.045s, episode steps: 395, steps per second: 193, episode reward: 236.009, mean reward: 0.597 [-18.388, 100.000], mean action: 0.975 [0.000, 3.000], mean observation: 0.035 [-0.811, 1.000], loss: 8.803154, mean_absolute_error: 50.591984, mean_q: 67.289207\n",
      " 403400/700000: episode: 1110, duration: 1.485s, episode steps: 292, steps per second: 197, episode reward: 195.454, mean reward: 0.669 [-13.329, 100.000], mean action: 0.863 [0.000, 3.000], mean observation: 0.148 [-0.984, 1.000], loss: 14.141757, mean_absolute_error: 50.607182, mean_q: 67.329262\n",
      " 403584/700000: episode: 1111, duration: 0.923s, episode steps: 184, steps per second: 199, episode reward: 218.307, mean reward: 1.186 [-3.200, 100.000], mean action: 1.293 [0.000, 3.000], mean observation: 0.056 [-0.993, 1.000], loss: 9.315973, mean_absolute_error: 50.196774, mean_q: 67.022079\n",
      " 404584/700000: episode: 1112, duration: 5.732s, episode steps: 1000, steps per second: 174, episode reward: 47.997, mean reward: 0.048 [-19.356, 20.549], mean action: 1.009 [0.000, 3.000], mean observation: 0.216 [-0.943, 1.000], loss: 9.352864, mean_absolute_error: 50.700562, mean_q: 67.378456\n",
      " 404851/700000: episode: 1113, duration: 1.393s, episode steps: 267, steps per second: 192, episode reward: 231.046, mean reward: 0.865 [-11.495, 100.000], mean action: 1.584 [0.000, 3.000], mean observation: 0.080 [-0.681, 1.000], loss: 9.656369, mean_absolute_error: 50.820255, mean_q: 67.722641\n",
      " 405383/700000: episode: 1114, duration: 2.767s, episode steps: 532, steps per second: 192, episode reward: 218.919, mean reward: 0.412 [-20.206, 100.000], mean action: 1.019 [0.000, 3.000], mean observation: 0.188 [-0.913, 1.000], loss: 6.509603, mean_absolute_error: 50.411751, mean_q: 67.413162\n",
      " 406105/700000: episode: 1115, duration: 3.784s, episode steps: 722, steps per second: 191, episode reward: 250.942, mean reward: 0.348 [-21.961, 100.000], mean action: 0.609 [0.000, 3.000], mean observation: 0.172 [-1.054, 1.000], loss: 8.213250, mean_absolute_error: 50.737816, mean_q: 67.765480\n",
      " 406317/700000: episode: 1116, duration: 1.078s, episode steps: 212, steps per second: 197, episode reward: 260.594, mean reward: 1.229 [-8.979, 100.000], mean action: 1.241 [0.000, 3.000], mean observation: 0.060 [-0.872, 1.000], loss: 10.157323, mean_absolute_error: 50.668022, mean_q: 67.612251\n",
      " 407042/700000: episode: 1117, duration: 4.022s, episode steps: 725, steps per second: 180, episode reward: 177.112, mean reward: 0.244 [-18.811, 100.000], mean action: 0.906 [0.000, 3.000], mean observation: 0.218 [-0.896, 1.000], loss: 8.392902, mean_absolute_error: 50.921219, mean_q: 67.911797\n",
      " 407308/700000: episode: 1118, duration: 1.358s, episode steps: 266, steps per second: 196, episode reward: 213.986, mean reward: 0.804 [-10.272, 100.000], mean action: 1.383 [0.000, 3.000], mean observation: 0.005 [-0.800, 1.000], loss: 6.804327, mean_absolute_error: 50.775146, mean_q: 67.823341\n",
      " 407545/700000: episode: 1119, duration: 1.200s, episode steps: 237, steps per second: 198, episode reward: 229.312, mean reward: 0.968 [-17.751, 100.000], mean action: 1.025 [0.000, 3.000], mean observation: 0.118 [-0.962, 1.000], loss: 6.348453, mean_absolute_error: 50.789650, mean_q: 67.985947\n",
      " 407834/700000: episode: 1120, duration: 1.479s, episode steps: 289, steps per second: 195, episode reward: 248.334, mean reward: 0.859 [-9.967, 100.000], mean action: 1.346 [0.000, 3.000], mean observation: 0.064 [-0.741, 1.087], loss: 9.343072, mean_absolute_error: 51.020672, mean_q: 68.133377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 408319/700000: episode: 1121, duration: 2.648s, episode steps: 485, steps per second: 183, episode reward: 233.895, mean reward: 0.482 [-22.204, 100.000], mean action: 1.390 [0.000, 3.000], mean observation: 0.119 [-0.863, 1.008], loss: 11.807820, mean_absolute_error: 50.523674, mean_q: 67.421394\n",
      " 408846/700000: episode: 1122, duration: 2.709s, episode steps: 527, steps per second: 195, episode reward: 232.585, mean reward: 0.441 [-20.302, 100.000], mean action: 0.554 [0.000, 3.000], mean observation: 0.199 [-0.961, 1.000], loss: 6.761757, mean_absolute_error: 50.888382, mean_q: 67.939445\n",
      " 409037/700000: episode: 1123, duration: 0.951s, episode steps: 191, steps per second: 201, episode reward: 214.282, mean reward: 1.122 [-7.919, 100.000], mean action: 1.058 [0.000, 3.000], mean observation: 0.063 [-0.951, 1.000], loss: 6.185714, mean_absolute_error: 51.453125, mean_q: 68.592537\n",
      " 409417/700000: episode: 1124, duration: 1.972s, episode steps: 380, steps per second: 193, episode reward: 218.059, mean reward: 0.574 [-18.912, 100.000], mean action: 1.150 [0.000, 3.000], mean observation: 0.165 [-0.994, 1.000], loss: 10.976155, mean_absolute_error: 51.768982, mean_q: 68.989906\n",
      " 409656/700000: episode: 1125, duration: 1.213s, episode steps: 239, steps per second: 197, episode reward: 207.476, mean reward: 0.868 [-10.177, 100.000], mean action: 0.983 [0.000, 3.000], mean observation: 0.093 [-0.949, 1.000], loss: 11.444888, mean_absolute_error: 51.564259, mean_q: 68.965446\n",
      " 409999/700000: episode: 1126, duration: 1.765s, episode steps: 343, steps per second: 194, episode reward: 218.620, mean reward: 0.637 [-17.809, 100.000], mean action: 1.499 [0.000, 3.000], mean observation: 0.044 [-0.641, 1.000], loss: 6.032229, mean_absolute_error: 51.745785, mean_q: 68.923058\n",
      " 410294/700000: episode: 1127, duration: 1.493s, episode steps: 295, steps per second: 198, episode reward: 223.134, mean reward: 0.756 [-18.018, 100.000], mean action: 1.163 [0.000, 3.000], mean observation: 0.115 [-0.938, 1.000], loss: 9.483998, mean_absolute_error: 51.758816, mean_q: 69.092796\n",
      " 410398/700000: episode: 1128, duration: 0.525s, episode steps: 104, steps per second: 198, episode reward: -11.018, mean reward: -0.106 [-100.000, 24.664], mean action: 1.692 [0.000, 3.000], mean observation: -0.032 [-0.860, 1.000], loss: 10.192678, mean_absolute_error: 51.956863, mean_q: 69.392479\n",
      " 410859/700000: episode: 1129, duration: 2.359s, episode steps: 461, steps per second: 195, episode reward: 232.260, mean reward: 0.504 [-20.936, 100.000], mean action: 0.729 [0.000, 3.000], mean observation: 0.183 [-0.981, 1.000], loss: 8.432488, mean_absolute_error: 51.943329, mean_q: 68.996719\n",
      " 411216/700000: episode: 1130, duration: 1.866s, episode steps: 357, steps per second: 191, episode reward: 207.351, mean reward: 0.581 [-11.164, 100.000], mean action: 1.053 [0.000, 3.000], mean observation: 0.099 [-0.886, 1.000], loss: 7.654085, mean_absolute_error: 51.972252, mean_q: 69.373672\n",
      " 411391/700000: episode: 1131, duration: 0.887s, episode steps: 175, steps per second: 197, episode reward: 236.724, mean reward: 1.353 [-12.794, 100.000], mean action: 1.240 [0.000, 3.000], mean observation: 0.078 [-1.002, 1.000], loss: 9.961676, mean_absolute_error: 52.034298, mean_q: 69.118324\n",
      " 411943/700000: episode: 1132, duration: 2.840s, episode steps: 552, steps per second: 194, episode reward: 173.591, mean reward: 0.314 [-19.399, 100.000], mean action: 0.833 [0.000, 3.000], mean observation: 0.185 [-1.147, 1.000], loss: 7.646772, mean_absolute_error: 51.757347, mean_q: 68.976517\n",
      " 412119/700000: episode: 1133, duration: 0.881s, episode steps: 176, steps per second: 200, episode reward: 182.851, mean reward: 1.039 [-9.589, 100.000], mean action: 1.097 [0.000, 3.000], mean observation: 0.055 [-0.964, 1.000], loss: 6.133778, mean_absolute_error: 52.115673, mean_q: 69.369705\n",
      " 412439/700000: episode: 1134, duration: 1.637s, episode steps: 320, steps per second: 196, episode reward: 248.650, mean reward: 0.777 [-17.537, 100.000], mean action: 1.184 [0.000, 3.000], mean observation: 0.089 [-0.638, 1.000], loss: 10.486684, mean_absolute_error: 52.096169, mean_q: 69.465660\n",
      " 412664/700000: episode: 1135, duration: 1.139s, episode steps: 225, steps per second: 198, episode reward: 236.184, mean reward: 1.050 [-9.094, 100.000], mean action: 1.293 [0.000, 3.000], mean observation: 0.088 [-0.858, 1.000], loss: 7.034525, mean_absolute_error: 51.906368, mean_q: 69.086952\n",
      " 413161/700000: episode: 1136, duration: 2.685s, episode steps: 497, steps per second: 185, episode reward: 195.094, mean reward: 0.393 [-19.421, 100.000], mean action: 1.946 [0.000, 3.000], mean observation: 0.133 [-1.033, 1.000], loss: 8.782736, mean_absolute_error: 52.034298, mean_q: 69.090904\n",
      " 413428/700000: episode: 1137, duration: 1.371s, episode steps: 267, steps per second: 195, episode reward: 178.351, mean reward: 0.668 [-17.613, 100.000], mean action: 1.318 [0.000, 3.000], mean observation: 0.142 [-0.950, 1.000], loss: 5.826572, mean_absolute_error: 51.794098, mean_q: 68.799904\n",
      " 413724/700000: episode: 1138, duration: 1.488s, episode steps: 296, steps per second: 199, episode reward: 205.240, mean reward: 0.693 [-7.531, 100.000], mean action: 1.071 [0.000, 3.000], mean observation: 0.156 [-0.928, 1.000], loss: 6.848579, mean_absolute_error: 51.720718, mean_q: 68.761040\n",
      " 414578/700000: episode: 1139, duration: 5.000s, episode steps: 854, steps per second: 171, episode reward: 240.174, mean reward: 0.281 [-21.060, 100.000], mean action: 2.025 [0.000, 3.000], mean observation: 0.247 [-0.803, 1.000], loss: 8.702900, mean_absolute_error: 51.671478, mean_q: 68.711273\n",
      " 415086/700000: episode: 1140, duration: 2.861s, episode steps: 508, steps per second: 178, episode reward: 227.738, mean reward: 0.448 [-18.033, 100.000], mean action: 0.579 [0.000, 3.000], mean observation: 0.164 [-0.897, 1.000], loss: 7.298068, mean_absolute_error: 51.826233, mean_q: 68.878601\n",
      " 415308/700000: episode: 1141, duration: 1.153s, episode steps: 222, steps per second: 193, episode reward: 208.339, mean reward: 0.938 [-17.581, 100.000], mean action: 1.041 [0.000, 3.000], mean observation: 0.120 [-0.900, 1.000], loss: 8.314489, mean_absolute_error: 51.663414, mean_q: 68.841614\n",
      " 415620/700000: episode: 1142, duration: 1.753s, episode steps: 312, steps per second: 178, episode reward: 211.760, mean reward: 0.679 [-19.980, 100.000], mean action: 0.917 [0.000, 3.000], mean observation: 0.140 [-0.960, 1.000], loss: 10.705058, mean_absolute_error: 52.005352, mean_q: 68.931694\n",
      " 415912/700000: episode: 1143, duration: 1.595s, episode steps: 292, steps per second: 183, episode reward: 238.087, mean reward: 0.815 [-18.776, 100.000], mean action: 1.466 [0.000, 3.000], mean observation: 0.102 [-0.822, 1.091], loss: 7.913115, mean_absolute_error: 51.838512, mean_q: 68.878967\n",
      " 416346/700000: episode: 1144, duration: 2.404s, episode steps: 434, steps per second: 181, episode reward: 194.096, mean reward: 0.447 [-19.118, 100.000], mean action: 0.968 [0.000, 3.000], mean observation: 0.180 [-1.001, 1.000], loss: 8.019010, mean_absolute_error: 51.836361, mean_q: 68.979752\n",
      " 416630/700000: episode: 1145, duration: 1.486s, episode steps: 284, steps per second: 191, episode reward: 267.550, mean reward: 0.942 [-3.123, 100.000], mean action: 0.835 [0.000, 3.000], mean observation: 0.126 [-0.886, 1.000], loss: 8.554762, mean_absolute_error: 51.671139, mean_q: 68.770752\n",
      " 416982/700000: episode: 1146, duration: 1.818s, episode steps: 352, steps per second: 194, episode reward: 207.725, mean reward: 0.590 [-18.521, 100.000], mean action: 0.685 [0.000, 3.000], mean observation: 0.182 [-0.898, 1.000], loss: 5.608629, mean_absolute_error: 51.797886, mean_q: 68.855949\n",
      " 417435/700000: episode: 1147, duration: 2.377s, episode steps: 453, steps per second: 191, episode reward: 254.906, mean reward: 0.563 [-10.312, 100.000], mean action: 1.143 [0.000, 3.000], mean observation: 0.090 [-0.888, 1.025], loss: 7.153009, mean_absolute_error: 51.685493, mean_q: 68.614426\n",
      " 417657/700000: episode: 1148, duration: 1.134s, episode steps: 222, steps per second: 196, episode reward: -260.956, mean reward: -1.175 [-100.000, 11.692], mean action: 1.905 [0.000, 3.000], mean observation: -0.009 [-2.106, 1.000], loss: 9.330277, mean_absolute_error: 51.726063, mean_q: 68.562294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 417886/700000: episode: 1149, duration: 1.149s, episode steps: 229, steps per second: 199, episode reward: 212.806, mean reward: 0.929 [-8.127, 100.000], mean action: 1.293 [0.000, 3.000], mean observation: 0.112 [-1.102, 1.000], loss: 8.062881, mean_absolute_error: 51.609703, mean_q: 68.496788\n",
      " 418106/700000: episode: 1150, duration: 1.112s, episode steps: 220, steps per second: 198, episode reward: 203.727, mean reward: 0.926 [-17.600, 100.000], mean action: 1.245 [0.000, 3.000], mean observation: 0.112 [-0.866, 1.093], loss: 9.357922, mean_absolute_error: 51.419376, mean_q: 68.504753\n",
      " 418323/700000: episode: 1151, duration: 1.088s, episode steps: 217, steps per second: 199, episode reward: 255.712, mean reward: 1.178 [-7.663, 100.000], mean action: 1.346 [0.000, 3.000], mean observation: 0.124 [-0.652, 1.000], loss: 4.958746, mean_absolute_error: 51.425880, mean_q: 68.488190\n",
      " 418589/700000: episode: 1152, duration: 1.347s, episode steps: 266, steps per second: 197, episode reward: 201.147, mean reward: 0.756 [-12.334, 100.000], mean action: 0.959 [0.000, 3.000], mean observation: 0.117 [-0.925, 1.000], loss: 6.462557, mean_absolute_error: 51.419426, mean_q: 68.550491\n",
      " 418815/700000: episode: 1153, duration: 1.159s, episode steps: 226, steps per second: 195, episode reward: -215.661, mean reward: -0.954 [-100.000, 12.538], mean action: 1.580 [0.000, 3.000], mean observation: 0.003 [-2.063, 1.000], loss: 6.861592, mean_absolute_error: 51.318913, mean_q: 68.075592\n",
      " 419290/700000: episode: 1154, duration: 2.443s, episode steps: 475, steps per second: 194, episode reward: 236.679, mean reward: 0.498 [-18.453, 100.000], mean action: 0.760 [0.000, 3.000], mean observation: 0.145 [-0.925, 1.000], loss: 7.560594, mean_absolute_error: 51.201153, mean_q: 68.300804\n",
      " 419817/700000: episode: 1155, duration: 2.695s, episode steps: 527, steps per second: 196, episode reward: 254.387, mean reward: 0.483 [-18.382, 100.000], mean action: 0.622 [0.000, 3.000], mean observation: 0.161 [-0.915, 1.000], loss: 9.123458, mean_absolute_error: 51.443794, mean_q: 68.600647\n",
      " 420203/700000: episode: 1156, duration: 1.988s, episode steps: 386, steps per second: 194, episode reward: 235.003, mean reward: 0.609 [-20.101, 100.000], mean action: 0.777 [0.000, 3.000], mean observation: 0.160 [-1.213, 1.000], loss: 11.745595, mean_absolute_error: 51.820107, mean_q: 69.166122\n",
      " 420479/700000: episode: 1157, duration: 1.401s, episode steps: 276, steps per second: 197, episode reward: 246.866, mean reward: 0.894 [-9.193, 100.000], mean action: 1.210 [0.000, 3.000], mean observation: 0.110 [-0.620, 1.012], loss: 10.196348, mean_absolute_error: 51.934330, mean_q: 69.049477\n",
      " 420898/700000: episode: 1158, duration: 2.168s, episode steps: 419, steps per second: 193, episode reward: 189.810, mean reward: 0.453 [-18.431, 100.000], mean action: 0.780 [0.000, 3.000], mean observation: 0.204 [-0.956, 1.169], loss: 6.998894, mean_absolute_error: 51.549316, mean_q: 68.473640\n",
      " 421796/700000: episode: 1159, duration: 4.960s, episode steps: 898, steps per second: 181, episode reward: 180.903, mean reward: 0.201 [-19.621, 100.000], mean action: 0.855 [0.000, 3.000], mean observation: 0.230 [-0.866, 1.000], loss: 7.910581, mean_absolute_error: 51.753548, mean_q: 68.708969\n",
      " 422050/700000: episode: 1160, duration: 1.301s, episode steps: 254, steps per second: 195, episode reward: 265.932, mean reward: 1.047 [-9.129, 100.000], mean action: 1.339 [0.000, 3.000], mean observation: 0.119 [-0.949, 1.091], loss: 9.675944, mean_absolute_error: 51.552803, mean_q: 68.417923\n",
      " 422488/700000: episode: 1161, duration: 2.323s, episode steps: 438, steps per second: 189, episode reward: 166.618, mean reward: 0.380 [-9.636, 100.000], mean action: 1.301 [0.000, 3.000], mean observation: 0.133 [-0.984, 1.068], loss: 9.695683, mean_absolute_error: 51.743183, mean_q: 68.565567\n",
      " 422735/700000: episode: 1162, duration: 1.256s, episode steps: 247, steps per second: 197, episode reward: 189.513, mean reward: 0.767 [-11.430, 100.000], mean action: 1.441 [0.000, 3.000], mean observation: 0.087 [-0.966, 1.000], loss: 7.855279, mean_absolute_error: 51.277100, mean_q: 68.032593\n",
      " 422976/700000: episode: 1163, duration: 1.225s, episode steps: 241, steps per second: 197, episode reward: 198.949, mean reward: 0.826 [-9.177, 100.000], mean action: 1.332 [0.000, 3.000], mean observation: 0.036 [-0.959, 1.000], loss: 11.004352, mean_absolute_error: 51.061756, mean_q: 67.927422\n",
      " 423323/700000: episode: 1164, duration: 1.786s, episode steps: 347, steps per second: 194, episode reward: 230.058, mean reward: 0.663 [-9.757, 100.000], mean action: 1.184 [0.000, 3.000], mean observation: 0.031 [-0.925, 1.000], loss: 7.302356, mean_absolute_error: 51.183876, mean_q: 68.076988\n",
      " 423519/700000: episode: 1165, duration: 0.980s, episode steps: 196, steps per second: 200, episode reward: 216.002, mean reward: 1.102 [-2.977, 100.000], mean action: 1.102 [0.000, 3.000], mean observation: 0.082 [-0.969, 1.000], loss: 6.287556, mean_absolute_error: 51.140804, mean_q: 67.323792\n",
      " 424032/700000: episode: 1166, duration: 2.653s, episode steps: 513, steps per second: 193, episode reward: 221.682, mean reward: 0.432 [-19.671, 100.000], mean action: 0.723 [0.000, 3.000], mean observation: 0.211 [-0.987, 1.000], loss: 7.282018, mean_absolute_error: 51.415344, mean_q: 68.196274\n",
      " 424349/700000: episode: 1167, duration: 1.628s, episode steps: 317, steps per second: 195, episode reward: 227.951, mean reward: 0.719 [-17.301, 100.000], mean action: 1.401 [0.000, 3.000], mean observation: 0.082 [-0.955, 1.000], loss: 6.515015, mean_absolute_error: 51.751450, mean_q: 68.476036\n",
      " 424647/700000: episode: 1168, duration: 1.746s, episode steps: 298, steps per second: 171, episode reward: 217.443, mean reward: 0.730 [-17.312, 100.000], mean action: 2.403 [0.000, 3.000], mean observation: 0.119 [-0.864, 1.000], loss: 9.113340, mean_absolute_error: 51.647945, mean_q: 68.813545\n",
      " 424849/700000: episode: 1169, duration: 1.014s, episode steps: 202, steps per second: 199, episode reward: 238.481, mean reward: 1.181 [-17.460, 100.000], mean action: 0.955 [0.000, 3.000], mean observation: 0.119 [-1.014, 1.000], loss: 10.537969, mean_absolute_error: 51.614769, mean_q: 68.451347\n",
      " 425163/700000: episode: 1170, duration: 1.578s, episode steps: 314, steps per second: 199, episode reward: 247.433, mean reward: 0.788 [-10.535, 100.000], mean action: 1.016 [0.000, 3.000], mean observation: 0.104 [-0.968, 1.119], loss: 9.080608, mean_absolute_error: 51.711105, mean_q: 68.739883\n",
      " 425288/700000: episode: 1171, duration: 0.623s, episode steps: 125, steps per second: 201, episode reward: -13.068, mean reward: -0.105 [-100.000, 14.170], mean action: 1.608 [0.000, 3.000], mean observation: 0.056 [-0.928, 1.000], loss: 6.840085, mean_absolute_error: 51.955711, mean_q: 68.750389\n",
      " 425486/700000: episode: 1172, duration: 1.000s, episode steps: 198, steps per second: 198, episode reward: 233.181, mean reward: 1.178 [-6.433, 100.000], mean action: 1.076 [0.000, 3.000], mean observation: 0.100 [-0.919, 1.000], loss: 5.288880, mean_absolute_error: 52.314095, mean_q: 69.586845\n",
      " 425650/700000: episode: 1173, duration: 0.820s, episode steps: 164, steps per second: 200, episode reward: 224.495, mean reward: 1.369 [-3.305, 100.000], mean action: 1.079 [0.000, 3.000], mean observation: 0.110 [-1.002, 1.000], loss: 11.893923, mean_absolute_error: 51.976341, mean_q: 69.176613\n",
      " 425878/700000: episode: 1174, duration: 1.134s, episode steps: 228, steps per second: 201, episode reward: 218.103, mean reward: 0.957 [-7.990, 100.000], mean action: 0.886 [0.000, 3.000], mean observation: 0.147 [-1.002, 1.000], loss: 7.544127, mean_absolute_error: 52.127121, mean_q: 69.076790\n",
      " 426154/700000: episode: 1175, duration: 1.398s, episode steps: 276, steps per second: 197, episode reward: 209.390, mean reward: 0.759 [-3.217, 100.000], mean action: 0.946 [0.000, 3.000], mean observation: 0.126 [-0.822, 1.000], loss: 7.313401, mean_absolute_error: 52.296295, mean_q: 69.496826\n",
      " 426340/700000: episode: 1176, duration: 0.948s, episode steps: 186, steps per second: 196, episode reward: 215.245, mean reward: 1.157 [-7.283, 100.000], mean action: 1.435 [0.000, 3.000], mean observation: 0.067 [-0.995, 1.000], loss: 12.810053, mean_absolute_error: 52.429161, mean_q: 69.586143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 426607/700000: episode: 1177, duration: 1.342s, episode steps: 267, steps per second: 199, episode reward: 225.581, mean reward: 0.845 [-3.780, 100.000], mean action: 0.914 [0.000, 3.000], mean observation: 0.150 [-0.950, 1.000], loss: 10.733925, mean_absolute_error: 52.156048, mean_q: 69.288620\n",
      " 427038/700000: episode: 1178, duration: 2.339s, episode steps: 431, steps per second: 184, episode reward: 217.643, mean reward: 0.505 [-19.332, 100.000], mean action: 1.183 [0.000, 3.000], mean observation: 0.134 [-0.959, 1.036], loss: 7.844600, mean_absolute_error: 52.153339, mean_q: 69.292870\n",
      " 427429/700000: episode: 1179, duration: 2.037s, episode steps: 391, steps per second: 192, episode reward: 222.395, mean reward: 0.569 [-17.443, 100.000], mean action: 1.074 [0.000, 3.000], mean observation: 0.114 [-1.070, 1.000], loss: 9.254855, mean_absolute_error: 51.977402, mean_q: 68.796379\n",
      " 428000/700000: episode: 1180, duration: 3.301s, episode steps: 571, steps per second: 173, episode reward: 213.700, mean reward: 0.374 [-18.180, 100.000], mean action: 1.967 [0.000, 3.000], mean observation: 0.098 [-0.798, 1.036], loss: 8.470158, mean_absolute_error: 52.112225, mean_q: 69.186676\n",
      " 428268/700000: episode: 1181, duration: 1.398s, episode steps: 268, steps per second: 192, episode reward: 252.553, mean reward: 0.942 [-20.762, 100.000], mean action: 1.354 [0.000, 3.000], mean observation: 0.079 [-0.972, 1.000], loss: 6.452431, mean_absolute_error: 51.681522, mean_q: 68.464958\n",
      " 428516/700000: episode: 1182, duration: 1.242s, episode steps: 248, steps per second: 200, episode reward: 261.778, mean reward: 1.056 [-18.829, 100.000], mean action: 1.234 [0.000, 3.000], mean observation: 0.135 [-0.908, 1.000], loss: 7.732246, mean_absolute_error: 52.258575, mean_q: 69.473991\n",
      " 428809/700000: episode: 1183, duration: 1.541s, episode steps: 293, steps per second: 190, episode reward: 238.875, mean reward: 0.815 [-9.620, 100.000], mean action: 1.188 [0.000, 3.000], mean observation: 0.160 [-0.874, 1.123], loss: 8.108679, mean_absolute_error: 52.165970, mean_q: 69.232185\n",
      " 429056/700000: episode: 1184, duration: 1.254s, episode steps: 247, steps per second: 197, episode reward: 262.471, mean reward: 1.063 [-18.144, 100.000], mean action: 1.162 [0.000, 3.000], mean observation: 0.054 [-0.829, 1.000], loss: 13.117887, mean_absolute_error: 51.933125, mean_q: 69.266022\n",
      " 429878/700000: episode: 1185, duration: 5.017s, episode steps: 822, steps per second: 164, episode reward: 70.084, mean reward: 0.085 [-19.286, 100.000], mean action: 1.429 [0.000, 3.000], mean observation: 0.102 [-1.050, 1.033], loss: 9.825669, mean_absolute_error: 51.945366, mean_q: 69.077744\n",
      " 430203/700000: episode: 1186, duration: 1.665s, episode steps: 325, steps per second: 195, episode reward: 221.087, mean reward: 0.680 [-19.259, 100.000], mean action: 0.631 [0.000, 3.000], mean observation: 0.158 [-1.000, 1.353], loss: 5.109403, mean_absolute_error: 52.014259, mean_q: 69.258194\n",
      " 431203/700000: episode: 1187, duration: 5.985s, episode steps: 1000, steps per second: 167, episode reward: 75.312, mean reward: 0.075 [-18.461, 15.231], mean action: 0.789 [0.000, 3.000], mean observation: 0.190 [-0.923, 1.649], loss: 8.213375, mean_absolute_error: 51.794399, mean_q: 68.831314\n",
      " 431428/700000: episode: 1188, duration: 1.155s, episode steps: 225, steps per second: 195, episode reward: 237.325, mean reward: 1.055 [-4.326, 100.000], mean action: 1.311 [0.000, 3.000], mean observation: 0.035 [-0.708, 1.000], loss: 7.212818, mean_absolute_error: 51.654194, mean_q: 68.818924\n",
      " 431635/700000: episode: 1189, duration: 1.041s, episode steps: 207, steps per second: 199, episode reward: 228.403, mean reward: 1.103 [-3.389, 100.000], mean action: 1.242 [0.000, 3.000], mean observation: 0.118 [-0.970, 1.000], loss: 6.781950, mean_absolute_error: 51.700417, mean_q: 68.761612\n",
      " 431742/700000: episode: 1190, duration: 0.541s, episode steps: 107, steps per second: 198, episode reward: -45.901, mean reward: -0.429 [-100.000, 13.538], mean action: 1.654 [0.000, 3.000], mean observation: 0.021 [-0.950, 1.000], loss: 6.102141, mean_absolute_error: 51.748466, mean_q: 68.723557\n",
      " 431972/700000: episode: 1191, duration: 1.151s, episode steps: 230, steps per second: 200, episode reward: 200.858, mean reward: 0.873 [-17.456, 100.000], mean action: 0.900 [0.000, 3.000], mean observation: 0.127 [-0.948, 1.000], loss: 6.975383, mean_absolute_error: 51.226799, mean_q: 68.301468\n",
      " 432337/700000: episode: 1192, duration: 1.868s, episode steps: 365, steps per second: 195, episode reward: 255.131, mean reward: 0.699 [-6.782, 100.000], mean action: 1.027 [0.000, 3.000], mean observation: 0.052 [-0.929, 1.000], loss: 8.145494, mean_absolute_error: 51.551117, mean_q: 68.613861\n",
      " 432639/700000: episode: 1193, duration: 1.555s, episode steps: 302, steps per second: 194, episode reward: 246.552, mean reward: 0.816 [-3.094, 100.000], mean action: 1.384 [0.000, 3.000], mean observation: 0.045 [-1.235, 1.000], loss: 10.157275, mean_absolute_error: 50.858463, mean_q: 67.678658\n",
      " 432917/700000: episode: 1194, duration: 1.414s, episode steps: 278, steps per second: 197, episode reward: 210.729, mean reward: 0.758 [-17.422, 100.000], mean action: 1.306 [0.000, 3.000], mean observation: 0.079 [-0.723, 1.000], loss: 11.973752, mean_absolute_error: 51.340878, mean_q: 67.912666\n",
      " 433287/700000: episode: 1195, duration: 1.866s, episode steps: 370, steps per second: 198, episode reward: 231.680, mean reward: 0.626 [-17.840, 100.000], mean action: 0.743 [0.000, 3.000], mean observation: 0.153 [-1.194, 1.000], loss: 9.234364, mean_absolute_error: 51.266861, mean_q: 68.193604\n",
      " 433700/700000: episode: 1196, duration: 2.153s, episode steps: 413, steps per second: 192, episode reward: 242.071, mean reward: 0.586 [-17.400, 100.000], mean action: 0.978 [0.000, 3.000], mean observation: 0.105 [-0.757, 1.180], loss: 6.335785, mean_absolute_error: 50.958687, mean_q: 67.719887\n",
      " 434700/700000: episode: 1197, duration: 5.491s, episode steps: 1000, steps per second: 182, episode reward: 78.910, mean reward: 0.079 [-19.369, 22.143], mean action: 0.870 [0.000, 3.000], mean observation: 0.223 [-1.166, 1.000], loss: 8.148798, mean_absolute_error: 50.924953, mean_q: 67.690758\n",
      " 434908/700000: episode: 1198, duration: 1.051s, episode steps: 208, steps per second: 198, episode reward: 245.287, mean reward: 1.179 [-3.635, 100.000], mean action: 1.043 [0.000, 3.000], mean observation: 0.125 [-0.854, 1.000], loss: 7.696727, mean_absolute_error: 50.967716, mean_q: 67.633377\n",
      " 435040/700000: episode: 1199, duration: 0.663s, episode steps: 132, steps per second: 199, episode reward: -56.186, mean reward: -0.426 [-100.000, 8.235], mean action: 1.788 [0.000, 3.000], mean observation: 0.060 [-0.917, 3.120], loss: 9.148371, mean_absolute_error: 51.576355, mean_q: 67.869354\n",
      " 435445/700000: episode: 1200, duration: 2.049s, episode steps: 405, steps per second: 198, episode reward: 230.661, mean reward: 0.570 [-17.557, 100.000], mean action: 1.054 [0.000, 3.000], mean observation: 0.086 [-1.111, 1.000], loss: 9.177601, mean_absolute_error: 51.311047, mean_q: 68.328133\n",
      " 435604/700000: episode: 1201, duration: 0.801s, episode steps: 159, steps per second: 198, episode reward: -85.026, mean reward: -0.535 [-100.000, 11.412], mean action: 1.698 [0.000, 3.000], mean observation: -0.079 [-0.724, 1.559], loss: 12.899168, mean_absolute_error: 50.868305, mean_q: 67.558006\n",
      " 435824/700000: episode: 1202, duration: 1.124s, episode steps: 220, steps per second: 196, episode reward: 223.641, mean reward: 1.017 [-3.693, 100.000], mean action: 1.414 [0.000, 3.000], mean observation: 0.072 [-0.684, 1.000], loss: 7.442057, mean_absolute_error: 51.090164, mean_q: 67.986366\n",
      " 436154/700000: episode: 1203, duration: 1.699s, episode steps: 330, steps per second: 194, episode reward: 272.243, mean reward: 0.825 [-17.457, 100.000], mean action: 1.170 [0.000, 3.000], mean observation: 0.148 [-1.053, 1.000], loss: 9.396200, mean_absolute_error: 51.405468, mean_q: 68.024773\n",
      " 436704/700000: episode: 1204, duration: 2.845s, episode steps: 550, steps per second: 193, episode reward: 257.989, mean reward: 0.469 [-18.816, 100.000], mean action: 0.849 [0.000, 3.000], mean observation: 0.061 [-0.805, 1.000], loss: 10.268895, mean_absolute_error: 51.054020, mean_q: 67.879471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 436985/700000: episode: 1205, duration: 1.443s, episode steps: 281, steps per second: 195, episode reward: 205.801, mean reward: 0.732 [-17.547, 100.000], mean action: 1.836 [0.000, 3.000], mean observation: 0.102 [-1.110, 1.000], loss: 11.272510, mean_absolute_error: 51.247066, mean_q: 68.254341\n",
      " 437161/700000: episode: 1206, duration: 0.959s, episode steps: 176, steps per second: 184, episode reward: -97.104, mean reward: -0.552 [-100.000, 10.936], mean action: 1.773 [0.000, 3.000], mean observation: -0.091 [-1.353, 1.000], loss: 8.294566, mean_absolute_error: 51.025028, mean_q: 68.050850\n",
      " 437521/700000: episode: 1207, duration: 2.095s, episode steps: 360, steps per second: 172, episode reward: 244.149, mean reward: 0.678 [-17.340, 100.000], mean action: 1.278 [0.000, 3.000], mean observation: 0.053 [-0.780, 1.211], loss: 7.900784, mean_absolute_error: 50.928856, mean_q: 67.543861\n",
      " 437914/700000: episode: 1208, duration: 2.167s, episode steps: 393, steps per second: 181, episode reward: 217.917, mean reward: 0.554 [-9.638, 100.000], mean action: 1.214 [0.000, 3.000], mean observation: 0.003 [-0.995, 1.000], loss: 7.284567, mean_absolute_error: 50.829472, mean_q: 67.682480\n",
      " 438247/700000: episode: 1209, duration: 1.825s, episode steps: 333, steps per second: 182, episode reward: 202.363, mean reward: 0.608 [-18.855, 100.000], mean action: 0.661 [0.000, 3.000], mean observation: 0.192 [-1.059, 1.000], loss: 6.969412, mean_absolute_error: 51.077702, mean_q: 68.073151\n",
      " 438365/700000: episode: 1210, duration: 0.606s, episode steps: 118, steps per second: 195, episode reward: -22.862, mean reward: -0.194 [-100.000, 16.929], mean action: 1.534 [0.000, 3.000], mean observation: 0.006 [-0.852, 1.468], loss: 9.968225, mean_absolute_error: 51.020573, mean_q: 68.104813\n",
      " 438485/700000: episode: 1211, duration: 0.765s, episode steps: 120, steps per second: 157, episode reward: -28.442, mean reward: -0.237 [-100.000, 19.070], mean action: 1.783 [0.000, 3.000], mean observation: 0.004 [-0.723, 1.622], loss: 9.193893, mean_absolute_error: 50.683361, mean_q: 67.701233\n",
      " 438680/700000: episode: 1212, duration: 1.158s, episode steps: 195, steps per second: 168, episode reward: 233.419, mean reward: 1.197 [-7.103, 100.000], mean action: 1.190 [0.000, 3.000], mean observation: 0.102 [-0.909, 1.250], loss: 8.640675, mean_absolute_error: 51.037579, mean_q: 67.999451\n",
      " 438990/700000: episode: 1213, duration: 1.589s, episode steps: 310, steps per second: 195, episode reward: 201.625, mean reward: 0.650 [-19.149, 100.000], mean action: 1.010 [0.000, 3.000], mean observation: 0.138 [-1.025, 1.177], loss: 6.273415, mean_absolute_error: 50.479156, mean_q: 67.469910\n",
      " 439352/700000: episode: 1214, duration: 1.910s, episode steps: 362, steps per second: 190, episode reward: 247.162, mean reward: 0.683 [-10.199, 100.000], mean action: 1.390 [0.000, 3.000], mean observation: 0.068 [-0.757, 1.755], loss: 8.346220, mean_absolute_error: 50.833347, mean_q: 68.005905\n",
      " 439469/700000: episode: 1215, duration: 0.596s, episode steps: 117, steps per second: 196, episode reward: -59.790, mean reward: -0.511 [-100.000, 9.291], mean action: 1.983 [0.000, 3.000], mean observation: -0.048 [-0.831, 1.462], loss: 6.672132, mean_absolute_error: 51.331417, mean_q: 68.569565\n",
      " 439803/700000: episode: 1216, duration: 1.712s, episode steps: 334, steps per second: 195, episode reward: 230.819, mean reward: 0.691 [-9.357, 100.000], mean action: 1.147 [0.000, 3.000], mean observation: 0.166 [-0.860, 1.389], loss: 7.593448, mean_absolute_error: 51.132877, mean_q: 68.239136\n",
      " 440110/700000: episode: 1217, duration: 1.554s, episode steps: 307, steps per second: 198, episode reward: 237.909, mean reward: 0.775 [-17.445, 100.000], mean action: 1.078 [0.000, 3.000], mean observation: 0.105 [-0.916, 1.034], loss: 12.352148, mean_absolute_error: 50.649483, mean_q: 67.763191\n",
      " 440343/700000: episode: 1218, duration: 1.167s, episode steps: 233, steps per second: 200, episode reward: 194.547, mean reward: 0.835 [-9.376, 100.000], mean action: 0.931 [0.000, 3.000], mean observation: 0.126 [-0.953, 1.000], loss: 7.006389, mean_absolute_error: 50.483543, mean_q: 67.325378\n",
      " 440588/700000: episode: 1219, duration: 1.235s, episode steps: 245, steps per second: 198, episode reward: 212.252, mean reward: 0.866 [-9.226, 100.000], mean action: 1.127 [0.000, 3.000], mean observation: 0.101 [-0.836, 1.000], loss: 11.228917, mean_absolute_error: 50.638485, mean_q: 67.653244\n",
      " 440839/700000: episode: 1220, duration: 1.274s, episode steps: 251, steps per second: 197, episode reward: 233.144, mean reward: 0.929 [-8.349, 100.000], mean action: 1.594 [0.000, 3.000], mean observation: 0.045 [-0.717, 1.015], loss: 6.997413, mean_absolute_error: 50.623383, mean_q: 67.520622\n",
      " 441058/700000: episode: 1221, duration: 1.097s, episode steps: 219, steps per second: 200, episode reward: 232.590, mean reward: 1.062 [-12.793, 100.000], mean action: 1.279 [0.000, 3.000], mean observation: 0.065 [-0.966, 1.000], loss: 11.774657, mean_absolute_error: 50.186935, mean_q: 67.303772\n",
      " 441595/700000: episode: 1222, duration: 2.789s, episode steps: 537, steps per second: 193, episode reward: 232.057, mean reward: 0.432 [-18.484, 100.000], mean action: 1.076 [0.000, 3.000], mean observation: 0.121 [-0.808, 1.012], loss: 6.239972, mean_absolute_error: 50.658188, mean_q: 67.860512\n",
      " 441914/700000: episode: 1223, duration: 1.651s, episode steps: 319, steps per second: 193, episode reward: 227.390, mean reward: 0.713 [-19.798, 100.000], mean action: 1.451 [0.000, 3.000], mean observation: 0.063 [-0.721, 1.422], loss: 5.998192, mean_absolute_error: 50.180595, mean_q: 67.210953\n",
      " 442417/700000: episode: 1224, duration: 2.561s, episode steps: 503, steps per second: 196, episode reward: 266.118, mean reward: 0.529 [-17.404, 100.000], mean action: 0.759 [0.000, 3.000], mean observation: 0.144 [-0.829, 1.000], loss: 7.548404, mean_absolute_error: 50.607655, mean_q: 67.673134\n",
      " 442865/700000: episode: 1225, duration: 2.304s, episode steps: 448, steps per second: 194, episode reward: 235.476, mean reward: 0.526 [-9.571, 100.000], mean action: 0.797 [0.000, 3.000], mean observation: 0.153 [-0.845, 1.000], loss: 7.332001, mean_absolute_error: 50.383259, mean_q: 67.237465\n",
      " 442988/700000: episode: 1226, duration: 0.615s, episode steps: 123, steps per second: 200, episode reward: -9.799, mean reward: -0.080 [-100.000, 19.837], mean action: 1.618 [0.000, 3.000], mean observation: 0.058 [-0.980, 1.017], loss: 6.725185, mean_absolute_error: 50.629978, mean_q: 67.938553\n",
      " 443558/700000: episode: 1227, duration: 2.992s, episode steps: 570, steps per second: 191, episode reward: 203.603, mean reward: 0.357 [-17.801, 100.000], mean action: 0.940 [0.000, 3.000], mean observation: 0.206 [-0.832, 1.000], loss: 8.014925, mean_absolute_error: 50.529179, mean_q: 67.665855\n",
      " 443773/700000: episode: 1228, duration: 1.078s, episode steps: 215, steps per second: 199, episode reward: 263.490, mean reward: 1.226 [-3.122, 100.000], mean action: 1.200 [0.000, 3.000], mean observation: 0.127 [-0.681, 1.231], loss: 5.744027, mean_absolute_error: 50.980652, mean_q: 68.345215\n",
      " 444022/700000: episode: 1229, duration: 1.281s, episode steps: 249, steps per second: 194, episode reward: 221.212, mean reward: 0.888 [-22.410, 100.000], mean action: 1.321 [0.000, 3.000], mean observation: 0.076 [-0.956, 1.000], loss: 12.850881, mean_absolute_error: 50.635471, mean_q: 67.645706\n",
      " 444213/700000: episode: 1230, duration: 0.957s, episode steps: 191, steps per second: 200, episode reward: 212.459, mean reward: 1.112 [-2.926, 100.000], mean action: 1.340 [0.000, 3.000], mean observation: 0.061 [-0.994, 1.000], loss: 12.004272, mean_absolute_error: 50.423889, mean_q: 67.339745\n",
      " 444439/700000: episode: 1231, duration: 1.131s, episode steps: 226, steps per second: 200, episode reward: 245.252, mean reward: 1.085 [-18.543, 100.000], mean action: 1.168 [0.000, 3.000], mean observation: 0.080 [-0.908, 1.000], loss: 11.797029, mean_absolute_error: 50.746185, mean_q: 67.655029\n",
      " 444620/700000: episode: 1232, duration: 0.904s, episode steps: 181, steps per second: 200, episode reward: 242.298, mean reward: 1.339 [-8.229, 100.000], mean action: 1.077 [0.000, 3.000], mean observation: 0.111 [-1.048, 1.000], loss: 10.886446, mean_absolute_error: 50.264618, mean_q: 67.149055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 444967/700000: episode: 1233, duration: 1.844s, episode steps: 347, steps per second: 188, episode reward: 196.757, mean reward: 0.567 [-21.414, 100.000], mean action: 0.931 [0.000, 3.000], mean observation: 0.139 [-0.903, 1.000], loss: 7.883616, mean_absolute_error: 50.716801, mean_q: 67.454102\n",
      " 445086/700000: episode: 1234, duration: 0.605s, episode steps: 119, steps per second: 197, episode reward: 12.846, mean reward: 0.108 [-100.000, 16.783], mean action: 1.639 [0.000, 3.000], mean observation: -0.050 [-0.891, 1.000], loss: 9.661957, mean_absolute_error: 50.181866, mean_q: 66.979057\n",
      " 445256/700000: episode: 1235, duration: 0.849s, episode steps: 170, steps per second: 200, episode reward: 233.781, mean reward: 1.375 [-17.889, 100.000], mean action: 1.276 [0.000, 3.000], mean observation: 0.047 [-0.963, 1.000], loss: 6.152945, mean_absolute_error: 50.729858, mean_q: 67.729202\n",
      " 445477/700000: episode: 1236, duration: 1.111s, episode steps: 221, steps per second: 199, episode reward: 234.327, mean reward: 1.060 [-9.593, 100.000], mean action: 1.050 [0.000, 3.000], mean observation: 0.138 [-1.268, 1.000], loss: 7.711205, mean_absolute_error: 50.542656, mean_q: 67.270363\n",
      " 445769/700000: episode: 1237, duration: 1.483s, episode steps: 292, steps per second: 197, episode reward: 214.637, mean reward: 0.735 [-18.523, 100.000], mean action: 0.949 [0.000, 3.000], mean observation: 0.142 [-1.005, 1.000], loss: 6.731239, mean_absolute_error: 50.565659, mean_q: 67.432266\n",
      " 446026/700000: episode: 1238, duration: 1.298s, episode steps: 257, steps per second: 198, episode reward: 236.406, mean reward: 0.920 [-18.837, 100.000], mean action: 1.654 [0.000, 3.000], mean observation: 0.029 [-0.757, 1.000], loss: 10.340040, mean_absolute_error: 50.497864, mean_q: 67.237686\n",
      " 446278/700000: episode: 1239, duration: 1.267s, episode steps: 252, steps per second: 199, episode reward: 241.307, mean reward: 0.958 [-17.630, 100.000], mean action: 1.028 [0.000, 3.000], mean observation: 0.129 [-0.717, 1.058], loss: 11.720193, mean_absolute_error: 50.942745, mean_q: 67.871994\n",
      " 446582/700000: episode: 1240, duration: 1.578s, episode steps: 304, steps per second: 193, episode reward: 165.524, mean reward: 0.544 [-18.171, 100.000], mean action: 1.316 [0.000, 3.000], mean observation: 0.126 [-1.039, 1.118], loss: 9.032825, mean_absolute_error: 50.722797, mean_q: 67.409958\n",
      " 446779/700000: episode: 1241, duration: 0.991s, episode steps: 197, steps per second: 199, episode reward: 237.228, mean reward: 1.204 [-9.998, 100.000], mean action: 1.122 [0.000, 3.000], mean observation: 0.106 [-0.880, 1.001], loss: 9.689669, mean_absolute_error: 50.836414, mean_q: 67.485809\n",
      " 446984/700000: episode: 1242, duration: 1.036s, episode steps: 205, steps per second: 198, episode reward: 193.435, mean reward: 0.944 [-8.415, 100.000], mean action: 1.434 [0.000, 3.000], mean observation: 0.038 [-0.674, 1.000], loss: 6.802288, mean_absolute_error: 50.854019, mean_q: 67.560844\n",
      " 447244/700000: episode: 1243, duration: 1.314s, episode steps: 260, steps per second: 198, episode reward: 240.926, mean reward: 0.927 [-21.641, 100.000], mean action: 1.100 [0.000, 3.000], mean observation: 0.071 [-0.889, 1.000], loss: 11.256422, mean_absolute_error: 50.560570, mean_q: 67.339439\n",
      " 447741/700000: episode: 1244, duration: 2.532s, episode steps: 497, steps per second: 196, episode reward: 239.774, mean reward: 0.482 [-20.545, 100.000], mean action: 0.590 [0.000, 3.000], mean observation: 0.187 [-0.979, 1.000], loss: 8.415382, mean_absolute_error: 50.696869, mean_q: 67.297043\n",
      " 447926/700000: episode: 1245, duration: 0.923s, episode steps: 185, steps per second: 200, episode reward: 230.880, mean reward: 1.248 [-18.524, 100.000], mean action: 1.173 [0.000, 3.000], mean observation: 0.094 [-0.873, 1.000], loss: 8.614739, mean_absolute_error: 50.726723, mean_q: 67.224693\n",
      " 448284/700000: episode: 1246, duration: 1.895s, episode steps: 358, steps per second: 189, episode reward: 231.918, mean reward: 0.648 [-20.261, 100.000], mean action: 1.656 [0.000, 3.000], mean observation: 0.065 [-0.870, 1.000], loss: 7.653701, mean_absolute_error: 50.410439, mean_q: 66.844971\n",
      " 448394/700000: episode: 1247, duration: 0.564s, episode steps: 110, steps per second: 195, episode reward: 5.989, mean reward: 0.054 [-100.000, 19.628], mean action: 1.791 [0.000, 3.000], mean observation: -0.064 [-0.969, 1.627], loss: 4.635060, mean_absolute_error: 50.627880, mean_q: 67.250687\n",
      " 448762/700000: episode: 1248, duration: 1.845s, episode steps: 368, steps per second: 199, episode reward: 233.828, mean reward: 0.635 [-18.985, 100.000], mean action: 0.663 [0.000, 3.000], mean observation: 0.171 [-0.967, 1.000], loss: 10.539649, mean_absolute_error: 50.729225, mean_q: 67.366455\n",
      " 448941/700000: episode: 1249, duration: 0.893s, episode steps: 179, steps per second: 201, episode reward: 225.291, mean reward: 1.259 [-19.737, 100.000], mean action: 1.045 [0.000, 3.000], mean observation: 0.104 [-1.244, 1.000], loss: 8.235291, mean_absolute_error: 50.216198, mean_q: 66.966537\n",
      " 449131/700000: episode: 1250, duration: 0.950s, episode steps: 190, steps per second: 200, episode reward: 226.814, mean reward: 1.194 [-19.076, 100.000], mean action: 0.937 [0.000, 3.000], mean observation: 0.096 [-1.083, 1.000], loss: 5.368447, mean_absolute_error: 50.630192, mean_q: 67.494637\n",
      " 449425/700000: episode: 1251, duration: 1.506s, episode steps: 294, steps per second: 195, episode reward: 153.446, mean reward: 0.522 [-18.916, 100.000], mean action: 1.054 [0.000, 3.000], mean observation: 0.124 [-1.008, 1.000], loss: 8.626876, mean_absolute_error: 50.556194, mean_q: 67.377403\n",
      " 449629/700000: episode: 1252, duration: 1.032s, episode steps: 204, steps per second: 198, episode reward: 238.401, mean reward: 1.169 [-10.386, 100.000], mean action: 1.412 [0.000, 3.000], mean observation: 0.103 [-0.741, 1.000], loss: 10.185045, mean_absolute_error: 50.970032, mean_q: 67.771820\n",
      " 449781/700000: episode: 1253, duration: 0.765s, episode steps: 152, steps per second: 199, episode reward: 197.217, mean reward: 1.297 [-3.449, 100.000], mean action: 1.401 [0.000, 3.000], mean observation: 0.098 [-0.930, 1.000], loss: 13.763047, mean_absolute_error: 50.688141, mean_q: 67.259583\n",
      " 450016/700000: episode: 1254, duration: 1.184s, episode steps: 235, steps per second: 198, episode reward: 242.908, mean reward: 1.034 [-8.038, 100.000], mean action: 1.391 [0.000, 3.000], mean observation: 0.067 [-0.990, 1.000], loss: 11.742993, mean_absolute_error: 50.124641, mean_q: 66.785210\n",
      " 450345/700000: episode: 1255, duration: 1.662s, episode steps: 329, steps per second: 198, episode reward: 193.708, mean reward: 0.589 [-17.712, 100.000], mean action: 0.711 [0.000, 3.000], mean observation: 0.162 [-0.976, 1.000], loss: 9.386981, mean_absolute_error: 50.233738, mean_q: 66.511963\n",
      " 450608/700000: episode: 1256, duration: 1.352s, episode steps: 263, steps per second: 195, episode reward: 231.937, mean reward: 0.882 [-18.106, 100.000], mean action: 1.194 [0.000, 3.000], mean observation: 0.070 [-0.912, 1.000], loss: 8.226618, mean_absolute_error: 50.619518, mean_q: 67.207703\n",
      " 450765/700000: episode: 1257, duration: 0.785s, episode steps: 157, steps per second: 200, episode reward: 242.367, mean reward: 1.544 [-12.368, 100.000], mean action: 1.471 [0.000, 3.000], mean observation: 0.020 [-1.020, 1.000], loss: 9.771918, mean_absolute_error: 50.470524, mean_q: 67.283997\n",
      " 451081/700000: episode: 1258, duration: 1.641s, episode steps: 316, steps per second: 193, episode reward: 248.831, mean reward: 0.787 [-20.114, 100.000], mean action: 1.063 [0.000, 3.000], mean observation: 0.058 [-0.901, 1.000], loss: 6.253161, mean_absolute_error: 50.577873, mean_q: 67.446541\n",
      " 451796/700000: episode: 1259, duration: 3.803s, episode steps: 715, steps per second: 188, episode reward: 217.880, mean reward: 0.305 [-20.502, 100.000], mean action: 1.151 [0.000, 3.000], mean observation: 0.135 [-0.899, 1.000], loss: 9.051179, mean_absolute_error: 50.499466, mean_q: 67.184219\n",
      " 452033/700000: episode: 1260, duration: 1.218s, episode steps: 237, steps per second: 195, episode reward: 242.914, mean reward: 1.025 [-3.084, 100.000], mean action: 1.287 [0.000, 3.000], mean observation: 0.048 [-1.018, 1.000], loss: 7.262014, mean_absolute_error: 50.478630, mean_q: 67.223923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 452327/700000: episode: 1261, duration: 1.518s, episode steps: 294, steps per second: 194, episode reward: 224.410, mean reward: 0.763 [-19.237, 100.000], mean action: 0.993 [0.000, 3.000], mean observation: 0.110 [-0.800, 1.000], loss: 34.205502, mean_absolute_error: 50.412724, mean_q: 67.066864\n",
      " 452498/700000: episode: 1262, duration: 0.851s, episode steps: 171, steps per second: 201, episode reward: -2.023, mean reward: -0.012 [-100.000, 15.086], mean action: 1.544 [0.000, 3.000], mean observation: 0.018 [-1.062, 1.122], loss: 10.668517, mean_absolute_error: 50.299767, mean_q: 67.272263\n",
      " 452703/700000: episode: 1263, duration: 1.022s, episode steps: 205, steps per second: 201, episode reward: 235.276, mean reward: 1.148 [-19.099, 100.000], mean action: 1.122 [0.000, 3.000], mean observation: 0.123 [-0.869, 1.000], loss: 5.770717, mean_absolute_error: 50.443741, mean_q: 67.395355\n",
      " 452876/700000: episode: 1264, duration: 0.868s, episode steps: 173, steps per second: 199, episode reward: 213.760, mean reward: 1.236 [-3.884, 100.000], mean action: 1.486 [0.000, 3.000], mean observation: 0.075 [-0.955, 1.000], loss: 4.733875, mean_absolute_error: 50.908588, mean_q: 68.107498\n",
      " 453003/700000: episode: 1265, duration: 0.647s, episode steps: 127, steps per second: 196, episode reward: -58.863, mean reward: -0.463 [-100.000, 12.074], mean action: 2.008 [0.000, 3.000], mean observation: -0.138 [-1.035, 1.055], loss: 5.439521, mean_absolute_error: 50.263199, mean_q: 67.173676\n",
      " 453447/700000: episode: 1266, duration: 2.265s, episode steps: 444, steps per second: 196, episode reward: 208.058, mean reward: 0.469 [-21.714, 100.000], mean action: 0.613 [0.000, 3.000], mean observation: 0.178 [-0.978, 1.228], loss: 21.101084, mean_absolute_error: 50.313915, mean_q: 67.214180\n",
      " 453922/700000: episode: 1267, duration: 2.476s, episode steps: 475, steps per second: 192, episode reward: 254.379, mean reward: 0.536 [-20.164, 100.000], mean action: 0.661 [0.000, 3.000], mean observation: 0.178 [-0.817, 1.000], loss: 8.239082, mean_absolute_error: 50.287540, mean_q: 67.101341\n",
      " 454088/700000: episode: 1268, duration: 0.834s, episode steps: 166, steps per second: 199, episode reward: 248.556, mean reward: 1.497 [-9.636, 100.000], mean action: 1.247 [0.000, 3.000], mean observation: 0.113 [-0.869, 1.000], loss: 7.185493, mean_absolute_error: 50.269485, mean_q: 67.396088\n",
      " 454416/700000: episode: 1269, duration: 1.653s, episode steps: 328, steps per second: 198, episode reward: 216.763, mean reward: 0.661 [-18.266, 100.000], mean action: 0.841 [0.000, 3.000], mean observation: 0.163 [-0.986, 1.000], loss: 9.163094, mean_absolute_error: 50.966881, mean_q: 68.219414\n",
      " 454788/700000: episode: 1270, duration: 1.963s, episode steps: 372, steps per second: 189, episode reward: 234.367, mean reward: 0.630 [-18.864, 100.000], mean action: 1.118 [0.000, 3.000], mean observation: 0.116 [-0.703, 1.043], loss: 9.115633, mean_absolute_error: 50.627926, mean_q: 67.728218\n",
      " 455191/700000: episode: 1271, duration: 2.061s, episode steps: 403, steps per second: 196, episode reward: 257.749, mean reward: 0.640 [-8.731, 100.000], mean action: 0.861 [0.000, 3.000], mean observation: 0.149 [-0.923, 1.000], loss: 9.633418, mean_absolute_error: 50.836010, mean_q: 67.738541\n",
      " 456191/700000: episode: 1272, duration: 5.491s, episode steps: 1000, steps per second: 182, episode reward: 59.972, mean reward: 0.060 [-22.587, 21.954], mean action: 1.986 [0.000, 3.000], mean observation: 0.194 [-0.761, 1.000], loss: 8.071047, mean_absolute_error: 50.828632, mean_q: 67.818512\n",
      " 456415/700000: episode: 1273, duration: 1.131s, episode steps: 224, steps per second: 198, episode reward: -23.047, mean reward: -0.103 [-100.000, 16.818], mean action: 1.839 [0.000, 3.000], mean observation: -0.071 [-0.894, 1.046], loss: 7.699985, mean_absolute_error: 51.158028, mean_q: 67.960312\n",
      " 456582/700000: episode: 1274, duration: 0.831s, episode steps: 167, steps per second: 201, episode reward: 231.945, mean reward: 1.389 [-8.933, 100.000], mean action: 1.311 [0.000, 3.000], mean observation: 0.097 [-0.974, 1.000], loss: 9.558197, mean_absolute_error: 50.918438, mean_q: 67.749191\n",
      " 456819/700000: episode: 1275, duration: 1.196s, episode steps: 237, steps per second: 198, episode reward: 251.589, mean reward: 1.062 [-10.107, 100.000], mean action: 1.262 [0.000, 3.000], mean observation: 0.016 [-0.744, 1.174], loss: 9.184868, mean_absolute_error: 51.162060, mean_q: 68.084320\n",
      " 457211/700000: episode: 1276, duration: 2.020s, episode steps: 392, steps per second: 194, episode reward: 209.223, mean reward: 0.534 [-20.005, 100.000], mean action: 0.878 [0.000, 3.000], mean observation: 0.168 [-0.972, 1.000], loss: 43.264370, mean_absolute_error: 51.320404, mean_q: 68.181679\n",
      " 458126/700000: episode: 1277, duration: 4.828s, episode steps: 915, steps per second: 190, episode reward: 238.319, mean reward: 0.260 [-18.999, 100.000], mean action: 1.242 [0.000, 3.000], mean observation: 0.188 [-0.875, 1.000], loss: 8.040159, mean_absolute_error: 51.699554, mean_q: 68.956429\n",
      " 458387/700000: episode: 1278, duration: 1.307s, episode steps: 261, steps per second: 200, episode reward: 230.497, mean reward: 0.883 [-8.372, 100.000], mean action: 0.808 [0.000, 3.000], mean observation: 0.140 [-0.994, 1.302], loss: 10.209195, mean_absolute_error: 51.950726, mean_q: 69.388161\n",
      " 458648/700000: episode: 1279, duration: 1.323s, episode steps: 261, steps per second: 197, episode reward: 204.457, mean reward: 0.783 [-17.433, 100.000], mean action: 1.092 [0.000, 3.000], mean observation: 0.044 [-1.006, 1.000], loss: 8.105042, mean_absolute_error: 52.166348, mean_q: 69.379158\n",
      " 458867/700000: episode: 1280, duration: 1.105s, episode steps: 219, steps per second: 198, episode reward: 200.821, mean reward: 0.917 [-9.313, 100.000], mean action: 1.416 [0.000, 3.000], mean observation: 0.029 [-0.858, 1.054], loss: 7.901627, mean_absolute_error: 51.996922, mean_q: 69.277802\n",
      " 459039/700000: episode: 1281, duration: 0.870s, episode steps: 172, steps per second: 198, episode reward: 210.354, mean reward: 1.223 [-3.368, 100.000], mean action: 1.233 [0.000, 3.000], mean observation: 0.098 [-1.336, 1.000], loss: 5.414601, mean_absolute_error: 52.210300, mean_q: 69.592590\n",
      " 459244/700000: episode: 1282, duration: 1.036s, episode steps: 205, steps per second: 198, episode reward: 241.457, mean reward: 1.178 [-10.925, 100.000], mean action: 1.117 [0.000, 3.000], mean observation: 0.055 [-1.063, 1.000], loss: 14.614822, mean_absolute_error: 52.290905, mean_q: 70.004776\n",
      " 459799/700000: episode: 1283, duration: 2.913s, episode steps: 555, steps per second: 191, episode reward: -198.575, mean reward: -0.358 [-100.000, 19.741], mean action: 1.178 [0.000, 3.000], mean observation: 0.162 [-1.391, 1.000], loss: 8.229329, mean_absolute_error: 51.941139, mean_q: 69.533096\n",
      " 460109/700000: episode: 1284, duration: 1.637s, episode steps: 310, steps per second: 189, episode reward: 178.006, mean reward: 0.574 [-18.135, 100.000], mean action: 1.300 [0.000, 3.000], mean observation: 0.167 [-1.020, 1.000], loss: 8.322354, mean_absolute_error: 52.300880, mean_q: 69.720428\n",
      " 460301/700000: episode: 1285, duration: 1.024s, episode steps: 192, steps per second: 188, episode reward: 223.629, mean reward: 1.165 [-10.121, 100.000], mean action: 0.859 [0.000, 3.000], mean observation: 0.099 [-1.210, 1.000], loss: 8.935933, mean_absolute_error: 51.790333, mean_q: 68.997841\n",
      " 460604/700000: episode: 1286, duration: 1.684s, episode steps: 303, steps per second: 180, episode reward: 238.102, mean reward: 0.786 [-17.918, 100.000], mean action: 0.842 [0.000, 3.000], mean observation: 0.193 [-0.888, 1.000], loss: 8.104076, mean_absolute_error: 52.057056, mean_q: 69.675629\n",
      " 460992/700000: episode: 1287, duration: 2.176s, episode steps: 388, steps per second: 178, episode reward: 180.934, mean reward: 0.466 [-19.090, 100.000], mean action: 0.972 [0.000, 3.000], mean observation: 0.119 [-1.079, 1.000], loss: 9.321611, mean_absolute_error: 52.389603, mean_q: 69.750069\n",
      " 461212/700000: episode: 1288, duration: 1.136s, episode steps: 220, steps per second: 194, episode reward: 220.053, mean reward: 1.000 [-19.393, 100.000], mean action: 1.286 [0.000, 3.000], mean observation: 0.082 [-1.053, 1.000], loss: 10.033793, mean_absolute_error: 51.572720, mean_q: 68.314812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 461596/700000: episode: 1289, duration: 2.248s, episode steps: 384, steps per second: 171, episode reward: 236.979, mean reward: 0.617 [-9.982, 100.000], mean action: 1.357 [0.000, 3.000], mean observation: 0.006 [-0.934, 1.000], loss: 7.621586, mean_absolute_error: 51.947063, mean_q: 69.468376\n",
      " 461789/700000: episode: 1290, duration: 1.006s, episode steps: 193, steps per second: 192, episode reward: 228.653, mean reward: 1.185 [-9.119, 100.000], mean action: 1.171 [0.000, 3.000], mean observation: 0.110 [-1.005, 1.000], loss: 9.742319, mean_absolute_error: 52.066113, mean_q: 69.692612\n",
      " 462243/700000: episode: 1291, duration: 2.536s, episode steps: 454, steps per second: 179, episode reward: 212.554, mean reward: 0.468 [-18.839, 100.000], mean action: 0.943 [0.000, 3.000], mean observation: 0.190 [-1.004, 1.000], loss: 6.912411, mean_absolute_error: 51.864048, mean_q: 69.065025\n",
      " 462508/700000: episode: 1292, duration: 1.351s, episode steps: 265, steps per second: 196, episode reward: 239.758, mean reward: 0.905 [-10.481, 100.000], mean action: 1.075 [0.000, 3.000], mean observation: 0.117 [-1.106, 1.014], loss: 7.189357, mean_absolute_error: 51.467262, mean_q: 68.449074\n",
      " 462759/700000: episode: 1293, duration: 1.275s, episode steps: 251, steps per second: 197, episode reward: 220.177, mean reward: 0.877 [-12.536, 100.000], mean action: 0.936 [0.000, 3.000], mean observation: 0.119 [-1.034, 1.000], loss: 5.810697, mean_absolute_error: 51.329948, mean_q: 68.418808\n",
      " 463041/700000: episode: 1294, duration: 1.452s, episode steps: 282, steps per second: 194, episode reward: 214.812, mean reward: 0.762 [-17.987, 100.000], mean action: 1.241 [0.000, 3.000], mean observation: 0.099 [-0.986, 1.000], loss: 40.263725, mean_absolute_error: 51.608162, mean_q: 68.814590\n",
      " 463137/700000: episode: 1295, duration: 0.484s, episode steps: 96, steps per second: 198, episode reward: -36.581, mean reward: -0.381 [-100.000, 9.532], mean action: 1.833 [0.000, 3.000], mean observation: 0.016 [-1.308, 1.000], loss: 7.722064, mean_absolute_error: 51.542126, mean_q: 68.866730\n",
      " 463579/700000: episode: 1296, duration: 2.272s, episode steps: 442, steps per second: 195, episode reward: 260.546, mean reward: 0.589 [-17.763, 100.000], mean action: 0.776 [0.000, 3.000], mean observation: 0.143 [-1.023, 1.000], loss: 8.131911, mean_absolute_error: 51.418163, mean_q: 68.372665\n",
      " 463841/700000: episode: 1297, duration: 1.354s, episode steps: 262, steps per second: 194, episode reward: 238.245, mean reward: 0.909 [-21.622, 100.000], mean action: 0.969 [0.000, 3.000], mean observation: 0.117 [-0.905, 1.000], loss: 10.745566, mean_absolute_error: 51.874187, mean_q: 69.257408\n",
      " 464348/700000: episode: 1298, duration: 2.816s, episode steps: 507, steps per second: 180, episode reward: 203.382, mean reward: 0.401 [-19.634, 100.000], mean action: 1.753 [0.000, 3.000], mean observation: 0.029 [-1.266, 1.000], loss: 8.681497, mean_absolute_error: 51.324120, mean_q: 68.145821\n",
      " 464513/700000: episode: 1299, duration: 0.826s, episode steps: 165, steps per second: 200, episode reward: 221.949, mean reward: 1.345 [-2.893, 100.000], mean action: 1.055 [0.000, 3.000], mean observation: 0.088 [-0.989, 1.122], loss: 9.399059, mean_absolute_error: 51.403385, mean_q: 68.524376\n",
      " 464773/700000: episode: 1300, duration: 1.334s, episode steps: 260, steps per second: 195, episode reward: 195.776, mean reward: 0.753 [-17.603, 100.000], mean action: 1.135 [0.000, 3.000], mean observation: 0.109 [-0.957, 1.000], loss: 11.039484, mean_absolute_error: 51.582226, mean_q: 68.642265\n",
      " 465172/700000: episode: 1301, duration: 2.020s, episode steps: 399, steps per second: 198, episode reward: 188.805, mean reward: 0.473 [-21.736, 100.000], mean action: 0.634 [0.000, 3.000], mean observation: 0.159 [-1.476, 1.000], loss: 9.728904, mean_absolute_error: 51.447063, mean_q: 68.805298\n",
      " 465582/700000: episode: 1302, duration: 2.108s, episode steps: 410, steps per second: 194, episode reward: 245.739, mean reward: 0.599 [-9.399, 100.000], mean action: 0.924 [0.000, 3.000], mean observation: 0.129 [-1.027, 1.000], loss: 15.086585, mean_absolute_error: 51.542267, mean_q: 68.617043\n",
      " 465880/700000: episode: 1303, duration: 1.544s, episode steps: 298, steps per second: 193, episode reward: 208.841, mean reward: 0.701 [-7.836, 100.000], mean action: 1.138 [0.000, 3.000], mean observation: 0.105 [-0.783, 1.167], loss: 10.595288, mean_absolute_error: 51.767742, mean_q: 68.913879\n",
      " 466117/700000: episode: 1304, duration: 1.201s, episode steps: 237, steps per second: 197, episode reward: 208.891, mean reward: 0.881 [-9.000, 100.000], mean action: 1.198 [0.000, 3.000], mean observation: 0.089 [-0.987, 1.000], loss: 10.712687, mean_absolute_error: 51.637138, mean_q: 68.847275\n",
      " 466458/700000: episode: 1305, duration: 1.783s, episode steps: 341, steps per second: 191, episode reward: 147.414, mean reward: 0.432 [-10.318, 100.000], mean action: 1.246 [0.000, 3.000], mean observation: 0.139 [-1.032, 1.000], loss: 8.826505, mean_absolute_error: 51.369698, mean_q: 68.383446\n",
      " 466788/700000: episode: 1306, duration: 1.697s, episode steps: 330, steps per second: 194, episode reward: 243.652, mean reward: 0.738 [-19.487, 100.000], mean action: 0.885 [0.000, 3.000], mean observation: 0.140 [-1.304, 1.000], loss: 14.419845, mean_absolute_error: 51.419384, mean_q: 68.474182\n",
      " 466978/700000: episode: 1307, duration: 0.955s, episode steps: 190, steps per second: 199, episode reward: 237.731, mean reward: 1.251 [-10.310, 100.000], mean action: 1.168 [0.000, 3.000], mean observation: 0.074 [-0.917, 1.000], loss: 9.061102, mean_absolute_error: 51.560394, mean_q: 69.026779\n",
      " 467484/700000: episode: 1308, duration: 2.843s, episode steps: 506, steps per second: 178, episode reward: 218.221, mean reward: 0.431 [-18.579, 100.000], mean action: 1.182 [0.000, 3.000], mean observation: 0.102 [-1.017, 1.000], loss: 6.400910, mean_absolute_error: 51.014080, mean_q: 68.123024\n",
      " 467738/700000: episode: 1309, duration: 1.276s, episode steps: 254, steps per second: 199, episode reward: 218.365, mean reward: 0.860 [-9.445, 100.000], mean action: 0.783 [0.000, 3.000], mean observation: 0.086 [-1.078, 1.000], loss: 9.535329, mean_absolute_error: 50.646713, mean_q: 67.819458\n",
      " 467972/700000: episode: 1310, duration: 1.209s, episode steps: 234, steps per second: 194, episode reward: 237.243, mean reward: 1.014 [-9.743, 100.000], mean action: 1.415 [0.000, 3.000], mean observation: 0.073 [-0.916, 1.000], loss: 8.501280, mean_absolute_error: 50.961853, mean_q: 67.948166\n",
      " 468355/700000: episode: 1311, duration: 2.004s, episode steps: 383, steps per second: 191, episode reward: 204.489, mean reward: 0.534 [-18.577, 100.000], mean action: 1.070 [0.000, 3.000], mean observation: 0.096 [-0.656, 1.000], loss: 8.087746, mean_absolute_error: 51.003578, mean_q: 68.154770\n",
      " 468734/700000: episode: 1312, duration: 1.986s, episode steps: 379, steps per second: 191, episode reward: 258.306, mean reward: 0.682 [-10.673, 100.000], mean action: 1.040 [0.000, 3.000], mean observation: 0.098 [-1.014, 1.018], loss: 7.085844, mean_absolute_error: 50.553749, mean_q: 67.530373\n",
      " 469037/700000: episode: 1313, duration: 1.550s, episode steps: 303, steps per second: 196, episode reward: 212.917, mean reward: 0.703 [-10.329, 100.000], mean action: 0.832 [0.000, 3.000], mean observation: 0.075 [-1.036, 1.000], loss: 8.444755, mean_absolute_error: 50.644859, mean_q: 67.631935\n",
      " 469463/700000: episode: 1314, duration: 2.200s, episode steps: 426, steps per second: 194, episode reward: 240.105, mean reward: 0.564 [-18.374, 100.000], mean action: 1.141 [0.000, 3.000], mean observation: 0.116 [-1.031, 1.000], loss: 11.525373, mean_absolute_error: 50.615410, mean_q: 67.579720\n",
      " 469913/700000: episode: 1315, duration: 2.379s, episode steps: 450, steps per second: 189, episode reward: 224.954, mean reward: 0.500 [-8.658, 100.000], mean action: 1.460 [0.000, 3.000], mean observation: 0.052 [-0.647, 1.004], loss: 6.491204, mean_absolute_error: 50.468632, mean_q: 67.423721\n",
      " 470811/700000: episode: 1316, duration: 4.840s, episode steps: 898, steps per second: 186, episode reward: 231.359, mean reward: 0.258 [-19.944, 100.000], mean action: 1.657 [0.000, 3.000], mean observation: 0.174 [-1.118, 1.077], loss: 8.927528, mean_absolute_error: 50.414341, mean_q: 67.140640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 471116/700000: episode: 1317, duration: 1.555s, episode steps: 305, steps per second: 196, episode reward: 207.159, mean reward: 0.679 [-7.915, 100.000], mean action: 0.964 [0.000, 3.000], mean observation: 0.117 [-0.904, 1.000], loss: 10.270650, mean_absolute_error: 50.255768, mean_q: 67.006752\n",
      " 471203/700000: episode: 1318, duration: 0.438s, episode steps: 87, steps per second: 199, episode reward: -14.413, mean reward: -0.166 [-100.000, 21.764], mean action: 1.552 [0.000, 3.000], mean observation: 0.015 [-1.113, 1.000], loss: 7.209741, mean_absolute_error: 50.497074, mean_q: 67.203667\n",
      " 471355/700000: episode: 1319, duration: 0.761s, episode steps: 152, steps per second: 200, episode reward: 241.947, mean reward: 1.592 [-9.917, 100.000], mean action: 1.296 [0.000, 3.000], mean observation: 0.057 [-1.133, 1.000], loss: 16.330217, mean_absolute_error: 50.348709, mean_q: 67.185661\n",
      " 471466/700000: episode: 1320, duration: 0.558s, episode steps: 111, steps per second: 199, episode reward: -50.031, mean reward: -0.451 [-100.000, 20.056], mean action: 1.315 [0.000, 3.000], mean observation: 0.086 [-1.781, 1.000], loss: 11.507046, mean_absolute_error: 50.197544, mean_q: 66.673058\n",
      " 471821/700000: episode: 1321, duration: 1.813s, episode steps: 355, steps per second: 196, episode reward: 221.884, mean reward: 0.625 [-17.371, 100.000], mean action: 1.056 [0.000, 3.000], mean observation: 0.104 [-1.567, 1.000], loss: 15.831113, mean_absolute_error: 50.530479, mean_q: 67.130951\n",
      " 471906/700000: episode: 1322, duration: 0.433s, episode steps: 85, steps per second: 196, episode reward: -28.205, mean reward: -0.332 [-100.000, 18.680], mean action: 1.800 [0.000, 3.000], mean observation: 0.041 [-1.826, 1.000], loss: 4.910263, mean_absolute_error: 49.789642, mean_q: 66.122711\n",
      " 472289/700000: episode: 1323, duration: 1.994s, episode steps: 383, steps per second: 192, episode reward: 261.709, mean reward: 0.683 [-18.571, 100.000], mean action: 1.120 [0.000, 3.000], mean observation: 0.076 [-0.851, 1.000], loss: 8.196940, mean_absolute_error: 50.297691, mean_q: 66.834465\n",
      " 472381/700000: episode: 1324, duration: 0.459s, episode steps: 92, steps per second: 201, episode reward: -57.031, mean reward: -0.620 [-100.000, 18.328], mean action: 1.370 [0.000, 3.000], mean observation: 0.018 [-1.653, 1.000], loss: 6.101874, mean_absolute_error: 50.135159, mean_q: 66.412819\n",
      " 472454/700000: episode: 1325, duration: 0.371s, episode steps: 73, steps per second: 197, episode reward: -28.336, mean reward: -0.388 [-100.000, 20.468], mean action: 1.466 [0.000, 3.000], mean observation: 0.051 [-2.190, 1.000], loss: 8.032704, mean_absolute_error: 50.600315, mean_q: 67.345184\n",
      " 472523/700000: episode: 1326, duration: 0.348s, episode steps: 69, steps per second: 199, episode reward: -52.301, mean reward: -0.758 [-100.000, 23.664], mean action: 1.362 [0.000, 3.000], mean observation: 0.061 [-2.564, 1.000], loss: 6.863975, mean_absolute_error: 50.397572, mean_q: 67.049942\n",
      " 472718/700000: episode: 1327, duration: 0.989s, episode steps: 195, steps per second: 197, episode reward: 264.050, mean reward: 1.354 [-10.124, 100.000], mean action: 1.190 [0.000, 3.000], mean observation: 0.069 [-0.902, 1.000], loss: 9.458625, mean_absolute_error: 49.988014, mean_q: 66.495918\n",
      " 473037/700000: episode: 1328, duration: 1.711s, episode steps: 319, steps per second: 186, episode reward: 250.309, mean reward: 0.785 [-8.097, 100.000], mean action: 1.169 [0.000, 3.000], mean observation: 0.077 [-0.999, 1.000], loss: 5.545057, mean_absolute_error: 50.381084, mean_q: 67.147346\n",
      " 473147/700000: episode: 1329, duration: 0.543s, episode steps: 110, steps per second: 202, episode reward: -24.767, mean reward: -0.225 [-100.000, 21.244], mean action: 1.518 [0.000, 3.000], mean observation: 0.003 [-1.546, 1.000], loss: 5.225393, mean_absolute_error: 50.459076, mean_q: 66.883324\n",
      " 473275/700000: episode: 1330, duration: 0.642s, episode steps: 128, steps per second: 199, episode reward: -40.769, mean reward: -0.319 [-100.000, 45.608], mean action: 1.672 [0.000, 3.000], mean observation: 0.139 [-1.110, 1.118], loss: 7.094089, mean_absolute_error: 50.391808, mean_q: 67.412453\n",
      " 473515/700000: episode: 1331, duration: 1.207s, episode steps: 240, steps per second: 199, episode reward: 240.141, mean reward: 1.001 [-17.374, 100.000], mean action: 1.121 [0.000, 3.000], mean observation: 0.162 [-1.153, 1.000], loss: 10.333449, mean_absolute_error: 50.549366, mean_q: 66.808029\n",
      " 473835/700000: episode: 1332, duration: 1.635s, episode steps: 320, steps per second: 196, episode reward: 226.280, mean reward: 0.707 [-7.799, 100.000], mean action: 1.206 [0.000, 3.000], mean observation: 0.023 [-1.017, 1.000], loss: 9.308302, mean_absolute_error: 50.115196, mean_q: 66.811462\n",
      " 474037/700000: episode: 1333, duration: 1.008s, episode steps: 202, steps per second: 200, episode reward: 201.532, mean reward: 0.998 [-9.377, 100.000], mean action: 1.040 [0.000, 3.000], mean observation: 0.048 [-0.957, 1.000], loss: 7.623487, mean_absolute_error: 50.047436, mean_q: 66.961937\n",
      " 474479/700000: episode: 1334, duration: 2.263s, episode steps: 442, steps per second: 195, episode reward: 226.816, mean reward: 0.513 [-17.413, 100.000], mean action: 0.724 [0.000, 3.000], mean observation: 0.183 [-1.235, 1.000], loss: 9.549103, mean_absolute_error: 50.736320, mean_q: 67.826187\n",
      " 474786/700000: episode: 1335, duration: 1.567s, episode steps: 307, steps per second: 196, episode reward: 266.676, mean reward: 0.869 [-10.254, 100.000], mean action: 1.293 [0.000, 3.000], mean observation: 0.064 [-0.863, 1.057], loss: 9.572584, mean_absolute_error: 51.293068, mean_q: 68.325500\n",
      " 474991/700000: episode: 1336, duration: 1.021s, episode steps: 205, steps per second: 201, episode reward: 215.959, mean reward: 1.053 [-6.556, 100.000], mean action: 1.112 [0.000, 3.000], mean observation: 0.092 [-1.175, 1.000], loss: 9.848480, mean_absolute_error: 50.798347, mean_q: 67.895653\n",
      " 475196/700000: episode: 1337, duration: 1.044s, episode steps: 205, steps per second: 196, episode reward: 251.808, mean reward: 1.228 [-12.919, 100.000], mean action: 1.395 [0.000, 3.000], mean observation: 0.040 [-0.881, 1.000], loss: 8.970655, mean_absolute_error: 50.865490, mean_q: 67.982491\n",
      " 475306/700000: episode: 1338, duration: 0.558s, episode steps: 110, steps per second: 197, episode reward: -127.211, mean reward: -1.156 [-100.000, 19.252], mean action: 1.745 [0.000, 3.000], mean observation: -0.062 [-0.914, 1.812], loss: 8.762282, mean_absolute_error: 51.605968, mean_q: 68.434280\n",
      " 475555/700000: episode: 1339, duration: 1.271s, episode steps: 249, steps per second: 196, episode reward: 267.491, mean reward: 1.074 [-7.638, 100.000], mean action: 1.213 [0.000, 3.000], mean observation: 0.126 [-0.796, 1.000], loss: 9.823350, mean_absolute_error: 50.959717, mean_q: 67.840034\n",
      " 475796/700000: episode: 1340, duration: 1.237s, episode steps: 241, steps per second: 195, episode reward: 232.284, mean reward: 0.964 [-18.885, 100.000], mean action: 1.112 [0.000, 3.000], mean observation: 0.033 [-0.973, 1.000], loss: 8.574460, mean_absolute_error: 51.075661, mean_q: 68.142326\n",
      " 476796/700000: episode: 1341, duration: 5.317s, episode steps: 1000, steps per second: 188, episode reward: 88.777, mean reward: 0.089 [-20.300, 22.748], mean action: 0.964 [0.000, 3.000], mean observation: 0.194 [-0.745, 1.000], loss: 7.741759, mean_absolute_error: 50.802361, mean_q: 67.755951\n",
      " 477102/700000: episode: 1342, duration: 1.554s, episode steps: 306, steps per second: 197, episode reward: 237.635, mean reward: 0.777 [-11.428, 100.000], mean action: 1.092 [0.000, 3.000], mean observation: 0.114 [-0.783, 1.000], loss: 9.396041, mean_absolute_error: 51.265888, mean_q: 68.585609\n",
      " 477333/700000: episode: 1343, duration: 1.154s, episode steps: 231, steps per second: 200, episode reward: 212.497, mean reward: 0.920 [-3.919, 100.000], mean action: 0.801 [0.000, 3.000], mean observation: 0.135 [-0.983, 1.117], loss: 7.175807, mean_absolute_error: 50.674011, mean_q: 67.586594\n",
      " 477447/700000: episode: 1344, duration: 0.574s, episode steps: 114, steps per second: 198, episode reward: -3.198, mean reward: -0.028 [-100.000, 12.926], mean action: 1.754 [0.000, 3.000], mean observation: 0.052 [-1.314, 1.000], loss: 8.275034, mean_absolute_error: 51.129940, mean_q: 68.025764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 477685/700000: episode: 1345, duration: 1.214s, episode steps: 238, steps per second: 196, episode reward: 252.560, mean reward: 1.061 [-9.982, 100.000], mean action: 1.139 [0.000, 3.000], mean observation: 0.109 [-0.940, 1.455], loss: 10.291264, mean_absolute_error: 51.569916, mean_q: 68.862282\n",
      " 477887/700000: episode: 1346, duration: 1.012s, episode steps: 202, steps per second: 200, episode reward: 224.873, mean reward: 1.113 [-2.888, 100.000], mean action: 0.975 [0.000, 3.000], mean observation: 0.121 [-0.971, 1.000], loss: 9.644251, mean_absolute_error: 51.409565, mean_q: 68.512230\n",
      " 478183/700000: episode: 1347, duration: 1.485s, episode steps: 296, steps per second: 199, episode reward: 244.260, mean reward: 0.825 [-19.890, 100.000], mean action: 0.966 [0.000, 3.000], mean observation: 0.101 [-0.935, 1.000], loss: 6.916751, mean_absolute_error: 51.447666, mean_q: 68.606895\n",
      " 478637/700000: episode: 1348, duration: 2.378s, episode steps: 454, steps per second: 191, episode reward: 242.038, mean reward: 0.533 [-18.877, 100.000], mean action: 0.663 [0.000, 3.000], mean observation: 0.170 [-0.954, 1.000], loss: 20.425371, mean_absolute_error: 51.698463, mean_q: 68.687775\n",
      " 478880/700000: episode: 1349, duration: 1.225s, episode steps: 243, steps per second: 198, episode reward: 248.267, mean reward: 1.022 [-4.053, 100.000], mean action: 1.226 [0.000, 3.000], mean observation: 0.140 [-0.979, 1.001], loss: 9.067369, mean_absolute_error: 51.715229, mean_q: 69.034767\n",
      " 479001/700000: episode: 1350, duration: 0.603s, episode steps: 121, steps per second: 201, episode reward: -41.766, mean reward: -0.345 [-100.000, 11.589], mean action: 1.636 [0.000, 3.000], mean observation: 0.081 [-1.631, 1.000], loss: 5.414211, mean_absolute_error: 51.469986, mean_q: 68.780937\n",
      " 479172/700000: episode: 1351, duration: 0.863s, episode steps: 171, steps per second: 198, episode reward: -36.661, mean reward: -0.214 [-100.000, 20.168], mean action: 1.678 [0.000, 3.000], mean observation: 0.096 [-1.690, 1.000], loss: 10.795062, mean_absolute_error: 51.932758, mean_q: 69.179176\n",
      " 479331/700000: episode: 1352, duration: 0.792s, episode steps: 159, steps per second: 201, episode reward: 245.331, mean reward: 1.543 [-8.511, 100.000], mean action: 1.189 [0.000, 3.000], mean observation: 0.087 [-1.031, 1.000], loss: 4.206071, mean_absolute_error: 51.689304, mean_q: 69.152344\n",
      " 479576/700000: episode: 1353, duration: 1.235s, episode steps: 245, steps per second: 198, episode reward: 209.039, mean reward: 0.853 [-9.598, 100.000], mean action: 1.049 [0.000, 3.000], mean observation: 0.096 [-0.902, 1.022], loss: 6.863666, mean_absolute_error: 51.643211, mean_q: 68.648132\n",
      " 480247/700000: episode: 1354, duration: 3.571s, episode steps: 671, steps per second: 188, episode reward: 248.937, mean reward: 0.371 [-20.609, 100.000], mean action: 0.754 [0.000, 3.000], mean observation: 0.235 [-0.925, 1.000], loss: 12.282653, mean_absolute_error: 51.697006, mean_q: 68.983009\n",
      " 480529/700000: episode: 1355, duration: 1.471s, episode steps: 282, steps per second: 192, episode reward: 250.720, mean reward: 0.889 [-10.076, 100.000], mean action: 1.195 [0.000, 3.000], mean observation: 0.103 [-0.924, 1.000], loss: 5.661712, mean_absolute_error: 51.235546, mean_q: 68.091545\n",
      " 481301/700000: episode: 1356, duration: 4.071s, episode steps: 772, steps per second: 190, episode reward: 235.278, mean reward: 0.305 [-19.094, 100.000], mean action: 0.839 [0.000, 3.000], mean observation: 0.209 [-0.956, 1.000], loss: 9.105616, mean_absolute_error: 51.177998, mean_q: 68.007942\n",
      " 481659/700000: episode: 1357, duration: 1.838s, episode steps: 358, steps per second: 195, episode reward: 239.135, mean reward: 0.668 [-10.401, 100.000], mean action: 0.953 [0.000, 3.000], mean observation: 0.141 [-0.937, 1.000], loss: 5.768084, mean_absolute_error: 50.828011, mean_q: 67.620171\n",
      " 481870/700000: episode: 1358, duration: 1.079s, episode steps: 211, steps per second: 196, episode reward: 216.510, mean reward: 1.026 [-10.512, 100.000], mean action: 0.744 [0.000, 3.000], mean observation: 0.095 [-0.944, 1.000], loss: 14.713700, mean_absolute_error: 50.270142, mean_q: 67.060272\n",
      " 482097/700000: episode: 1359, duration: 1.367s, episode steps: 227, steps per second: 166, episode reward: -34.593, mean reward: -0.152 [-100.000, 20.900], mean action: 1.656 [0.000, 3.000], mean observation: 0.116 [-1.526, 1.000], loss: 8.839127, mean_absolute_error: 50.570442, mean_q: 67.236046\n",
      " 482325/700000: episode: 1360, duration: 1.152s, episode steps: 228, steps per second: 198, episode reward: 218.411, mean reward: 0.958 [-6.372, 100.000], mean action: 1.224 [0.000, 3.000], mean observation: 0.057 [-0.927, 1.000], loss: 9.972294, mean_absolute_error: 50.729355, mean_q: 67.368080\n",
      " 482785/700000: episode: 1361, duration: 2.452s, episode steps: 460, steps per second: 188, episode reward: 211.343, mean reward: 0.459 [-21.990, 100.000], mean action: 1.093 [0.000, 3.000], mean observation: 0.145 [-0.795, 1.000], loss: 8.978425, mean_absolute_error: 50.220211, mean_q: 66.761612\n",
      " 483007/700000: episode: 1362, duration: 1.119s, episode steps: 222, steps per second: 198, episode reward: 208.349, mean reward: 0.939 [-11.335, 100.000], mean action: 1.153 [0.000, 3.000], mean observation: 0.118 [-1.002, 1.000], loss: 8.890180, mean_absolute_error: 50.500107, mean_q: 67.561958\n",
      " 483386/700000: episode: 1363, duration: 2.057s, episode steps: 379, steps per second: 184, episode reward: 210.640, mean reward: 0.556 [-9.974, 100.000], mean action: 1.174 [0.000, 3.000], mean observation: 0.059 [-1.014, 1.018], loss: 9.218391, mean_absolute_error: 50.460472, mean_q: 67.077988\n",
      " 483643/700000: episode: 1364, duration: 1.485s, episode steps: 257, steps per second: 173, episode reward: 206.479, mean reward: 0.803 [-11.103, 100.000], mean action: 1.471 [0.000, 3.000], mean observation: 0.046 [-0.658, 1.000], loss: 8.602125, mean_absolute_error: 50.411751, mean_q: 67.228737\n",
      " 483837/700000: episode: 1365, duration: 1.009s, episode steps: 194, steps per second: 192, episode reward: 208.690, mean reward: 1.076 [-11.594, 100.000], mean action: 1.392 [0.000, 3.000], mean observation: 0.084 [-0.827, 1.000], loss: 9.022206, mean_absolute_error: 50.408173, mean_q: 67.263000\n",
      " 484123/700000: episode: 1366, duration: 1.503s, episode steps: 286, steps per second: 190, episode reward: 224.586, mean reward: 0.785 [-10.588, 100.000], mean action: 0.920 [0.000, 3.000], mean observation: 0.129 [-0.821, 1.000], loss: 5.072869, mean_absolute_error: 50.436413, mean_q: 67.234131\n",
      " 484306/700000: episode: 1367, duration: 0.947s, episode steps: 183, steps per second: 193, episode reward: 196.443, mean reward: 1.073 [-6.592, 100.000], mean action: 1.175 [0.000, 3.000], mean observation: 0.078 [-0.866, 1.143], loss: 13.239167, mean_absolute_error: 50.783657, mean_q: 67.417793\n",
      " 484518/700000: episode: 1368, duration: 1.238s, episode steps: 212, steps per second: 171, episode reward: 256.872, mean reward: 1.212 [-2.960, 100.000], mean action: 1.387 [0.000, 3.000], mean observation: 0.024 [-0.872, 1.000], loss: 5.975155, mean_absolute_error: 50.298607, mean_q: 67.302979\n",
      " 484800/700000: episode: 1369, duration: 1.466s, episode steps: 282, steps per second: 192, episode reward: 217.581, mean reward: 0.772 [-17.735, 100.000], mean action: 0.872 [0.000, 3.000], mean observation: 0.166 [-0.957, 1.000], loss: 7.777330, mean_absolute_error: 51.136040, mean_q: 67.901108\n",
      " 484938/700000: episode: 1370, duration: 0.777s, episode steps: 138, steps per second: 178, episode reward: 23.864, mean reward: 0.173 [-100.000, 15.498], mean action: 1.688 [0.000, 3.000], mean observation: 0.041 [-0.820, 1.255], loss: 8.987608, mean_absolute_error: 50.723999, mean_q: 67.662453\n",
      " 485217/700000: episode: 1371, duration: 1.441s, episode steps: 279, steps per second: 194, episode reward: 211.657, mean reward: 0.759 [-17.853, 100.000], mean action: 0.857 [0.000, 3.000], mean observation: 0.180 [-1.003, 1.000], loss: 10.077126, mean_absolute_error: 51.216061, mean_q: 68.545395\n",
      " 485642/700000: episode: 1372, duration: 2.259s, episode steps: 425, steps per second: 188, episode reward: 212.024, mean reward: 0.499 [-17.382, 100.000], mean action: 0.508 [0.000, 3.000], mean observation: 0.176 [-1.035, 1.000], loss: 14.310641, mean_absolute_error: 51.535130, mean_q: 68.534538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 486642/700000: episode: 1373, duration: 6.495s, episode steps: 1000, steps per second: 154, episode reward: -50.488, mean reward: -0.050 [-5.235, 6.301], mean action: 1.765 [0.000, 3.000], mean observation: -0.027 [-0.811, 0.939], loss: 10.052736, mean_absolute_error: 51.234318, mean_q: 68.169327\n",
      " 486919/700000: episode: 1374, duration: 1.431s, episode steps: 277, steps per second: 194, episode reward: 216.671, mean reward: 0.782 [-9.500, 100.000], mean action: 1.014 [0.000, 3.000], mean observation: 0.115 [-1.086, 1.000], loss: 8.724457, mean_absolute_error: 50.713596, mean_q: 67.443375\n",
      " 487919/700000: episode: 1375, duration: 5.702s, episode steps: 1000, steps per second: 175, episode reward: 38.542, mean reward: 0.039 [-19.824, 22.875], mean action: 1.536 [0.000, 3.000], mean observation: 0.134 [-0.828, 1.000], loss: 8.283174, mean_absolute_error: 50.714256, mean_q: 67.701080\n",
      " 488919/700000: episode: 1376, duration: 5.858s, episode steps: 1000, steps per second: 171, episode reward: 44.793, mean reward: 0.045 [-19.804, 18.768], mean action: 2.214 [0.000, 3.000], mean observation: 0.176 [-1.036, 1.000], loss: 7.410434, mean_absolute_error: 50.598797, mean_q: 67.602058\n",
      " 489239/700000: episode: 1377, duration: 1.640s, episode steps: 320, steps per second: 195, episode reward: 247.312, mean reward: 0.773 [-9.959, 100.000], mean action: 1.144 [0.000, 3.000], mean observation: 0.147 [-1.063, 1.000], loss: 10.519922, mean_absolute_error: 50.932671, mean_q: 68.067596\n",
      " 489555/700000: episode: 1378, duration: 1.621s, episode steps: 316, steps per second: 195, episode reward: 227.294, mean reward: 0.719 [-9.567, 100.000], mean action: 1.411 [0.000, 3.000], mean observation: 0.052 [-0.829, 1.000], loss: 6.656623, mean_absolute_error: 50.527363, mean_q: 67.337540\n",
      " 489922/700000: episode: 1379, duration: 1.949s, episode steps: 367, steps per second: 188, episode reward: 250.804, mean reward: 0.683 [-18.205, 100.000], mean action: 0.978 [0.000, 3.000], mean observation: 0.113 [-0.702, 1.000], loss: 11.097522, mean_absolute_error: 50.979958, mean_q: 68.193031\n",
      " 490265/700000: episode: 1380, duration: 1.742s, episode steps: 343, steps per second: 197, episode reward: 239.105, mean reward: 0.697 [-10.960, 100.000], mean action: 1.064 [0.000, 3.000], mean observation: 0.185 [-0.947, 1.128], loss: 7.573260, mean_absolute_error: 50.565945, mean_q: 67.613747\n",
      " 490608/700000: episode: 1381, duration: 1.821s, episode steps: 343, steps per second: 188, episode reward: 179.996, mean reward: 0.525 [-12.907, 100.000], mean action: 2.012 [0.000, 3.000], mean observation: 0.044 [-1.040, 1.000], loss: 13.671436, mean_absolute_error: 51.084110, mean_q: 68.235435\n",
      " 490924/700000: episode: 1382, duration: 1.603s, episode steps: 316, steps per second: 197, episode reward: 212.657, mean reward: 0.673 [-9.534, 100.000], mean action: 0.930 [0.000, 3.000], mean observation: 0.154 [-1.003, 1.334], loss: 7.897686, mean_absolute_error: 50.682625, mean_q: 67.319664\n",
      " 491049/700000: episode: 1383, duration: 0.634s, episode steps: 125, steps per second: 197, episode reward: -7.523, mean reward: -0.060 [-100.000, 19.516], mean action: 1.672 [0.000, 3.000], mean observation: 0.038 [-0.815, 1.000], loss: 5.950204, mean_absolute_error: 50.635067, mean_q: 67.223206\n",
      " 491167/700000: episode: 1384, duration: 0.596s, episode steps: 118, steps per second: 198, episode reward: -29.687, mean reward: -0.252 [-100.000, 11.800], mean action: 1.695 [0.000, 3.000], mean observation: 0.067 [-0.957, 1.000], loss: 9.248824, mean_absolute_error: 50.596153, mean_q: 67.409805\n",
      " 491544/700000: episode: 1385, duration: 1.995s, episode steps: 377, steps per second: 189, episode reward: 179.236, mean reward: 0.475 [-13.821, 100.000], mean action: 1.143 [0.000, 3.000], mean observation: 0.175 [-1.293, 1.000], loss: 13.109918, mean_absolute_error: 50.913311, mean_q: 67.826981\n",
      " 491803/700000: episode: 1386, duration: 1.339s, episode steps: 259, steps per second: 193, episode reward: 245.129, mean reward: 0.946 [-2.985, 100.000], mean action: 1.355 [0.000, 3.000], mean observation: 0.066 [-1.042, 1.000], loss: 12.774192, mean_absolute_error: 50.831802, mean_q: 67.723587\n",
      " 492337/700000: episode: 1387, duration: 2.753s, episode steps: 534, steps per second: 194, episode reward: 206.757, mean reward: 0.387 [-18.011, 100.000], mean action: 0.848 [0.000, 3.000], mean observation: 0.142 [-0.621, 1.000], loss: 6.915723, mean_absolute_error: 50.438847, mean_q: 67.524567\n",
      " 492589/700000: episode: 1388, duration: 1.294s, episode steps: 252, steps per second: 195, episode reward: 199.936, mean reward: 0.793 [-9.370, 100.000], mean action: 1.167 [0.000, 3.000], mean observation: 0.098 [-0.772, 1.000], loss: 4.400600, mean_absolute_error: 50.925163, mean_q: 67.902809\n",
      " 492806/700000: episode: 1389, duration: 1.105s, episode steps: 217, steps per second: 196, episode reward: 232.277, mean reward: 1.070 [-5.776, 100.000], mean action: 1.060 [0.000, 3.000], mean observation: 0.087 [-0.907, 1.000], loss: 7.131074, mean_absolute_error: 51.183235, mean_q: 68.309288\n",
      " 493171/700000: episode: 1390, duration: 1.859s, episode steps: 365, steps per second: 196, episode reward: 212.330, mean reward: 0.582 [-17.935, 100.000], mean action: 0.679 [0.000, 3.000], mean observation: 0.162 [-0.927, 1.092], loss: 7.462241, mean_absolute_error: 50.660625, mean_q: 67.784569\n",
      " 493590/700000: episode: 1391, duration: 2.252s, episode steps: 419, steps per second: 186, episode reward: 245.996, mean reward: 0.587 [-8.156, 100.000], mean action: 1.277 [0.000, 3.000], mean observation: 0.086 [-0.715, 1.003], loss: 12.172334, mean_absolute_error: 50.381599, mean_q: 67.400589\n",
      " 493753/700000: episode: 1392, duration: 0.815s, episode steps: 163, steps per second: 200, episode reward: -82.812, mean reward: -0.508 [-100.000, 7.276], mean action: 1.528 [0.000, 3.000], mean observation: 0.163 [-0.645, 1.105], loss: 11.882624, mean_absolute_error: 51.158096, mean_q: 68.255692\n",
      " 494044/700000: episode: 1393, duration: 1.476s, episode steps: 291, steps per second: 197, episode reward: 242.578, mean reward: 0.834 [-21.856, 100.000], mean action: 1.052 [0.000, 3.000], mean observation: 0.084 [-0.822, 1.000], loss: 7.387107, mean_absolute_error: 50.706383, mean_q: 67.892883\n",
      " 494322/700000: episode: 1394, duration: 1.405s, episode steps: 278, steps per second: 198, episode reward: 231.837, mean reward: 0.834 [-18.795, 100.000], mean action: 1.083 [0.000, 3.000], mean observation: 0.087 [-0.946, 1.000], loss: 4.861098, mean_absolute_error: 50.704113, mean_q: 67.748138\n",
      " 494524/700000: episode: 1395, duration: 1.008s, episode steps: 202, steps per second: 200, episode reward: 251.586, mean reward: 1.245 [-19.675, 100.000], mean action: 1.054 [0.000, 3.000], mean observation: 0.078 [-0.854, 1.000], loss: 6.588522, mean_absolute_error: 50.264256, mean_q: 67.406860\n",
      " 494725/700000: episode: 1396, duration: 1.005s, episode steps: 201, steps per second: 200, episode reward: 222.461, mean reward: 1.107 [-5.329, 100.000], mean action: 1.010 [0.000, 3.000], mean observation: 0.116 [-1.094, 1.207], loss: 6.027542, mean_absolute_error: 51.127350, mean_q: 68.355553\n",
      " 495497/700000: episode: 1397, duration: 4.212s, episode steps: 772, steps per second: 183, episode reward: 120.805, mean reward: 0.156 [-20.376, 100.000], mean action: 0.917 [0.000, 3.000], mean observation: 0.208 [-1.305, 1.000], loss: 8.146936, mean_absolute_error: 51.023628, mean_q: 68.135345\n",
      " 495690/700000: episode: 1398, duration: 0.979s, episode steps: 193, steps per second: 197, episode reward: 197.738, mean reward: 1.025 [-8.620, 100.000], mean action: 1.420 [0.000, 3.000], mean observation: 0.150 [-1.030, 1.000], loss: 8.921551, mean_absolute_error: 50.850010, mean_q: 67.989235\n",
      " 495879/700000: episode: 1399, duration: 0.946s, episode steps: 189, steps per second: 200, episode reward: 206.061, mean reward: 1.090 [-10.658, 100.000], mean action: 1.138 [0.000, 3.000], mean observation: 0.085 [-0.945, 1.000], loss: 12.248206, mean_absolute_error: 51.097595, mean_q: 67.906113\n",
      " 496384/700000: episode: 1400, duration: 2.606s, episode steps: 505, steps per second: 194, episode reward: 207.477, mean reward: 0.411 [-20.279, 100.000], mean action: 0.840 [0.000, 3.000], mean observation: 0.167 [-1.059, 1.615], loss: 10.385813, mean_absolute_error: 51.382504, mean_q: 68.466972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 496579/700000: episode: 1401, duration: 0.977s, episode steps: 195, steps per second: 200, episode reward: 242.190, mean reward: 1.242 [-9.423, 100.000], mean action: 1.072 [0.000, 3.000], mean observation: 0.090 [-0.995, 1.060], loss: 10.138759, mean_absolute_error: 51.577122, mean_q: 68.847404\n",
      " 496808/700000: episode: 1402, duration: 1.153s, episode steps: 229, steps per second: 199, episode reward: 223.093, mean reward: 0.974 [-8.951, 100.000], mean action: 1.319 [0.000, 3.000], mean observation: 0.104 [-0.825, 1.461], loss: 9.178043, mean_absolute_error: 51.231293, mean_q: 68.575317\n",
      " 497180/700000: episode: 1403, duration: 1.883s, episode steps: 372, steps per second: 198, episode reward: 232.794, mean reward: 0.626 [-22.999, 100.000], mean action: 0.723 [0.000, 3.000], mean observation: 0.170 [-1.094, 1.236], loss: 13.641322, mean_absolute_error: 51.344742, mean_q: 68.654610\n",
      " 497393/700000: episode: 1404, duration: 1.072s, episode steps: 213, steps per second: 199, episode reward: 209.650, mean reward: 0.984 [-11.769, 100.000], mean action: 1.160 [0.000, 3.000], mean observation: 0.044 [-0.931, 1.055], loss: 8.816393, mean_absolute_error: 51.219685, mean_q: 68.648407\n",
      " 497585/700000: episode: 1405, duration: 0.960s, episode steps: 192, steps per second: 200, episode reward: 200.222, mean reward: 1.043 [-8.286, 100.000], mean action: 1.083 [0.000, 3.000], mean observation: 0.069 [-1.049, 1.000], loss: 13.000793, mean_absolute_error: 51.035564, mean_q: 68.253777\n",
      " 497844/700000: episode: 1406, duration: 1.319s, episode steps: 259, steps per second: 196, episode reward: 189.530, mean reward: 0.732 [-10.515, 100.000], mean action: 1.259 [0.000, 3.000], mean observation: 0.136 [-0.999, 1.000], loss: 7.986257, mean_absolute_error: 51.176003, mean_q: 68.242912\n",
      " 498029/700000: episode: 1407, duration: 0.936s, episode steps: 185, steps per second: 198, episode reward: -565.380, mean reward: -3.056 [-100.000, 3.438], mean action: 1.951 [0.000, 3.000], mean observation: 0.540 [-0.757, 4.071], loss: 5.343239, mean_absolute_error: 51.462437, mean_q: 68.548943\n",
      " 498410/700000: episode: 1408, duration: 1.953s, episode steps: 381, steps per second: 195, episode reward: 227.023, mean reward: 0.596 [-17.456, 100.000], mean action: 0.801 [0.000, 3.000], mean observation: 0.128 [-0.773, 1.000], loss: 6.465389, mean_absolute_error: 51.311371, mean_q: 68.606422\n",
      " 498631/700000: episode: 1409, duration: 1.106s, episode steps: 221, steps per second: 200, episode reward: 203.744, mean reward: 0.922 [-8.560, 100.000], mean action: 0.900 [0.000, 3.000], mean observation: 0.107 [-0.798, 1.000], loss: 9.134234, mean_absolute_error: 51.206440, mean_q: 68.487206\n",
      " 498892/700000: episode: 1410, duration: 1.329s, episode steps: 261, steps per second: 196, episode reward: 231.791, mean reward: 0.888 [-19.791, 100.000], mean action: 1.092 [0.000, 3.000], mean observation: 0.123 [-1.059, 1.000], loss: 17.417080, mean_absolute_error: 51.608948, mean_q: 68.971390\n",
      " 499051/700000: episode: 1411, duration: 0.800s, episode steps: 159, steps per second: 199, episode reward: -32.977, mean reward: -0.207 [-100.000, 16.720], mean action: 1.780 [0.000, 3.000], mean observation: -0.073 [-0.891, 1.278], loss: 9.162530, mean_absolute_error: 50.832325, mean_q: 68.046387\n",
      " 499253/700000: episode: 1412, duration: 0.996s, episode steps: 202, steps per second: 203, episode reward: 204.668, mean reward: 1.013 [-6.053, 100.000], mean action: 0.817 [0.000, 3.000], mean observation: 0.100 [-0.825, 1.000], loss: 14.368145, mean_absolute_error: 51.278015, mean_q: 68.553879\n",
      " 499527/700000: episode: 1413, duration: 1.399s, episode steps: 274, steps per second: 196, episode reward: 220.436, mean reward: 0.805 [-3.787, 100.000], mean action: 1.339 [0.000, 3.000], mean observation: 0.113 [-0.635, 1.455], loss: 7.626481, mean_absolute_error: 51.557770, mean_q: 68.898712\n",
      " 499953/700000: episode: 1414, duration: 2.137s, episode steps: 426, steps per second: 199, episode reward: 242.641, mean reward: 0.570 [-18.655, 100.000], mean action: 0.812 [0.000, 3.000], mean observation: 0.139 [-0.920, 1.424], loss: 8.914382, mean_absolute_error: 51.633541, mean_q: 69.089088\n",
      " 500037/700000: episode: 1415, duration: 0.425s, episode steps: 84, steps per second: 198, episode reward: -35.410, mean reward: -0.422 [-100.000, 9.983], mean action: 1.845 [0.000, 3.000], mean observation: 0.069 [-1.041, 1.000], loss: 13.586015, mean_absolute_error: 51.616844, mean_q: 69.437759\n",
      " 500430/700000: episode: 1416, duration: 2.061s, episode steps: 393, steps per second: 191, episode reward: 163.725, mean reward: 0.417 [-12.588, 100.000], mean action: 1.896 [0.000, 3.000], mean observation: 0.063 [-0.783, 1.002], loss: 8.295775, mean_absolute_error: 51.287140, mean_q: 68.697868\n",
      " 500718/700000: episode: 1417, duration: 1.451s, episode steps: 288, steps per second: 199, episode reward: 189.927, mean reward: 0.659 [-18.274, 100.000], mean action: 0.799 [0.000, 3.000], mean observation: 0.124 [-0.947, 1.000], loss: 9.637004, mean_absolute_error: 51.122974, mean_q: 68.567192\n",
      " 501079/700000: episode: 1418, duration: 1.853s, episode steps: 361, steps per second: 195, episode reward: 232.926, mean reward: 0.645 [-10.634, 100.000], mean action: 0.690 [0.000, 3.000], mean observation: 0.152 [-0.904, 1.000], loss: 6.733845, mean_absolute_error: 51.394760, mean_q: 68.885941\n",
      " 501295/700000: episode: 1419, duration: 1.096s, episode steps: 216, steps per second: 197, episode reward: 211.557, mean reward: 0.979 [-8.608, 100.000], mean action: 1.319 [0.000, 3.000], mean observation: 0.036 [-0.856, 1.000], loss: 7.673229, mean_absolute_error: 51.639103, mean_q: 69.202133\n",
      " 501642/700000: episode: 1420, duration: 1.814s, episode steps: 347, steps per second: 191, episode reward: 195.236, mean reward: 0.563 [-18.184, 100.000], mean action: 0.934 [0.000, 3.000], mean observation: 0.100 [-0.847, 1.000], loss: 12.318317, mean_absolute_error: 51.410225, mean_q: 68.989388\n",
      " 501881/700000: episode: 1421, duration: 1.230s, episode steps: 239, steps per second: 194, episode reward: -52.337, mean reward: -0.219 [-100.000, 17.942], mean action: 1.987 [0.000, 3.000], mean observation: 0.108 [-1.047, 1.019], loss: 12.414767, mean_absolute_error: 51.330822, mean_q: 68.950562\n",
      " 502055/700000: episode: 1422, duration: 0.866s, episode steps: 174, steps per second: 201, episode reward: 232.786, mean reward: 1.338 [-9.422, 100.000], mean action: 1.121 [0.000, 3.000], mean observation: 0.076 [-1.092, 1.000], loss: 9.108869, mean_absolute_error: 51.661346, mean_q: 69.036034\n",
      " 502163/700000: episode: 1423, duration: 0.542s, episode steps: 108, steps per second: 199, episode reward: -65.302, mean reward: -0.605 [-100.000, 7.402], mean action: 1.824 [0.000, 3.000], mean observation: 0.091 [-1.077, 1.687], loss: 7.611516, mean_absolute_error: 51.866234, mean_q: 69.207924\n",
      " 502246/700000: episode: 1424, duration: 0.423s, episode steps: 83, steps per second: 196, episode reward: -54.088, mean reward: -0.652 [-100.000, 18.246], mean action: 1.675 [0.000, 3.000], mean observation: -0.135 [-0.904, 1.622], loss: 7.929910, mean_absolute_error: 51.147408, mean_q: 68.570145\n",
      " 502408/700000: episode: 1425, duration: 0.806s, episode steps: 162, steps per second: 201, episode reward: 229.966, mean reward: 1.420 [-8.090, 100.000], mean action: 1.080 [0.000, 3.000], mean observation: 0.149 [-1.110, 1.000], loss: 6.393929, mean_absolute_error: 51.330994, mean_q: 69.004715\n",
      " 502586/700000: episode: 1426, duration: 0.894s, episode steps: 178, steps per second: 199, episode reward: 209.146, mean reward: 1.175 [-9.633, 100.000], mean action: 1.315 [0.000, 3.000], mean observation: 0.126 [-0.940, 1.000], loss: 10.976505, mean_absolute_error: 51.229809, mean_q: 68.664345\n",
      " 502878/700000: episode: 1427, duration: 1.547s, episode steps: 292, steps per second: 189, episode reward: 204.166, mean reward: 0.699 [-18.902, 100.000], mean action: 1.253 [0.000, 3.000], mean observation: 0.133 [-0.712, 1.000], loss: 8.572105, mean_absolute_error: 52.008396, mean_q: 69.480576\n",
      " 503111/700000: episode: 1428, duration: 1.165s, episode steps: 233, steps per second: 200, episode reward: 217.272, mean reward: 0.932 [-9.512, 100.000], mean action: 1.047 [0.000, 3.000], mean observation: 0.094 [-0.865, 1.000], loss: 7.829192, mean_absolute_error: 51.507710, mean_q: 68.953796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 503352/700000: episode: 1429, duration: 1.209s, episode steps: 241, steps per second: 199, episode reward: 247.774, mean reward: 1.028 [-6.238, 100.000], mean action: 1.100 [0.000, 3.000], mean observation: 0.029 [-0.752, 1.000], loss: 19.258873, mean_absolute_error: 51.021412, mean_q: 68.149498\n",
      " 503595/700000: episode: 1430, duration: 1.235s, episode steps: 243, steps per second: 197, episode reward: 235.279, mean reward: 0.968 [-11.134, 100.000], mean action: 0.996 [0.000, 3.000], mean observation: 0.063 [-0.777, 1.158], loss: 10.038820, mean_absolute_error: 51.592415, mean_q: 69.048164\n",
      " 503854/700000: episode: 1431, duration: 1.315s, episode steps: 259, steps per second: 197, episode reward: 195.601, mean reward: 0.755 [-19.874, 100.000], mean action: 1.216 [0.000, 3.000], mean observation: 0.136 [-0.934, 1.000], loss: 24.822142, mean_absolute_error: 51.536869, mean_q: 68.998344\n",
      " 503969/700000: episode: 1432, duration: 0.575s, episode steps: 115, steps per second: 200, episode reward: -77.660, mean reward: -0.675 [-100.000, 7.822], mean action: 1.670 [0.000, 3.000], mean observation: 0.062 [-0.983, 3.107], loss: 7.427534, mean_absolute_error: 51.336441, mean_q: 68.656624\n",
      " 504331/700000: episode: 1433, duration: 1.915s, episode steps: 362, steps per second: 189, episode reward: 227.702, mean reward: 0.629 [-17.600, 100.000], mean action: 1.227 [0.000, 3.000], mean observation: 0.045 [-0.870, 1.000], loss: 11.428408, mean_absolute_error: 51.416142, mean_q: 68.846481\n",
      " 504625/700000: episode: 1434, duration: 1.513s, episode steps: 294, steps per second: 194, episode reward: 237.799, mean reward: 0.809 [-7.455, 100.000], mean action: 1.119 [0.000, 3.000], mean observation: 0.138 [-0.905, 1.000], loss: 7.335662, mean_absolute_error: 51.457199, mean_q: 68.801994\n",
      " 504862/700000: episode: 1435, duration: 1.195s, episode steps: 237, steps per second: 198, episode reward: 217.910, mean reward: 0.919 [-8.258, 100.000], mean action: 1.287 [0.000, 3.000], mean observation: 0.084 [-0.811, 1.000], loss: 10.699369, mean_absolute_error: 51.189312, mean_q: 68.822098\n",
      " 505266/700000: episode: 1436, duration: 2.177s, episode steps: 404, steps per second: 186, episode reward: 198.379, mean reward: 0.491 [-11.405, 100.000], mean action: 1.968 [0.000, 3.000], mean observation: 0.130 [-0.871, 1.000], loss: 7.181984, mean_absolute_error: 51.320915, mean_q: 68.920189\n",
      " 505573/700000: episode: 1437, duration: 1.590s, episode steps: 307, steps per second: 193, episode reward: 206.664, mean reward: 0.673 [-19.582, 100.000], mean action: 0.941 [0.000, 3.000], mean observation: 0.089 [-0.865, 1.000], loss: 10.528471, mean_absolute_error: 51.369171, mean_q: 69.028152\n",
      " 505796/700000: episode: 1438, duration: 1.106s, episode steps: 223, steps per second: 202, episode reward: 242.658, mean reward: 1.088 [-11.649, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.095 [-0.989, 1.016], loss: 10.615000, mean_absolute_error: 51.984524, mean_q: 69.749039\n",
      " 506019/700000: episode: 1439, duration: 1.190s, episode steps: 223, steps per second: 187, episode reward: 223.964, mean reward: 1.004 [-3.849, 100.000], mean action: 1.148 [0.000, 3.000], mean observation: 0.111 [-0.843, 1.000], loss: 7.657963, mean_absolute_error: 51.692921, mean_q: 69.157593\n",
      " 506239/700000: episode: 1440, duration: 1.215s, episode steps: 220, steps per second: 181, episode reward: 214.391, mean reward: 0.975 [-3.235, 100.000], mean action: 1.045 [0.000, 3.000], mean observation: 0.115 [-0.995, 1.000], loss: 7.267369, mean_absolute_error: 51.260475, mean_q: 68.551689\n",
      " 506601/700000: episode: 1441, duration: 1.997s, episode steps: 362, steps per second: 181, episode reward: 229.381, mean reward: 0.634 [-9.758, 100.000], mean action: 0.994 [0.000, 3.000], mean observation: 0.149 [-0.947, 1.000], loss: 6.943794, mean_absolute_error: 51.447178, mean_q: 68.793068\n",
      " 506814/700000: episode: 1442, duration: 1.106s, episode steps: 213, steps per second: 193, episode reward: 229.763, mean reward: 1.079 [-18.932, 100.000], mean action: 1.230 [0.000, 3.000], mean observation: 0.099 [-0.659, 1.171], loss: 4.758622, mean_absolute_error: 51.223545, mean_q: 68.781784\n",
      " 507334/700000: episode: 1443, duration: 3.288s, episode steps: 520, steps per second: 158, episode reward: 261.297, mean reward: 0.502 [-17.658, 100.000], mean action: 0.658 [0.000, 3.000], mean observation: 0.150 [-0.739, 1.000], loss: 6.158433, mean_absolute_error: 51.279762, mean_q: 68.732712\n",
      " 507619/700000: episode: 1444, duration: 1.591s, episode steps: 285, steps per second: 179, episode reward: 240.567, mean reward: 0.844 [-8.992, 100.000], mean action: 1.361 [0.000, 3.000], mean observation: 0.081 [-0.655, 1.000], loss: 8.668990, mean_absolute_error: 51.244461, mean_q: 68.656647\n",
      " 508050/700000: episode: 1445, duration: 2.238s, episode steps: 431, steps per second: 193, episode reward: 204.302, mean reward: 0.474 [-20.021, 100.000], mean action: 1.295 [0.000, 3.000], mean observation: 0.047 [-0.943, 1.382], loss: 8.565104, mean_absolute_error: 51.466843, mean_q: 68.954414\n",
      " 508324/700000: episode: 1446, duration: 1.391s, episode steps: 274, steps per second: 197, episode reward: 224.281, mean reward: 0.819 [-18.548, 100.000], mean action: 1.303 [0.000, 3.000], mean observation: 0.132 [-0.837, 1.000], loss: 14.945819, mean_absolute_error: 51.853901, mean_q: 69.477592\n",
      " 508779/700000: episode: 1447, duration: 2.464s, episode steps: 455, steps per second: 185, episode reward: 207.838, mean reward: 0.457 [-19.958, 100.000], mean action: 1.176 [0.000, 3.000], mean observation: 0.168 [-0.833, 1.000], loss: 7.019203, mean_absolute_error: 51.479694, mean_q: 68.626091\n",
      " 508892/700000: episode: 1448, duration: 0.570s, episode steps: 113, steps per second: 198, episode reward: -25.482, mean reward: -0.226 [-100.000, 16.420], mean action: 1.442 [0.000, 3.000], mean observation: 0.056 [-1.593, 1.000], loss: 13.169492, mean_absolute_error: 51.997192, mean_q: 69.201019\n",
      " 509234/700000: episode: 1449, duration: 1.743s, episode steps: 342, steps per second: 196, episode reward: 279.435, mean reward: 0.817 [-17.341, 100.000], mean action: 0.757 [0.000, 3.000], mean observation: 0.141 [-0.973, 1.000], loss: 8.504717, mean_absolute_error: 51.205593, mean_q: 68.325302\n",
      " 509455/700000: episode: 1450, duration: 1.127s, episode steps: 221, steps per second: 196, episode reward: 257.457, mean reward: 1.165 [-9.408, 100.000], mean action: 1.249 [0.000, 3.000], mean observation: 0.068 [-0.809, 1.000], loss: 8.349827, mean_absolute_error: 51.579132, mean_q: 68.896072\n",
      " 509634/700000: episode: 1451, duration: 0.895s, episode steps: 179, steps per second: 200, episode reward: 253.090, mean reward: 1.414 [-17.344, 100.000], mean action: 1.117 [0.000, 3.000], mean observation: 0.150 [-1.032, 1.000], loss: 13.798730, mean_absolute_error: 51.365192, mean_q: 68.548576\n",
      " 510543/700000: episode: 1452, duration: 4.935s, episode steps: 909, steps per second: 184, episode reward: 199.400, mean reward: 0.219 [-19.474, 100.000], mean action: 0.831 [0.000, 3.000], mean observation: 0.222 [-0.988, 1.013], loss: 9.798653, mean_absolute_error: 51.431908, mean_q: 68.692566\n",
      " 510659/700000: episode: 1453, duration: 0.588s, episode steps: 116, steps per second: 197, episode reward: -14.843, mean reward: -0.128 [-100.000, 20.834], mean action: 1.647 [0.000, 3.000], mean observation: -0.062 [-0.771, 1.000], loss: 11.342007, mean_absolute_error: 50.672146, mean_q: 67.686821\n",
      " 510765/700000: episode: 1454, duration: 0.532s, episode steps: 106, steps per second: 199, episode reward: -52.902, mean reward: -0.499 [-100.000, 11.471], mean action: 1.481 [0.000, 3.000], mean observation: 0.070 [-1.793, 1.000], loss: 4.678968, mean_absolute_error: 51.489590, mean_q: 68.404251\n",
      " 511036/700000: episode: 1455, duration: 1.381s, episode steps: 271, steps per second: 196, episode reward: 259.715, mean reward: 0.958 [-10.609, 100.000], mean action: 1.151 [0.000, 3.000], mean observation: 0.131 [-0.958, 1.000], loss: 8.172209, mean_absolute_error: 51.729271, mean_q: 69.211197\n",
      " 511296/700000: episode: 1456, duration: 1.323s, episode steps: 260, steps per second: 197, episode reward: 226.648, mean reward: 0.872 [-9.071, 100.000], mean action: 1.369 [0.000, 3.000], mean observation: 0.133 [-1.111, 1.000], loss: 13.305386, mean_absolute_error: 51.418530, mean_q: 68.549690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 511608/700000: episode: 1457, duration: 1.600s, episode steps: 312, steps per second: 195, episode reward: 257.429, mean reward: 0.825 [-11.894, 100.000], mean action: 1.250 [0.000, 3.000], mean observation: 0.086 [-1.551, 1.072], loss: 10.160022, mean_absolute_error: 51.470345, mean_q: 68.877502\n",
      " 511879/700000: episode: 1458, duration: 1.354s, episode steps: 271, steps per second: 200, episode reward: 265.170, mean reward: 0.978 [-12.384, 100.000], mean action: 0.985 [0.000, 3.000], mean observation: 0.141 [-0.923, 1.023], loss: 12.360262, mean_absolute_error: 51.471771, mean_q: 68.831558\n",
      " 512278/700000: episode: 1459, duration: 2.020s, episode steps: 399, steps per second: 198, episode reward: 250.772, mean reward: 0.629 [-9.041, 100.000], mean action: 0.980 [0.000, 3.000], mean observation: 0.111 [-0.935, 1.000], loss: 7.245112, mean_absolute_error: 51.335678, mean_q: 68.734764\n",
      " 512360/700000: episode: 1460, duration: 0.421s, episode steps: 82, steps per second: 195, episode reward: -14.990, mean reward: -0.183 [-100.000, 19.247], mean action: 1.707 [0.000, 3.000], mean observation: 0.013 [-1.028, 1.093], loss: 7.145880, mean_absolute_error: 51.491699, mean_q: 69.077675\n",
      " 512644/700000: episode: 1461, duration: 1.447s, episode steps: 284, steps per second: 196, episode reward: 242.259, mean reward: 0.853 [-9.215, 100.000], mean action: 0.968 [0.000, 3.000], mean observation: 0.101 [-1.058, 1.000], loss: 9.772583, mean_absolute_error: 51.366322, mean_q: 68.517174\n",
      " 512889/700000: episode: 1462, duration: 1.230s, episode steps: 245, steps per second: 199, episode reward: 243.545, mean reward: 0.994 [-18.048, 100.000], mean action: 0.963 [0.000, 3.000], mean observation: 0.119 [-0.986, 1.000], loss: 10.273410, mean_absolute_error: 51.604824, mean_q: 69.105515\n",
      " 513434/700000: episode: 1463, duration: 2.902s, episode steps: 545, steps per second: 188, episode reward: 143.121, mean reward: 0.263 [-20.334, 100.000], mean action: 1.793 [0.000, 3.000], mean observation: 0.160 [-1.103, 1.000], loss: 8.062627, mean_absolute_error: 51.520756, mean_q: 68.806931\n",
      " 513529/700000: episode: 1464, duration: 0.486s, episode steps: 95, steps per second: 195, episode reward: -24.637, mean reward: -0.259 [-100.000, 20.735], mean action: 1.789 [0.000, 3.000], mean observation: -0.062 [-0.977, 1.600], loss: 5.438353, mean_absolute_error: 51.424149, mean_q: 68.499550\n",
      " 514073/700000: episode: 1465, duration: 2.877s, episode steps: 544, steps per second: 189, episode reward: 230.043, mean reward: 0.423 [-17.860, 100.000], mean action: 0.767 [0.000, 3.000], mean observation: 0.200 [-1.101, 1.000], loss: 8.457550, mean_absolute_error: 51.510693, mean_q: 68.840721\n",
      " 514357/700000: episode: 1466, duration: 1.455s, episode steps: 284, steps per second: 195, episode reward: 203.592, mean reward: 0.717 [-17.163, 100.000], mean action: 1.602 [0.000, 3.000], mean observation: 0.217 [-1.012, 1.148], loss: 7.938079, mean_absolute_error: 51.066490, mean_q: 68.261902\n",
      " 514602/700000: episode: 1467, duration: 1.231s, episode steps: 245, steps per second: 199, episode reward: 268.127, mean reward: 1.094 [-11.234, 100.000], mean action: 1.200 [0.000, 3.000], mean observation: 0.073 [-0.895, 1.000], loss: 8.609485, mean_absolute_error: 51.245590, mean_q: 68.443016\n",
      " 514875/700000: episode: 1468, duration: 1.390s, episode steps: 273, steps per second: 196, episode reward: 204.886, mean reward: 0.750 [-22.640, 100.000], mean action: 1.044 [0.000, 3.000], mean observation: 0.110 [-1.093, 1.000], loss: 8.703005, mean_absolute_error: 51.689907, mean_q: 69.241402\n",
      " 515114/700000: episode: 1469, duration: 1.202s, episode steps: 239, steps per second: 199, episode reward: 233.984, mean reward: 0.979 [-9.124, 100.000], mean action: 1.255 [0.000, 3.000], mean observation: 0.031 [-0.927, 1.014], loss: 14.132621, mean_absolute_error: 51.004749, mean_q: 68.103477\n",
      " 515330/700000: episode: 1470, duration: 1.086s, episode steps: 216, steps per second: 199, episode reward: 248.550, mean reward: 1.151 [-9.660, 100.000], mean action: 1.111 [0.000, 3.000], mean observation: 0.131 [-0.892, 1.387], loss: 10.057323, mean_absolute_error: 50.736012, mean_q: 67.749573\n",
      " 515626/700000: episode: 1471, duration: 1.501s, episode steps: 296, steps per second: 197, episode reward: 266.798, mean reward: 0.901 [-9.957, 100.000], mean action: 1.095 [0.000, 3.000], mean observation: 0.099 [-0.914, 1.000], loss: 10.966236, mean_absolute_error: 50.907879, mean_q: 67.960144\n",
      " 515799/700000: episode: 1472, duration: 0.864s, episode steps: 173, steps per second: 200, episode reward: 218.691, mean reward: 1.264 [-6.830, 100.000], mean action: 1.289 [0.000, 3.000], mean observation: 0.073 [-1.000, 1.000], loss: 5.800756, mean_absolute_error: 51.409130, mean_q: 68.348579\n",
      " 516063/700000: episode: 1473, duration: 1.360s, episode steps: 264, steps per second: 194, episode reward: 254.980, mean reward: 0.966 [-19.188, 100.000], mean action: 1.182 [0.000, 3.000], mean observation: 0.131 [-1.124, 1.000], loss: 7.902828, mean_absolute_error: 51.266117, mean_q: 68.199722\n",
      " 516274/700000: episode: 1474, duration: 1.052s, episode steps: 211, steps per second: 201, episode reward: 207.902, mean reward: 0.985 [-11.820, 100.000], mean action: 0.995 [0.000, 3.000], mean observation: 0.109 [-0.964, 1.000], loss: 14.515083, mean_absolute_error: 50.915459, mean_q: 68.230888\n",
      " 516906/700000: episode: 1475, duration: 3.426s, episode steps: 632, steps per second: 184, episode reward: 120.899, mean reward: 0.191 [-24.053, 100.000], mean action: 1.326 [0.000, 3.000], mean observation: 0.183 [-0.981, 1.000], loss: 10.601704, mean_absolute_error: 50.973003, mean_q: 67.952309\n",
      " 517142/700000: episode: 1476, duration: 1.183s, episode steps: 236, steps per second: 200, episode reward: 253.136, mean reward: 1.073 [-17.519, 100.000], mean action: 1.140 [0.000, 3.000], mean observation: 0.123 [-0.911, 1.161], loss: 5.758717, mean_absolute_error: 50.647980, mean_q: 67.507950\n",
      " 517357/700000: episode: 1477, duration: 1.083s, episode steps: 215, steps per second: 198, episode reward: 210.276, mean reward: 0.978 [-11.335, 100.000], mean action: 1.070 [0.000, 3.000], mean observation: 0.107 [-1.022, 1.000], loss: 11.780650, mean_absolute_error: 51.090778, mean_q: 68.018883\n",
      " 517558/700000: episode: 1478, duration: 1.004s, episode steps: 201, steps per second: 200, episode reward: 246.446, mean reward: 1.226 [-7.478, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.142 [-0.913, 1.000], loss: 9.428028, mean_absolute_error: 50.508415, mean_q: 67.596169\n",
      " 518009/700000: episode: 1479, duration: 2.363s, episode steps: 451, steps per second: 191, episode reward: 215.736, mean reward: 0.478 [-18.178, 100.000], mean action: 1.120 [0.000, 3.000], mean observation: 0.129 [-0.930, 1.000], loss: 9.242959, mean_absolute_error: 50.803604, mean_q: 67.877647\n",
      " 518316/700000: episode: 1480, duration: 1.576s, episode steps: 307, steps per second: 195, episode reward: 218.521, mean reward: 0.712 [-17.510, 100.000], mean action: 0.883 [0.000, 3.000], mean observation: 0.099 [-0.781, 1.000], loss: 9.691375, mean_absolute_error: 50.584492, mean_q: 67.626160\n",
      " 518637/700000: episode: 1481, duration: 1.696s, episode steps: 321, steps per second: 189, episode reward: 150.778, mean reward: 0.470 [-16.251, 100.000], mean action: 1.492 [0.000, 3.000], mean observation: 0.140 [-1.041, 1.011], loss: 10.548318, mean_absolute_error: 50.847164, mean_q: 68.121277\n",
      " 519057/700000: episode: 1482, duration: 2.191s, episode steps: 420, steps per second: 192, episode reward: 219.827, mean reward: 0.523 [-18.070, 100.000], mean action: 1.183 [0.000, 3.000], mean observation: 0.213 [-0.932, 1.000], loss: 8.546585, mean_absolute_error: 50.893688, mean_q: 67.881119\n",
      " 519200/700000: episode: 1483, duration: 0.725s, episode steps: 143, steps per second: 197, episode reward: 13.879, mean reward: 0.097 [-100.000, 15.544], mean action: 1.867 [0.000, 3.000], mean observation: 0.078 [-1.065, 1.000], loss: 11.813688, mean_absolute_error: 51.256737, mean_q: 68.304878\n",
      " 519508/700000: episode: 1484, duration: 1.639s, episode steps: 308, steps per second: 188, episode reward: 227.099, mean reward: 0.737 [-11.131, 100.000], mean action: 1.175 [0.000, 3.000], mean observation: 0.131 [-1.009, 1.197], loss: 10.478285, mean_absolute_error: 51.133930, mean_q: 68.231102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 519728/700000: episode: 1485, duration: 1.118s, episode steps: 220, steps per second: 197, episode reward: 216.194, mean reward: 0.983 [-18.702, 100.000], mean action: 0.955 [0.000, 3.000], mean observation: 0.101 [-0.931, 1.000], loss: 8.349031, mean_absolute_error: 50.765240, mean_q: 67.815178\n",
      " 520087/700000: episode: 1486, duration: 1.894s, episode steps: 359, steps per second: 190, episode reward: 207.764, mean reward: 0.579 [-19.459, 100.000], mean action: 2.245 [0.000, 3.000], mean observation: 0.197 [-0.839, 1.000], loss: 13.324205, mean_absolute_error: 50.930130, mean_q: 67.941650\n",
      " 520361/700000: episode: 1487, duration: 1.405s, episode steps: 274, steps per second: 195, episode reward: 236.348, mean reward: 0.863 [-6.327, 100.000], mean action: 1.785 [0.000, 3.000], mean observation: 0.206 [-0.875, 1.378], loss: 11.107065, mean_absolute_error: 51.047260, mean_q: 68.299232\n",
      " 520661/700000: episode: 1488, duration: 1.510s, episode steps: 300, steps per second: 199, episode reward: 232.898, mean reward: 0.776 [-9.984, 100.000], mean action: 0.820 [0.000, 3.000], mean observation: 0.104 [-0.956, 1.046], loss: 11.099112, mean_absolute_error: 51.319458, mean_q: 68.494995\n",
      " 520893/700000: episode: 1489, duration: 1.180s, episode steps: 232, steps per second: 197, episode reward: 249.250, mean reward: 1.074 [-3.163, 100.000], mean action: 1.142 [0.000, 3.000], mean observation: 0.048 [-0.797, 1.074], loss: 11.467313, mean_absolute_error: 51.563797, mean_q: 68.644127\n",
      " 521114/700000: episode: 1490, duration: 1.101s, episode steps: 221, steps per second: 201, episode reward: 245.100, mean reward: 1.109 [-11.671, 100.000], mean action: 0.982 [0.000, 3.000], mean observation: 0.095 [-0.848, 1.110], loss: 10.325249, mean_absolute_error: 50.776424, mean_q: 67.788567\n",
      " 521216/700000: episode: 1491, duration: 0.512s, episode steps: 102, steps per second: 199, episode reward: -37.512, mean reward: -0.368 [-100.000, 14.435], mean action: 1.725 [0.000, 3.000], mean observation: -0.030 [-0.982, 1.205], loss: 10.074910, mean_absolute_error: 51.712841, mean_q: 69.017822\n",
      " 521374/700000: episode: 1492, duration: 0.788s, episode steps: 158, steps per second: 201, episode reward: 218.161, mean reward: 1.381 [-8.826, 100.000], mean action: 1.108 [0.000, 3.000], mean observation: 0.085 [-1.069, 1.000], loss: 10.059095, mean_absolute_error: 50.853554, mean_q: 67.894844\n",
      " 521657/700000: episode: 1493, duration: 1.437s, episode steps: 283, steps per second: 197, episode reward: 226.878, mean reward: 0.802 [-18.511, 100.000], mean action: 0.816 [0.000, 3.000], mean observation: 0.140 [-0.811, 1.251], loss: 10.373625, mean_absolute_error: 51.356457, mean_q: 68.516121\n",
      " 521896/700000: episode: 1494, duration: 1.208s, episode steps: 239, steps per second: 198, episode reward: 201.619, mean reward: 0.844 [-3.323, 100.000], mean action: 0.971 [0.000, 3.000], mean observation: 0.105 [-0.944, 1.000], loss: 8.606562, mean_absolute_error: 51.082096, mean_q: 68.295067\n",
      " 522147/700000: episode: 1495, duration: 1.275s, episode steps: 251, steps per second: 197, episode reward: 216.778, mean reward: 0.864 [-17.762, 100.000], mean action: 1.255 [0.000, 3.000], mean observation: 0.079 [-0.734, 1.000], loss: 6.815643, mean_absolute_error: 51.605274, mean_q: 69.111153\n",
      " 522661/700000: episode: 1496, duration: 2.661s, episode steps: 514, steps per second: 193, episode reward: 204.368, mean reward: 0.398 [-20.368, 100.000], mean action: 0.679 [0.000, 3.000], mean observation: 0.179 [-0.939, 1.000], loss: 8.329855, mean_absolute_error: 51.344032, mean_q: 68.689751\n",
      " 523120/700000: episode: 1497, duration: 2.379s, episode steps: 459, steps per second: 193, episode reward: 209.195, mean reward: 0.456 [-17.796, 100.000], mean action: 0.719 [0.000, 3.000], mean observation: 0.174 [-1.037, 1.000], loss: 7.162124, mean_absolute_error: 51.645386, mean_q: 69.158829\n",
      " 523540/700000: episode: 1498, duration: 2.229s, episode steps: 420, steps per second: 188, episode reward: 157.300, mean reward: 0.375 [-22.349, 100.000], mean action: 1.955 [0.000, 3.000], mean observation: 0.178 [-0.842, 1.000], loss: 10.279283, mean_absolute_error: 52.016296, mean_q: 69.520874\n",
      " 523679/700000: episode: 1499, duration: 0.702s, episode steps: 139, steps per second: 198, episode reward: -23.218, mean reward: -0.167 [-100.000, 35.566], mean action: 1.856 [0.000, 3.000], mean observation: 0.096 [-1.196, 1.000], loss: 10.374572, mean_absolute_error: 51.480621, mean_q: 68.849876\n",
      " 523784/700000: episode: 1500, duration: 0.527s, episode steps: 105, steps per second: 199, episode reward: -24.774, mean reward: -0.236 [-100.000, 15.491], mean action: 1.610 [0.000, 3.000], mean observation: 0.023 [-1.370, 1.000], loss: 15.586119, mean_absolute_error: 52.337261, mean_q: 70.048347\n",
      " 523987/700000: episode: 1501, duration: 1.012s, episode steps: 203, steps per second: 201, episode reward: 222.681, mean reward: 1.097 [-20.742, 100.000], mean action: 1.084 [0.000, 3.000], mean observation: 0.119 [-1.117, 1.000], loss: 7.482328, mean_absolute_error: 51.790688, mean_q: 69.223846\n",
      " 524180/700000: episode: 1502, duration: 0.963s, episode steps: 193, steps per second: 200, episode reward: 231.134, mean reward: 1.198 [-3.825, 100.000], mean action: 1.363 [0.000, 3.000], mean observation: 0.076 [-1.202, 1.000], loss: 13.792593, mean_absolute_error: 51.887753, mean_q: 69.160027\n",
      " 524449/700000: episode: 1503, duration: 1.371s, episode steps: 269, steps per second: 196, episode reward: 250.297, mean reward: 0.930 [-2.999, 100.000], mean action: 1.249 [0.000, 3.000], mean observation: 0.140 [-0.801, 1.000], loss: 8.472869, mean_absolute_error: 51.555836, mean_q: 69.056778\n",
      " 525449/700000: episode: 1504, duration: 5.546s, episode steps: 1000, steps per second: 180, episode reward: 101.477, mean reward: 0.101 [-18.847, 22.719], mean action: 1.057 [0.000, 3.000], mean observation: 0.230 [-0.795, 1.000], loss: 10.152632, mean_absolute_error: 51.704487, mean_q: 69.023857\n",
      " 525649/700000: episode: 1505, duration: 0.989s, episode steps: 200, steps per second: 202, episode reward: 240.790, mean reward: 1.204 [-3.077, 100.000], mean action: 1.005 [0.000, 3.000], mean observation: 0.089 [-1.044, 1.009], loss: 13.838576, mean_absolute_error: 51.995743, mean_q: 69.338562\n",
      " 525863/700000: episode: 1506, duration: 1.072s, episode steps: 214, steps per second: 200, episode reward: 239.216, mean reward: 1.118 [-7.363, 100.000], mean action: 1.481 [0.000, 3.000], mean observation: 0.099 [-1.364, 1.000], loss: 9.303036, mean_absolute_error: 51.609501, mean_q: 69.029297\n",
      " 526091/700000: episode: 1507, duration: 1.138s, episode steps: 228, steps per second: 200, episode reward: 237.988, mean reward: 1.044 [-8.315, 100.000], mean action: 1.154 [0.000, 3.000], mean observation: 0.031 [-1.208, 1.012], loss: 9.134275, mean_absolute_error: 52.323620, mean_q: 70.016754\n",
      " 526310/700000: episode: 1508, duration: 1.091s, episode steps: 219, steps per second: 201, episode reward: 217.657, mean reward: 0.994 [-18.356, 100.000], mean action: 1.164 [0.000, 3.000], mean observation: 0.101 [-1.037, 1.000], loss: 8.523333, mean_absolute_error: 52.137985, mean_q: 69.645332\n",
      " 526528/700000: episode: 1509, duration: 1.109s, episode steps: 218, steps per second: 197, episode reward: 226.734, mean reward: 1.040 [-10.316, 100.000], mean action: 1.115 [0.000, 3.000], mean observation: 0.084 [-1.048, 1.000], loss: 7.893766, mean_absolute_error: 52.715843, mean_q: 70.616928\n",
      " 526657/700000: episode: 1510, duration: 0.644s, episode steps: 129, steps per second: 200, episode reward: 2.677, mean reward: 0.021 [-100.000, 10.817], mean action: 1.581 [0.000, 3.000], mean observation: -0.043 [-0.824, 1.000], loss: 12.553861, mean_absolute_error: 51.853813, mean_q: 69.280266\n",
      " 527265/700000: episode: 1511, duration: 3.235s, episode steps: 608, steps per second: 188, episode reward: 197.227, mean reward: 0.324 [-19.895, 100.000], mean action: 1.798 [0.000, 3.000], mean observation: 0.122 [-0.970, 1.000], loss: 9.115016, mean_absolute_error: 52.297436, mean_q: 69.640808\n",
      " 527491/700000: episode: 1512, duration: 1.134s, episode steps: 226, steps per second: 199, episode reward: 218.889, mean reward: 0.969 [-4.510, 100.000], mean action: 0.925 [0.000, 3.000], mean observation: 0.156 [-1.023, 1.000], loss: 6.480102, mean_absolute_error: 52.857754, mean_q: 70.259758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 527921/700000: episode: 1513, duration: 2.264s, episode steps: 430, steps per second: 190, episode reward: 230.104, mean reward: 0.535 [-24.340, 100.000], mean action: 1.021 [0.000, 3.000], mean observation: 0.114 [-1.026, 1.000], loss: 8.473076, mean_absolute_error: 53.017689, mean_q: 70.654800\n",
      " 528244/700000: episode: 1514, duration: 1.689s, episode steps: 323, steps per second: 191, episode reward: 245.165, mean reward: 0.759 [-8.739, 100.000], mean action: 1.077 [0.000, 3.000], mean observation: 0.125 [-0.878, 1.000], loss: 12.692527, mean_absolute_error: 52.457272, mean_q: 69.836014\n",
      " 528561/700000: episode: 1515, duration: 1.606s, episode steps: 317, steps per second: 197, episode reward: 209.894, mean reward: 0.662 [-18.478, 100.000], mean action: 0.801 [0.000, 3.000], mean observation: 0.147 [-1.020, 1.000], loss: 9.813458, mean_absolute_error: 52.606258, mean_q: 70.287903\n",
      " 528894/700000: episode: 1516, duration: 1.670s, episode steps: 333, steps per second: 199, episode reward: 254.138, mean reward: 0.763 [-17.451, 100.000], mean action: 0.718 [0.000, 3.000], mean observation: 0.198 [-1.344, 1.000], loss: 12.525344, mean_absolute_error: 53.022358, mean_q: 70.826721\n",
      " 528998/700000: episode: 1517, duration: 0.613s, episode steps: 104, steps per second: 170, episode reward: -43.839, mean reward: -0.422 [-100.000, 17.871], mean action: 1.606 [0.000, 3.000], mean observation: -0.111 [-0.918, 1.102], loss: 8.468619, mean_absolute_error: 52.997356, mean_q: 70.925461\n",
      " 529111/700000: episode: 1518, duration: 0.565s, episode steps: 113, steps per second: 200, episode reward: -52.999, mean reward: -0.469 [-100.000, 18.456], mean action: 1.850 [0.000, 3.000], mean observation: 0.098 [-1.408, 1.000], loss: 11.140929, mean_absolute_error: 52.461014, mean_q: 69.761391\n",
      " 529387/700000: episode: 1519, duration: 1.597s, episode steps: 276, steps per second: 173, episode reward: 222.549, mean reward: 0.806 [-12.502, 100.000], mean action: 0.830 [0.000, 3.000], mean observation: 0.085 [-0.932, 1.000], loss: 8.293883, mean_absolute_error: 53.051056, mean_q: 70.806396\n",
      " 529727/700000: episode: 1520, duration: 1.772s, episode steps: 340, steps per second: 192, episode reward: 228.580, mean reward: 0.672 [-17.701, 100.000], mean action: 1.026 [0.000, 3.000], mean observation: 0.086 [-0.976, 1.936], loss: 13.891903, mean_absolute_error: 53.106308, mean_q: 70.812019\n",
      " 530204/700000: episode: 1521, duration: 2.592s, episode steps: 477, steps per second: 184, episode reward: 223.491, mean reward: 0.469 [-18.150, 100.000], mean action: 0.824 [0.000, 3.000], mean observation: 0.157 [-0.686, 1.000], loss: 11.183260, mean_absolute_error: 52.757328, mean_q: 70.439125\n",
      " 530415/700000: episode: 1522, duration: 1.346s, episode steps: 211, steps per second: 157, episode reward: 252.114, mean reward: 1.195 [-17.914, 100.000], mean action: 1.270 [0.000, 3.000], mean observation: 0.115 [-0.944, 1.000], loss: 8.778820, mean_absolute_error: 52.366020, mean_q: 70.174973\n",
      " 531415/700000: episode: 1523, duration: 5.639s, episode steps: 1000, steps per second: 177, episode reward: 85.597, mean reward: 0.086 [-18.375, 21.430], mean action: 2.128 [0.000, 3.000], mean observation: 0.186 [-1.004, 1.000], loss: 8.159748, mean_absolute_error: 52.833485, mean_q: 70.392113\n",
      " 532010/700000: episode: 1524, duration: 3.103s, episode steps: 595, steps per second: 192, episode reward: 233.406, mean reward: 0.392 [-18.530, 100.000], mean action: 0.583 [0.000, 3.000], mean observation: 0.193 [-1.397, 1.000], loss: 9.575293, mean_absolute_error: 52.653088, mean_q: 70.479698\n",
      " 532317/700000: episode: 1525, duration: 1.550s, episode steps: 307, steps per second: 198, episode reward: 239.377, mean reward: 0.780 [-9.245, 100.000], mean action: 0.915 [0.000, 3.000], mean observation: 0.094 [-0.842, 1.000], loss: 14.824957, mean_absolute_error: 52.499378, mean_q: 70.223740\n",
      " 532639/700000: episode: 1526, duration: 1.649s, episode steps: 322, steps per second: 195, episode reward: 228.028, mean reward: 0.708 [-17.695, 100.000], mean action: 0.981 [0.000, 3.000], mean observation: 0.127 [-0.948, 1.000], loss: 10.784425, mean_absolute_error: 52.850082, mean_q: 70.707932\n",
      " 532745/700000: episode: 1527, duration: 0.535s, episode steps: 106, steps per second: 198, episode reward: -37.964, mean reward: -0.358 [-100.000, 18.206], mean action: 1.802 [0.000, 3.000], mean observation: 0.062 [-1.472, 1.000], loss: 12.141334, mean_absolute_error: 52.668674, mean_q: 70.579399\n",
      " 533087/700000: episode: 1528, duration: 1.747s, episode steps: 342, steps per second: 196, episode reward: 228.377, mean reward: 0.668 [-11.149, 100.000], mean action: 0.784 [0.000, 3.000], mean observation: 0.123 [-0.836, 1.000], loss: 9.854931, mean_absolute_error: 52.328377, mean_q: 70.151466\n",
      " 533400/700000: episode: 1529, duration: 1.602s, episode steps: 313, steps per second: 195, episode reward: 247.848, mean reward: 0.792 [-16.080, 100.000], mean action: 1.460 [0.000, 3.000], mean observation: 0.123 [-0.833, 1.138], loss: 10.486762, mean_absolute_error: 51.983574, mean_q: 69.477974\n",
      " 533638/700000: episode: 1530, duration: 1.202s, episode steps: 238, steps per second: 198, episode reward: 226.736, mean reward: 0.953 [-9.197, 100.000], mean action: 1.231 [0.000, 3.000], mean observation: 0.104 [-0.752, 1.616], loss: 8.149235, mean_absolute_error: 52.384464, mean_q: 70.069183\n",
      " 533764/700000: episode: 1531, duration: 0.625s, episode steps: 126, steps per second: 202, episode reward: -63.359, mean reward: -0.503 [-100.000, 15.171], mean action: 1.516 [0.000, 3.000], mean observation: -0.043 [-1.501, 1.000], loss: 15.257184, mean_absolute_error: 52.473347, mean_q: 69.871155\n",
      " 533979/700000: episode: 1532, duration: 1.089s, episode steps: 215, steps per second: 197, episode reward: 269.069, mean reward: 1.251 [-11.257, 100.000], mean action: 1.019 [0.000, 3.000], mean observation: 0.104 [-0.932, 1.000], loss: 9.593641, mean_absolute_error: 52.229660, mean_q: 69.806107\n",
      " 534275/700000: episode: 1533, duration: 1.523s, episode steps: 296, steps per second: 194, episode reward: 216.697, mean reward: 0.732 [-17.533, 100.000], mean action: 1.101 [0.000, 3.000], mean observation: 0.142 [-0.907, 1.000], loss: 8.603997, mean_absolute_error: 52.167137, mean_q: 69.804214\n",
      " 534496/700000: episode: 1534, duration: 1.117s, episode steps: 221, steps per second: 198, episode reward: 231.861, mean reward: 1.049 [-8.834, 100.000], mean action: 1.154 [0.000, 3.000], mean observation: 0.075 [-0.846, 1.000], loss: 9.523788, mean_absolute_error: 52.401386, mean_q: 70.347954\n",
      " 534741/700000: episode: 1535, duration: 1.333s, episode steps: 245, steps per second: 184, episode reward: 228.668, mean reward: 0.933 [-10.538, 100.000], mean action: 1.220 [0.000, 3.000], mean observation: 0.114 [-0.905, 1.000], loss: 6.507518, mean_absolute_error: 52.263050, mean_q: 69.796471\n",
      " 534970/700000: episode: 1536, duration: 1.911s, episode steps: 229, steps per second: 120, episode reward: 229.536, mean reward: 1.002 [-9.394, 100.000], mean action: 1.013 [0.000, 3.000], mean observation: 0.128 [-0.940, 1.000], loss: 13.316317, mean_absolute_error: 52.214451, mean_q: 70.036896\n",
      " 535499/700000: episode: 1537, duration: 4.077s, episode steps: 529, steps per second: 130, episode reward: 196.065, mean reward: 0.371 [-20.708, 100.000], mean action: 0.983 [0.000, 3.000], mean observation: 0.180 [-1.021, 1.000], loss: 8.241970, mean_absolute_error: 52.303955, mean_q: 69.918922\n",
      " 535715/700000: episode: 1538, duration: 1.423s, episode steps: 216, steps per second: 152, episode reward: 237.592, mean reward: 1.100 [-5.494, 100.000], mean action: 1.157 [0.000, 3.000], mean observation: 0.098 [-0.989, 1.003], loss: 12.718904, mean_absolute_error: 52.683018, mean_q: 70.260033\n",
      " 536441/700000: episode: 1539, duration: 4.566s, episode steps: 726, steps per second: 159, episode reward: 229.044, mean reward: 0.315 [-17.487, 100.000], mean action: 2.004 [0.000, 3.000], mean observation: 0.172 [-0.822, 1.000], loss: 8.309540, mean_absolute_error: 52.284119, mean_q: 69.877106\n",
      " 536734/700000: episode: 1540, duration: 1.483s, episode steps: 293, steps per second: 198, episode reward: 230.681, mean reward: 0.787 [-24.228, 100.000], mean action: 1.041 [0.000, 3.000], mean observation: 0.127 [-0.890, 1.000], loss: 8.613377, mean_absolute_error: 52.222572, mean_q: 69.918068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 537088/700000: episode: 1541, duration: 1.843s, episode steps: 354, steps per second: 192, episode reward: 228.274, mean reward: 0.645 [-11.147, 100.000], mean action: 1.172 [0.000, 3.000], mean observation: 0.116 [-0.914, 1.000], loss: 9.348117, mean_absolute_error: 52.076778, mean_q: 69.720055\n",
      " 537381/700000: episode: 1542, duration: 1.490s, episode steps: 293, steps per second: 197, episode reward: 222.733, mean reward: 0.760 [-17.607, 100.000], mean action: 1.171 [0.000, 3.000], mean observation: 0.121 [-0.916, 1.000], loss: 9.169442, mean_absolute_error: 52.560524, mean_q: 70.275520\n",
      " 537694/700000: episode: 1543, duration: 1.592s, episode steps: 313, steps per second: 197, episode reward: 258.446, mean reward: 0.826 [-4.898, 100.000], mean action: 1.188 [0.000, 3.000], mean observation: 0.088 [-0.759, 1.000], loss: 8.374727, mean_absolute_error: 52.628384, mean_q: 70.098457\n",
      " 538140/700000: episode: 1544, duration: 2.399s, episode steps: 446, steps per second: 186, episode reward: 178.709, mean reward: 0.401 [-19.373, 100.000], mean action: 0.998 [0.000, 3.000], mean observation: 0.188 [-0.982, 1.000], loss: 11.880472, mean_absolute_error: 52.614052, mean_q: 70.207184\n",
      " 538325/700000: episode: 1545, duration: 0.999s, episode steps: 185, steps per second: 185, episode reward: 237.601, mean reward: 1.284 [-8.720, 100.000], mean action: 1.265 [0.000, 3.000], mean observation: 0.081 [-0.733, 1.000], loss: 6.827381, mean_absolute_error: 52.037586, mean_q: 69.697762\n",
      " 538526/700000: episode: 1546, duration: 1.228s, episode steps: 201, steps per second: 164, episode reward: 238.821, mean reward: 1.188 [-11.436, 100.000], mean action: 1.348 [0.000, 3.000], mean observation: 0.108 [-0.870, 1.000], loss: 12.015918, mean_absolute_error: 52.412891, mean_q: 70.278900\n",
      " 538798/700000: episode: 1547, duration: 2.451s, episode steps: 272, steps per second: 111, episode reward: 197.961, mean reward: 0.728 [-9.091, 100.000], mean action: 0.934 [0.000, 3.000], mean observation: 0.095 [-0.848, 1.000], loss: 10.393850, mean_absolute_error: 52.846016, mean_q: 70.470474\n",
      " 539152/700000: episode: 1548, duration: 2.150s, episode steps: 354, steps per second: 165, episode reward: 228.914, mean reward: 0.647 [-17.259, 100.000], mean action: 1.661 [0.000, 3.000], mean observation: 0.137 [-1.301, 1.000], loss: 12.790052, mean_absolute_error: 52.744423, mean_q: 70.637733\n",
      " 539291/700000: episode: 1549, duration: 0.705s, episode steps: 139, steps per second: 197, episode reward: 2.860, mean reward: 0.021 [-100.000, 13.424], mean action: 1.820 [0.000, 3.000], mean observation: 0.092 [-0.859, 1.267], loss: 6.433621, mean_absolute_error: 53.075897, mean_q: 70.728462\n",
      " 539523/700000: episode: 1550, duration: 1.178s, episode steps: 232, steps per second: 197, episode reward: 261.518, mean reward: 1.127 [-18.277, 100.000], mean action: 1.250 [0.000, 3.000], mean observation: 0.048 [-1.004, 1.000], loss: 10.144111, mean_absolute_error: 53.521492, mean_q: 71.276276\n",
      " 539756/700000: episode: 1551, duration: 1.185s, episode steps: 233, steps per second: 197, episode reward: 228.575, mean reward: 0.981 [-8.695, 100.000], mean action: 0.953 [0.000, 3.000], mean observation: 0.090 [-0.878, 1.000], loss: 9.102914, mean_absolute_error: 52.669403, mean_q: 70.217216\n",
      " 539949/700000: episode: 1552, duration: 0.987s, episode steps: 193, steps per second: 195, episode reward: 204.422, mean reward: 1.059 [-19.154, 100.000], mean action: 1.389 [0.000, 3.000], mean observation: 0.076 [-1.061, 1.000], loss: 6.839929, mean_absolute_error: 52.938797, mean_q: 70.739433\n",
      " 540132/700000: episode: 1553, duration: 0.933s, episode steps: 183, steps per second: 196, episode reward: 8.153, mean reward: 0.045 [-100.000, 16.445], mean action: 1.820 [0.000, 3.000], mean observation: 0.093 [-1.170, 1.000], loss: 8.295290, mean_absolute_error: 53.185680, mean_q: 71.131615\n",
      " 540359/700000: episode: 1554, duration: 1.137s, episode steps: 227, steps per second: 200, episode reward: 242.717, mean reward: 1.069 [-5.831, 100.000], mean action: 1.123 [0.000, 3.000], mean observation: 0.056 [-1.151, 1.000], loss: 8.373033, mean_absolute_error: 52.403629, mean_q: 69.986595\n",
      " 540973/700000: episode: 1555, duration: 3.298s, episode steps: 614, steps per second: 186, episode reward: 234.601, mean reward: 0.382 [-23.764, 100.000], mean action: 1.016 [0.000, 3.000], mean observation: 0.226 [-0.789, 1.000], loss: 8.610448, mean_absolute_error: 52.753380, mean_q: 70.428314\n",
      " 541139/700000: episode: 1556, duration: 0.861s, episode steps: 166, steps per second: 193, episode reward: 217.725, mean reward: 1.312 [-4.140, 100.000], mean action: 1.488 [0.000, 3.000], mean observation: 0.122 [-1.003, 1.000], loss: 7.401920, mean_absolute_error: 53.241035, mean_q: 71.065804\n",
      " 541327/700000: episode: 1557, duration: 0.940s, episode steps: 188, steps per second: 200, episode reward: 242.299, mean reward: 1.289 [-6.929, 100.000], mean action: 0.957 [0.000, 3.000], mean observation: 0.133 [-1.127, 1.000], loss: 7.879601, mean_absolute_error: 53.187382, mean_q: 71.333443\n",
      " 541550/700000: episode: 1558, duration: 1.146s, episode steps: 223, steps per second: 195, episode reward: 264.897, mean reward: 1.188 [-9.179, 100.000], mean action: 1.188 [0.000, 3.000], mean observation: 0.179 [-0.865, 1.000], loss: 6.029457, mean_absolute_error: 53.426079, mean_q: 71.424171\n",
      " 541817/700000: episode: 1559, duration: 1.390s, episode steps: 267, steps per second: 192, episode reward: 274.363, mean reward: 1.028 [-10.899, 100.000], mean action: 1.105 [0.000, 3.000], mean observation: 0.094 [-1.227, 1.000], loss: 16.476320, mean_absolute_error: 53.497498, mean_q: 71.468842\n",
      " 542089/700000: episode: 1560, duration: 1.399s, episode steps: 272, steps per second: 194, episode reward: 258.601, mean reward: 0.951 [-18.107, 100.000], mean action: 0.971 [0.000, 3.000], mean observation: 0.042 [-0.963, 1.000], loss: 7.120161, mean_absolute_error: 53.081120, mean_q: 71.001122\n",
      " 542330/700000: episode: 1561, duration: 1.214s, episode steps: 241, steps per second: 198, episode reward: 237.219, mean reward: 0.984 [-4.211, 100.000], mean action: 0.996 [0.000, 3.000], mean observation: 0.136 [-1.201, 1.000], loss: 12.740889, mean_absolute_error: 53.105629, mean_q: 71.084106\n",
      " 542463/700000: episode: 1562, duration: 0.667s, episode steps: 133, steps per second: 199, episode reward: -51.959, mean reward: -0.391 [-100.000, 18.031], mean action: 1.609 [0.000, 3.000], mean observation: -0.067 [-0.965, 1.774], loss: 9.357102, mean_absolute_error: 52.849087, mean_q: 70.827248\n",
      " 542711/700000: episode: 1563, duration: 1.280s, episode steps: 248, steps per second: 194, episode reward: 248.557, mean reward: 1.002 [-18.481, 100.000], mean action: 1.206 [0.000, 3.000], mean observation: 0.063 [-0.960, 1.041], loss: 7.012239, mean_absolute_error: 53.322113, mean_q: 71.390442\n",
      " 543096/700000: episode: 1564, duration: 1.959s, episode steps: 385, steps per second: 197, episode reward: 245.859, mean reward: 0.639 [-17.773, 100.000], mean action: 0.847 [0.000, 3.000], mean observation: 0.121 [-1.171, 1.000], loss: 7.778718, mean_absolute_error: 53.194118, mean_q: 71.117874\n",
      " 543418/700000: episode: 1565, duration: 1.613s, episode steps: 322, steps per second: 200, episode reward: 222.514, mean reward: 0.691 [-17.688, 100.000], mean action: 0.755 [0.000, 3.000], mean observation: 0.143 [-0.943, 1.000], loss: 8.900044, mean_absolute_error: 52.790409, mean_q: 70.692001\n",
      " 543721/700000: episode: 1566, duration: 1.523s, episode steps: 303, steps per second: 199, episode reward: 214.641, mean reward: 0.708 [-12.202, 100.000], mean action: 0.967 [0.000, 3.000], mean observation: 0.131 [-0.973, 1.000], loss: 8.776947, mean_absolute_error: 53.033310, mean_q: 70.859604\n",
      " 543890/700000: episode: 1567, duration: 0.854s, episode steps: 169, steps per second: 198, episode reward: 194.420, mean reward: 1.150 [-3.454, 100.000], mean action: 1.107 [0.000, 3.000], mean observation: 0.140 [-1.040, 1.298], loss: 8.547865, mean_absolute_error: 53.016342, mean_q: 70.706360\n",
      " 544215/700000: episode: 1568, duration: 1.672s, episode steps: 325, steps per second: 194, episode reward: 202.774, mean reward: 0.624 [-18.266, 100.000], mean action: 0.723 [0.000, 3.000], mean observation: 0.116 [-1.047, 1.000], loss: 10.003035, mean_absolute_error: 52.401588, mean_q: 70.026268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 544498/700000: episode: 1569, duration: 1.439s, episode steps: 283, steps per second: 197, episode reward: 246.352, mean reward: 0.871 [-21.618, 100.000], mean action: 1.035 [0.000, 3.000], mean observation: 0.127 [-0.841, 1.000], loss: 7.922506, mean_absolute_error: 52.659672, mean_q: 70.563217\n",
      " 544727/700000: episode: 1570, duration: 1.161s, episode steps: 229, steps per second: 197, episode reward: 197.798, mean reward: 0.864 [-20.065, 100.000], mean action: 1.279 [0.000, 3.000], mean observation: 0.091 [-0.953, 1.000], loss: 8.566310, mean_absolute_error: 52.848400, mean_q: 70.828674\n",
      " 545014/700000: episode: 1571, duration: 1.471s, episode steps: 287, steps per second: 195, episode reward: 242.943, mean reward: 0.846 [-4.202, 100.000], mean action: 1.098 [0.000, 3.000], mean observation: 0.162 [-0.891, 1.092], loss: 4.787218, mean_absolute_error: 52.473881, mean_q: 70.252968\n",
      " 546014/700000: episode: 1572, duration: 5.895s, episode steps: 1000, steps per second: 170, episode reward: 102.046, mean reward: 0.102 [-20.101, 16.650], mean action: 2.159 [0.000, 3.000], mean observation: 0.145 [-0.730, 1.000], loss: 7.644205, mean_absolute_error: 52.648613, mean_q: 70.451561\n",
      " 546325/700000: episode: 1573, duration: 1.611s, episode steps: 311, steps per second: 193, episode reward: 189.736, mean reward: 0.610 [-19.912, 100.000], mean action: 1.299 [0.000, 3.000], mean observation: 0.104 [-0.894, 1.000], loss: 10.238109, mean_absolute_error: 52.700977, mean_q: 70.507149\n",
      " 546533/700000: episode: 1574, duration: 1.050s, episode steps: 208, steps per second: 198, episode reward: 200.718, mean reward: 0.965 [-5.955, 100.000], mean action: 1.255 [0.000, 3.000], mean observation: 0.086 [-0.991, 1.000], loss: 7.298198, mean_absolute_error: 52.627686, mean_q: 70.449501\n",
      " 546759/700000: episode: 1575, duration: 1.138s, episode steps: 226, steps per second: 199, episode reward: 221.703, mean reward: 0.981 [-8.868, 100.000], mean action: 1.354 [0.000, 3.000], mean observation: 0.062 [-0.940, 1.000], loss: 10.094271, mean_absolute_error: 52.061211, mean_q: 69.986336\n",
      " 547274/700000: episode: 1576, duration: 2.782s, episode steps: 515, steps per second: 185, episode reward: 148.431, mean reward: 0.288 [-19.860, 100.000], mean action: 1.357 [0.000, 3.000], mean observation: 0.146 [-0.729, 1.000], loss: 7.885176, mean_absolute_error: 52.393589, mean_q: 70.246284\n",
      " 547400/700000: episode: 1577, duration: 0.632s, episode steps: 126, steps per second: 199, episode reward: -200.630, mean reward: -1.592 [-100.000, 8.097], mean action: 1.802 [0.000, 3.000], mean observation: -0.125 [-3.357, 1.000], loss: 7.694797, mean_absolute_error: 52.446346, mean_q: 70.445320\n",
      " 547624/700000: episode: 1578, duration: 1.150s, episode steps: 224, steps per second: 195, episode reward: 277.532, mean reward: 1.239 [-8.434, 100.000], mean action: 1.438 [0.000, 3.000], mean observation: 0.091 [-0.909, 1.000], loss: 5.927231, mean_absolute_error: 52.753883, mean_q: 70.620186\n",
      " 547842/700000: episode: 1579, duration: 1.110s, episode steps: 218, steps per second: 196, episode reward: 221.287, mean reward: 1.015 [-18.256, 100.000], mean action: 1.248 [0.000, 3.000], mean observation: 0.103 [-0.854, 1.000], loss: 12.077330, mean_absolute_error: 52.051910, mean_q: 69.602837\n",
      " 548058/700000: episode: 1580, duration: 1.100s, episode steps: 216, steps per second: 196, episode reward: 218.817, mean reward: 1.013 [-11.091, 100.000], mean action: 1.468 [0.000, 3.000], mean observation: 0.096 [-1.007, 1.000], loss: 10.025380, mean_absolute_error: 52.327175, mean_q: 70.145935\n",
      " 548448/700000: episode: 1581, duration: 2.012s, episode steps: 390, steps per second: 194, episode reward: 195.409, mean reward: 0.501 [-18.717, 100.000], mean action: 0.900 [0.000, 3.000], mean observation: 0.109 [-1.066, 1.000], loss: 16.783035, mean_absolute_error: 52.021526, mean_q: 69.505608\n",
      " 548775/700000: episode: 1582, duration: 1.694s, episode steps: 327, steps per second: 193, episode reward: 214.071, mean reward: 0.655 [-11.028, 100.000], mean action: 1.089 [0.000, 3.000], mean observation: 0.078 [-0.608, 1.000], loss: 6.780305, mean_absolute_error: 51.860222, mean_q: 69.294212\n",
      " 549107/700000: episode: 1583, duration: 1.736s, episode steps: 332, steps per second: 191, episode reward: 168.963, mean reward: 0.509 [-16.994, 100.000], mean action: 1.515 [0.000, 3.000], mean observation: 0.091 [-0.635, 1.315], loss: 8.866053, mean_absolute_error: 51.717091, mean_q: 69.076027\n",
      " 549235/700000: episode: 1584, duration: 0.639s, episode steps: 128, steps per second: 200, episode reward: -175.482, mean reward: -1.371 [-100.000, 15.131], mean action: 1.555 [0.000, 3.000], mean observation: 0.149 [-1.236, 3.176], loss: 7.886319, mean_absolute_error: 52.146618, mean_q: 69.701187\n",
      " 549537/700000: episode: 1585, duration: 1.545s, episode steps: 302, steps per second: 195, episode reward: 231.146, mean reward: 0.765 [-17.508, 100.000], mean action: 1.056 [0.000, 3.000], mean observation: 0.112 [-1.000, 1.000], loss: 10.277667, mean_absolute_error: 51.576862, mean_q: 69.029297\n",
      " 549899/700000: episode: 1586, duration: 1.895s, episode steps: 362, steps per second: 191, episode reward: 199.306, mean reward: 0.551 [-18.121, 100.000], mean action: 0.859 [0.000, 3.000], mean observation: 0.139 [-1.017, 1.000], loss: 7.134284, mean_absolute_error: 51.650166, mean_q: 69.214279\n",
      " 550204/700000: episode: 1587, duration: 1.584s, episode steps: 305, steps per second: 193, episode reward: 187.661, mean reward: 0.615 [-9.523, 100.000], mean action: 1.036 [0.000, 3.000], mean observation: 0.065 [-1.004, 1.000], loss: 12.389771, mean_absolute_error: 51.551479, mean_q: 68.765701\n",
      " 550451/700000: episode: 1588, duration: 1.237s, episode steps: 247, steps per second: 200, episode reward: 219.952, mean reward: 0.890 [-11.500, 100.000], mean action: 0.947 [0.000, 3.000], mean observation: 0.112 [-1.081, 1.000], loss: 7.622805, mean_absolute_error: 51.991798, mean_q: 69.577461\n",
      " 550847/700000: episode: 1589, duration: 2.014s, episode steps: 396, steps per second: 197, episode reward: 254.084, mean reward: 0.642 [-17.365, 100.000], mean action: 1.172 [0.000, 3.000], mean observation: 0.149 [-0.893, 1.000], loss: 6.962846, mean_absolute_error: 51.762074, mean_q: 69.275291\n",
      " 551047/700000: episode: 1590, duration: 1.089s, episode steps: 200, steps per second: 184, episode reward: 249.700, mean reward: 1.249 [-6.004, 100.000], mean action: 1.425 [0.000, 3.000], mean observation: 0.039 [-0.846, 1.000], loss: 6.923815, mean_absolute_error: 51.638599, mean_q: 69.060280\n",
      " 551308/700000: episode: 1591, duration: 1.518s, episode steps: 261, steps per second: 172, episode reward: 202.975, mean reward: 0.778 [-10.431, 100.000], mean action: 1.490 [0.000, 3.000], mean observation: 0.148 [-0.784, 1.299], loss: 8.547812, mean_absolute_error: 51.837593, mean_q: 69.475258\n",
      " 552138/700000: episode: 1592, duration: 4.785s, episode steps: 830, steps per second: 173, episode reward: 203.414, mean reward: 0.245 [-19.032, 100.000], mean action: 1.135 [0.000, 3.000], mean observation: 0.212 [-0.747, 1.342], loss: 7.077996, mean_absolute_error: 51.962818, mean_q: 69.686989\n",
      " 552309/700000: episode: 1593, duration: 0.964s, episode steps: 171, steps per second: 177, episode reward: 232.949, mean reward: 1.362 [-9.012, 100.000], mean action: 1.503 [0.000, 3.000], mean observation: 0.017 [-0.963, 1.000], loss: 6.975222, mean_absolute_error: 51.626999, mean_q: 69.236725\n",
      " 552479/700000: episode: 1594, duration: 0.888s, episode steps: 170, steps per second: 191, episode reward: 248.526, mean reward: 1.462 [-11.103, 100.000], mean action: 1.329 [0.000, 3.000], mean observation: 0.032 [-0.859, 1.000], loss: 6.909050, mean_absolute_error: 52.313511, mean_q: 70.287308\n",
      " 552696/700000: episode: 1595, duration: 1.309s, episode steps: 217, steps per second: 166, episode reward: 205.652, mean reward: 0.948 [-12.392, 100.000], mean action: 0.880 [0.000, 3.000], mean observation: 0.079 [-1.128, 1.000], loss: 6.356294, mean_absolute_error: 52.214954, mean_q: 69.859062\n",
      " 552896/700000: episode: 1596, duration: 1.013s, episode steps: 200, steps per second: 197, episode reward: 233.723, mean reward: 1.169 [-3.939, 100.000], mean action: 1.570 [0.000, 3.000], mean observation: 0.129 [-0.995, 1.000], loss: 8.954844, mean_absolute_error: 51.824520, mean_q: 69.147346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 553320/700000: episode: 1597, duration: 2.185s, episode steps: 424, steps per second: 194, episode reward: 249.165, mean reward: 0.588 [-19.674, 100.000], mean action: 1.651 [0.000, 3.000], mean observation: 0.132 [-1.033, 1.017], loss: 7.124447, mean_absolute_error: 52.166393, mean_q: 69.984344\n",
      " 553471/700000: episode: 1598, duration: 0.759s, episode steps: 151, steps per second: 199, episode reward: 50.491, mean reward: 0.334 [-100.000, 15.146], mean action: 1.682 [0.000, 3.000], mean observation: -0.026 [-0.813, 1.000], loss: 8.109415, mean_absolute_error: 52.738121, mean_q: 70.756813\n",
      " 553639/700000: episode: 1599, duration: 0.846s, episode steps: 168, steps per second: 199, episode reward: 201.473, mean reward: 1.199 [-9.449, 100.000], mean action: 1.554 [0.000, 3.000], mean observation: 0.072 [-0.992, 1.000], loss: 10.135277, mean_absolute_error: 53.070511, mean_q: 71.175804\n",
      " 553911/700000: episode: 1600, duration: 1.401s, episode steps: 272, steps per second: 194, episode reward: 261.179, mean reward: 0.960 [-17.579, 100.000], mean action: 1.202 [0.000, 3.000], mean observation: 0.087 [-1.056, 1.000], loss: 7.553680, mean_absolute_error: 52.742294, mean_q: 70.592712\n",
      " 554181/700000: episode: 1601, duration: 1.393s, episode steps: 270, steps per second: 194, episode reward: 240.408, mean reward: 0.890 [-19.704, 100.000], mean action: 1.204 [0.000, 3.000], mean observation: 0.134 [-1.076, 1.000], loss: 8.032338, mean_absolute_error: 52.646381, mean_q: 70.364067\n",
      " 554346/700000: episode: 1602, duration: 0.818s, episode steps: 165, steps per second: 202, episode reward: 223.446, mean reward: 1.354 [-3.539, 100.000], mean action: 1.212 [0.000, 3.000], mean observation: 0.078 [-1.002, 1.000], loss: 6.590014, mean_absolute_error: 52.200226, mean_q: 69.882248\n",
      " 554539/700000: episode: 1603, duration: 0.965s, episode steps: 193, steps per second: 200, episode reward: 240.550, mean reward: 1.246 [-3.862, 100.000], mean action: 1.207 [0.000, 3.000], mean observation: 0.075 [-1.011, 1.000], loss: 9.223271, mean_absolute_error: 52.716316, mean_q: 70.608559\n",
      " 554767/700000: episode: 1604, duration: 1.152s, episode steps: 228, steps per second: 198, episode reward: 208.446, mean reward: 0.914 [-10.163, 100.000], mean action: 0.961 [0.000, 3.000], mean observation: 0.184 [-1.064, 1.000], loss: 7.846964, mean_absolute_error: 52.617443, mean_q: 70.416023\n",
      " 555113/700000: episode: 1605, duration: 1.768s, episode steps: 346, steps per second: 196, episode reward: 247.550, mean reward: 0.715 [-14.800, 100.000], mean action: 1.098 [0.000, 3.000], mean observation: 0.070 [-1.051, 1.006], loss: 7.960379, mean_absolute_error: 52.748703, mean_q: 70.712738\n",
      " 555537/700000: episode: 1606, duration: 2.187s, episode steps: 424, steps per second: 194, episode reward: 238.158, mean reward: 0.562 [-19.779, 100.000], mean action: 0.854 [0.000, 3.000], mean observation: 0.199 [-0.966, 1.367], loss: 5.445862, mean_absolute_error: 52.832233, mean_q: 70.769890\n",
      " 555877/700000: episode: 1607, duration: 1.717s, episode steps: 340, steps per second: 198, episode reward: 249.591, mean reward: 0.734 [-20.103, 100.000], mean action: 0.853 [0.000, 3.000], mean observation: 0.083 [-0.946, 1.000], loss: 6.626565, mean_absolute_error: 52.566738, mean_q: 70.451180\n",
      " 555990/700000: episode: 1608, duration: 0.562s, episode steps: 113, steps per second: 201, episode reward: -33.733, mean reward: -0.299 [-100.000, 19.339], mean action: 1.478 [0.000, 3.000], mean observation: -0.020 [-0.945, 1.000], loss: 8.577087, mean_absolute_error: 51.789402, mean_q: 69.349602\n",
      " 556203/700000: episode: 1609, duration: 1.073s, episode steps: 213, steps per second: 199, episode reward: 199.121, mean reward: 0.935 [-7.033, 100.000], mean action: 1.347 [0.000, 3.000], mean observation: 0.048 [-0.678, 1.000], loss: 7.328557, mean_absolute_error: 52.944851, mean_q: 71.141525\n",
      " 556450/700000: episode: 1610, duration: 1.257s, episode steps: 247, steps per second: 197, episode reward: 187.060, mean reward: 0.757 [-10.920, 100.000], mean action: 1.300 [0.000, 3.000], mean observation: 0.091 [-1.022, 1.000], loss: 8.989869, mean_absolute_error: 52.907066, mean_q: 70.808151\n",
      " 556563/700000: episode: 1611, duration: 0.566s, episode steps: 113, steps per second: 200, episode reward: -15.196, mean reward: -0.134 [-100.000, 16.941], mean action: 1.646 [0.000, 3.000], mean observation: -0.076 [-1.637, 1.000], loss: 9.023017, mean_absolute_error: 52.225090, mean_q: 69.998726\n",
      " 556719/700000: episode: 1612, duration: 0.785s, episode steps: 156, steps per second: 199, episode reward: -29.388, mean reward: -0.188 [-100.000, 20.490], mean action: 1.679 [0.000, 3.000], mean observation: 0.014 [-1.089, 1.077], loss: 11.579936, mean_absolute_error: 52.944862, mean_q: 71.024986\n",
      " 557254/700000: episode: 1613, duration: 2.933s, episode steps: 535, steps per second: 182, episode reward: 228.971, mean reward: 0.428 [-19.827, 100.000], mean action: 1.077 [0.000, 3.000], mean observation: 0.139 [-0.852, 1.000], loss: 6.031889, mean_absolute_error: 52.712776, mean_q: 70.473251\n",
      " 557493/700000: episode: 1614, duration: 1.200s, episode steps: 239, steps per second: 199, episode reward: 203.601, mean reward: 0.852 [-2.681, 100.000], mean action: 0.874 [0.000, 3.000], mean observation: 0.062 [-0.916, 1.000], loss: 8.176069, mean_absolute_error: 52.746349, mean_q: 70.469749\n",
      " 558196/700000: episode: 1615, duration: 3.798s, episode steps: 703, steps per second: 185, episode reward: 156.422, mean reward: 0.223 [-21.002, 100.000], mean action: 1.213 [0.000, 3.000], mean observation: 0.083 [-0.764, 1.000], loss: 7.139588, mean_absolute_error: 53.090050, mean_q: 70.997971\n",
      " 558602/700000: episode: 1616, duration: 2.109s, episode steps: 406, steps per second: 192, episode reward: 207.047, mean reward: 0.510 [-18.345, 100.000], mean action: 0.800 [0.000, 3.000], mean observation: 0.121 [-0.942, 1.000], loss: 8.986939, mean_absolute_error: 52.976360, mean_q: 70.866707\n",
      " 558705/700000: episode: 1617, duration: 0.572s, episode steps: 103, steps per second: 180, episode reward: 2.210, mean reward: 0.021 [-100.000, 19.203], mean action: 1.398 [0.000, 3.000], mean observation: 0.043 [-1.045, 1.000], loss: 5.535732, mean_absolute_error: 52.937302, mean_q: 70.862877\n",
      " 558878/700000: episode: 1618, duration: 0.865s, episode steps: 173, steps per second: 200, episode reward: 217.661, mean reward: 1.258 [-5.193, 100.000], mean action: 1.578 [0.000, 3.000], mean observation: 0.096 [-0.999, 1.000], loss: 6.943431, mean_absolute_error: 52.135239, mean_q: 69.832825\n",
      " 558975/700000: episode: 1619, duration: 0.490s, episode steps: 97, steps per second: 198, episode reward: -27.162, mean reward: -0.280 [-100.000, 24.763], mean action: 1.732 [0.000, 3.000], mean observation: -0.054 [-0.941, 1.000], loss: 7.318761, mean_absolute_error: 53.150417, mean_q: 70.975723\n",
      " 559303/700000: episode: 1620, duration: 1.704s, episode steps: 328, steps per second: 193, episode reward: 226.906, mean reward: 0.692 [-8.000, 100.000], mean action: 1.232 [0.000, 3.000], mean observation: 0.082 [-0.995, 1.000], loss: 11.057599, mean_absolute_error: 52.799835, mean_q: 70.522041\n",
      " 559577/700000: episode: 1621, duration: 1.381s, episode steps: 274, steps per second: 198, episode reward: 218.793, mean reward: 0.799 [-11.827, 100.000], mean action: 0.898 [0.000, 3.000], mean observation: 0.128 [-1.118, 1.000], loss: 11.393046, mean_absolute_error: 52.524391, mean_q: 70.204315\n",
      " 559911/700000: episode: 1622, duration: 1.742s, episode steps: 334, steps per second: 192, episode reward: 246.764, mean reward: 0.739 [-17.609, 100.000], mean action: 0.841 [0.000, 3.000], mean observation: 0.075 [-0.769, 1.000], loss: 6.193388, mean_absolute_error: 52.512703, mean_q: 70.068489\n",
      " 560247/700000: episode: 1623, duration: 1.743s, episode steps: 336, steps per second: 193, episode reward: 240.110, mean reward: 0.715 [-10.736, 100.000], mean action: 1.634 [0.000, 3.000], mean observation: 0.117 [-1.004, 1.015], loss: 9.374118, mean_absolute_error: 52.841610, mean_q: 70.435776\n",
      " 560612/700000: episode: 1624, duration: 1.906s, episode steps: 365, steps per second: 191, episode reward: 212.543, mean reward: 0.582 [-17.332, 100.000], mean action: 1.323 [0.000, 3.000], mean observation: 0.224 [-0.920, 1.000], loss: 8.458852, mean_absolute_error: 52.832909, mean_q: 70.692764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 560802/700000: episode: 1625, duration: 0.959s, episode steps: 190, steps per second: 198, episode reward: 227.174, mean reward: 1.196 [-3.782, 100.000], mean action: 1.253 [0.000, 3.000], mean observation: 0.145 [-0.804, 1.000], loss: 6.158496, mean_absolute_error: 52.725620, mean_q: 70.756569\n",
      " 560930/700000: episode: 1626, duration: 0.647s, episode steps: 128, steps per second: 198, episode reward: -7.186, mean reward: -0.056 [-100.000, 29.509], mean action: 1.391 [0.000, 3.000], mean observation: -0.021 [-0.856, 1.000], loss: 13.717489, mean_absolute_error: 52.589653, mean_q: 70.651260\n",
      " 561307/700000: episode: 1627, duration: 1.970s, episode steps: 377, steps per second: 191, episode reward: 193.239, mean reward: 0.513 [-17.723, 100.000], mean action: 1.220 [0.000, 3.000], mean observation: 0.103 [-0.581, 1.000], loss: 8.679715, mean_absolute_error: 52.948746, mean_q: 70.669991\n",
      " 561677/700000: episode: 1628, duration: 1.937s, episode steps: 370, steps per second: 191, episode reward: 194.085, mean reward: 0.525 [-19.375, 100.000], mean action: 0.984 [0.000, 3.000], mean observation: 0.122 [-0.720, 1.000], loss: 7.238158, mean_absolute_error: 52.830257, mean_q: 70.699966\n",
      " 561775/700000: episode: 1629, duration: 0.492s, episode steps: 98, steps per second: 199, episode reward: -131.658, mean reward: -1.343 [-100.000, 9.678], mean action: 1.296 [0.000, 3.000], mean observation: 0.018 [-2.655, 1.000], loss: 7.080405, mean_absolute_error: 52.716881, mean_q: 70.674469\n",
      " 561887/700000: episode: 1630, duration: 0.565s, episode steps: 112, steps per second: 198, episode reward: -13.647, mean reward: -0.122 [-100.000, 16.500], mean action: 1.732 [0.000, 3.000], mean observation: -0.052 [-1.189, 1.000], loss: 7.878006, mean_absolute_error: 52.562202, mean_q: 70.212990\n",
      " 561978/700000: episode: 1631, duration: 0.459s, episode steps: 91, steps per second: 198, episode reward: -15.018, mean reward: -0.165 [-100.000, 20.398], mean action: 1.747 [0.000, 3.000], mean observation: -0.031 [-1.511, 1.000], loss: 4.029553, mean_absolute_error: 52.433716, mean_q: 70.356941\n",
      " 562076/700000: episode: 1632, duration: 0.494s, episode steps: 98, steps per second: 198, episode reward: -26.624, mean reward: -0.272 [-100.000, 20.256], mean action: 1.857 [0.000, 3.000], mean observation: -0.012 [-1.654, 1.000], loss: 10.553283, mean_absolute_error: 52.188961, mean_q: 69.727257\n",
      " 562196/700000: episode: 1633, duration: 0.605s, episode steps: 120, steps per second: 198, episode reward: 1.032, mean reward: 0.009 [-100.000, 9.944], mean action: 1.842 [0.000, 3.000], mean observation: 0.088 [-1.239, 1.000], loss: 12.091361, mean_absolute_error: 53.480755, mean_q: 71.529930\n",
      " 562376/700000: episode: 1634, duration: 0.888s, episode steps: 180, steps per second: 203, episode reward: 218.958, mean reward: 1.216 [-3.400, 100.000], mean action: 0.939 [0.000, 3.000], mean observation: 0.106 [-1.149, 1.000], loss: 9.311354, mean_absolute_error: 52.833725, mean_q: 70.767441\n",
      " 562574/700000: episode: 1635, duration: 0.996s, episode steps: 198, steps per second: 199, episode reward: 254.254, mean reward: 1.284 [-19.727, 100.000], mean action: 1.212 [0.000, 3.000], mean observation: 0.066 [-0.783, 1.000], loss: 9.231630, mean_absolute_error: 52.415207, mean_q: 70.174202\n",
      " 562932/700000: episode: 1636, duration: 1.884s, episode steps: 358, steps per second: 190, episode reward: 236.816, mean reward: 0.661 [-9.741, 100.000], mean action: 0.821 [0.000, 3.000], mean observation: 0.147 [-1.014, 1.000], loss: 8.018752, mean_absolute_error: 52.483234, mean_q: 70.283287\n",
      " 563168/700000: episode: 1637, duration: 1.212s, episode steps: 236, steps per second: 195, episode reward: 217.286, mean reward: 0.921 [-7.071, 100.000], mean action: 1.280 [0.000, 3.000], mean observation: 0.065 [-1.177, 1.000], loss: 7.967743, mean_absolute_error: 52.843277, mean_q: 70.640015\n",
      " 563531/700000: episode: 1638, duration: 1.911s, episode steps: 363, steps per second: 190, episode reward: 215.956, mean reward: 0.595 [-10.553, 100.000], mean action: 1.030 [0.000, 3.000], mean observation: 0.080 [-0.892, 1.000], loss: 9.139394, mean_absolute_error: 52.452927, mean_q: 70.215805\n",
      " 563820/700000: episode: 1639, duration: 1.498s, episode steps: 289, steps per second: 193, episode reward: 218.432, mean reward: 0.756 [-10.305, 100.000], mean action: 1.190 [0.000, 3.000], mean observation: 0.128 [-0.768, 1.000], loss: 8.825557, mean_absolute_error: 52.251999, mean_q: 70.094490\n",
      " 564104/700000: episode: 1640, duration: 1.465s, episode steps: 284, steps per second: 194, episode reward: 239.565, mean reward: 0.844 [-17.923, 100.000], mean action: 1.067 [0.000, 3.000], mean observation: 0.114 [-0.933, 1.000], loss: 7.301723, mean_absolute_error: 52.168129, mean_q: 69.991508\n",
      " 564200/700000: episode: 1641, duration: 0.485s, episode steps: 96, steps per second: 198, episode reward: -51.487, mean reward: -0.536 [-100.000, 19.832], mean action: 1.865 [0.000, 3.000], mean observation: 0.090 [-1.688, 1.000], loss: 5.080505, mean_absolute_error: 52.765850, mean_q: 70.729942\n",
      " 564418/700000: episode: 1642, duration: 1.108s, episode steps: 218, steps per second: 197, episode reward: 197.998, mean reward: 0.908 [-10.390, 100.000], mean action: 1.303 [0.000, 3.000], mean observation: 0.061 [-0.682, 1.000], loss: 4.594486, mean_absolute_error: 52.083992, mean_q: 69.816719\n",
      " 564612/700000: episode: 1643, duration: 0.989s, episode steps: 194, steps per second: 196, episode reward: 242.430, mean reward: 1.250 [-8.955, 100.000], mean action: 1.624 [0.000, 3.000], mean observation: 0.095 [-0.698, 1.000], loss: 10.299396, mean_absolute_error: 52.065811, mean_q: 69.794388\n",
      " 564895/700000: episode: 1644, duration: 1.450s, episode steps: 283, steps per second: 195, episode reward: 220.293, mean reward: 0.778 [-10.885, 100.000], mean action: 1.350 [0.000, 3.000], mean observation: 0.071 [-0.751, 1.000], loss: 10.617867, mean_absolute_error: 52.185047, mean_q: 69.938766\n",
      " 565335/700000: episode: 1645, duration: 2.294s, episode steps: 440, steps per second: 192, episode reward: 202.613, mean reward: 0.460 [-10.782, 100.000], mean action: 0.984 [0.000, 3.000], mean observation: 0.132 [-0.895, 1.000], loss: 9.759391, mean_absolute_error: 52.288700, mean_q: 70.024063\n",
      " 565629/700000: episode: 1646, duration: 1.491s, episode steps: 294, steps per second: 197, episode reward: 253.949, mean reward: 0.864 [-18.358, 100.000], mean action: 1.160 [0.000, 3.000], mean observation: 0.135 [-1.328, 1.000], loss: 9.196019, mean_absolute_error: 52.293720, mean_q: 69.866844\n",
      " 565729/700000: episode: 1647, duration: 0.506s, episode steps: 100, steps per second: 198, episode reward: -49.720, mean reward: -0.497 [-100.000, 16.016], mean action: 1.610 [0.000, 3.000], mean observation: -0.034 [-0.908, 1.629], loss: 5.639665, mean_absolute_error: 51.976700, mean_q: 69.597137\n",
      " 565971/700000: episode: 1648, duration: 1.225s, episode steps: 242, steps per second: 198, episode reward: 224.870, mean reward: 0.929 [-9.209, 100.000], mean action: 1.376 [0.000, 3.000], mean observation: 0.092 [-0.742, 1.000], loss: 8.376004, mean_absolute_error: 51.857601, mean_q: 69.539680\n",
      " 566086/700000: episode: 1649, duration: 0.583s, episode steps: 115, steps per second: 197, episode reward: 12.114, mean reward: 0.105 [-100.000, 15.482], mean action: 1.748 [0.000, 3.000], mean observation: -0.029 [-1.035, 1.000], loss: 11.260969, mean_absolute_error: 51.685410, mean_q: 69.039429\n",
      " 566427/700000: episode: 1650, duration: 1.759s, episode steps: 341, steps per second: 194, episode reward: 227.614, mean reward: 0.667 [-17.664, 100.000], mean action: 0.587 [0.000, 3.000], mean observation: 0.185 [-1.019, 1.000], loss: 7.275914, mean_absolute_error: 51.662254, mean_q: 69.056946\n",
      " 566634/700000: episode: 1651, duration: 1.039s, episode steps: 207, steps per second: 199, episode reward: 241.460, mean reward: 1.166 [-8.490, 100.000], mean action: 1.188 [0.000, 3.000], mean observation: 0.042 [-0.880, 1.000], loss: 8.773128, mean_absolute_error: 51.393532, mean_q: 69.094810\n",
      " 566926/700000: episode: 1652, duration: 1.547s, episode steps: 292, steps per second: 189, episode reward: 220.952, mean reward: 0.757 [-17.745, 100.000], mean action: 1.404 [0.000, 3.000], mean observation: 0.074 [-0.614, 1.000], loss: 7.078877, mean_absolute_error: 51.953968, mean_q: 69.409866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 567055/700000: episode: 1653, duration: 0.666s, episode steps: 129, steps per second: 194, episode reward: 17.452, mean reward: 0.135 [-100.000, 15.770], mean action: 1.690 [0.000, 3.000], mean observation: 0.059 [-0.993, 1.000], loss: 15.289721, mean_absolute_error: 52.098419, mean_q: 69.435440\n",
      " 567304/700000: episode: 1654, duration: 1.257s, episode steps: 249, steps per second: 198, episode reward: 246.521, mean reward: 0.990 [-10.351, 100.000], mean action: 1.020 [0.000, 3.000], mean observation: 0.078 [-1.281, 1.000], loss: 12.792688, mean_absolute_error: 51.249249, mean_q: 68.623886\n",
      " 567487/700000: episode: 1655, duration: 0.940s, episode steps: 183, steps per second: 195, episode reward: -15.179, mean reward: -0.083 [-100.000, 21.135], mean action: 1.727 [0.000, 3.000], mean observation: 0.038 [-1.078, 1.000], loss: 11.958326, mean_absolute_error: 51.037739, mean_q: 68.285110\n",
      " 567800/700000: episode: 1656, duration: 1.619s, episode steps: 313, steps per second: 193, episode reward: 194.957, mean reward: 0.623 [-9.865, 100.000], mean action: 1.268 [0.000, 3.000], mean observation: 0.067 [-0.686, 1.000], loss: 9.135077, mean_absolute_error: 51.348148, mean_q: 68.578796\n",
      " 568141/700000: episode: 1657, duration: 1.722s, episode steps: 341, steps per second: 198, episode reward: 266.479, mean reward: 0.781 [-9.923, 100.000], mean action: 1.126 [0.000, 3.000], mean observation: 0.129 [-1.034, 1.000], loss: 7.237554, mean_absolute_error: 51.585674, mean_q: 69.114151\n",
      " 568229/700000: episode: 1658, duration: 0.447s, episode steps: 88, steps per second: 197, episode reward: -82.894, mean reward: -0.942 [-100.000, 13.535], mean action: 1.545 [0.000, 3.000], mean observation: 0.075 [-2.438, 1.000], loss: 10.174148, mean_absolute_error: 50.779293, mean_q: 68.094048\n",
      " 568536/700000: episode: 1659, duration: 1.565s, episode steps: 307, steps per second: 196, episode reward: 209.059, mean reward: 0.681 [-17.671, 100.000], mean action: 1.049 [0.000, 3.000], mean observation: 0.115 [-0.905, 1.000], loss: 8.410137, mean_absolute_error: 51.238579, mean_q: 68.547813\n",
      " 568853/700000: episode: 1660, duration: 1.620s, episode steps: 317, steps per second: 196, episode reward: 251.581, mean reward: 0.794 [-19.387, 100.000], mean action: 1.142 [0.000, 3.000], mean observation: 0.141 [-0.911, 1.000], loss: 9.491317, mean_absolute_error: 51.221375, mean_q: 68.462997\n",
      " 569295/700000: episode: 1661, duration: 2.273s, episode steps: 442, steps per second: 194, episode reward: 190.042, mean reward: 0.430 [-18.336, 100.000], mean action: 0.812 [0.000, 3.000], mean observation: 0.157 [-0.947, 1.000], loss: 9.576293, mean_absolute_error: 51.394924, mean_q: 68.800941\n",
      " 569576/700000: episode: 1662, duration: 1.424s, episode steps: 281, steps per second: 197, episode reward: 272.493, mean reward: 0.970 [-11.875, 100.000], mean action: 1.025 [0.000, 3.000], mean observation: 0.095 [-1.027, 1.011], loss: 7.498754, mean_absolute_error: 51.646526, mean_q: 69.197861\n",
      " 569809/700000: episode: 1663, duration: 1.189s, episode steps: 233, steps per second: 196, episode reward: 231.799, mean reward: 0.995 [-12.220, 100.000], mean action: 1.249 [0.000, 3.000], mean observation: 0.064 [-0.937, 1.000], loss: 7.803523, mean_absolute_error: 51.326523, mean_q: 68.625748\n",
      " 570082/700000: episode: 1664, duration: 1.375s, episode steps: 273, steps per second: 199, episode reward: 203.246, mean reward: 0.744 [-17.538, 100.000], mean action: 0.941 [0.000, 3.000], mean observation: 0.178 [-1.024, 1.000], loss: 9.194213, mean_absolute_error: 51.433472, mean_q: 68.634041\n",
      " 570382/700000: episode: 1665, duration: 1.525s, episode steps: 300, steps per second: 197, episode reward: 213.052, mean reward: 0.710 [-17.754, 100.000], mean action: 0.853 [0.000, 3.000], mean observation: 0.139 [-0.799, 1.000], loss: 5.590398, mean_absolute_error: 51.239296, mean_q: 68.356689\n",
      " 570576/700000: episode: 1666, duration: 0.974s, episode steps: 194, steps per second: 199, episode reward: 223.269, mean reward: 1.151 [-10.194, 100.000], mean action: 1.206 [0.000, 3.000], mean observation: 0.103 [-1.163, 1.000], loss: 7.142747, mean_absolute_error: 51.336170, mean_q: 68.643730\n",
      " 571063/700000: episode: 1667, duration: 2.583s, episode steps: 487, steps per second: 189, episode reward: 191.514, mean reward: 0.393 [-19.816, 100.000], mean action: 0.982 [0.000, 3.000], mean observation: 0.190 [-1.148, 1.062], loss: 10.613235, mean_absolute_error: 51.565620, mean_q: 68.885368\n",
      " 571309/700000: episode: 1668, duration: 1.241s, episode steps: 246, steps per second: 198, episode reward: 214.158, mean reward: 0.871 [-17.605, 100.000], mean action: 0.780 [0.000, 3.000], mean observation: 0.125 [-1.050, 1.000], loss: 7.514781, mean_absolute_error: 51.363026, mean_q: 68.797096\n",
      " 571400/700000: episode: 1669, duration: 0.463s, episode steps: 91, steps per second: 197, episode reward: -26.992, mean reward: -0.297 [-100.000, 19.691], mean action: 1.934 [0.000, 3.000], mean observation: -0.054 [-0.991, 1.603], loss: 9.036821, mean_absolute_error: 51.372707, mean_q: 68.337807\n",
      " 571720/700000: episode: 1670, duration: 1.670s, episode steps: 320, steps per second: 192, episode reward: 219.430, mean reward: 0.686 [-11.325, 100.000], mean action: 1.234 [0.000, 3.000], mean observation: 0.108 [-0.885, 1.000], loss: 11.507582, mean_absolute_error: 51.632896, mean_q: 69.009888\n",
      " 572127/700000: episode: 1671, duration: 2.208s, episode steps: 407, steps per second: 184, episode reward: 251.446, mean reward: 0.618 [-9.820, 100.000], mean action: 1.332 [0.000, 3.000], mean observation: 0.061 [-0.730, 1.000], loss: 8.629898, mean_absolute_error: 51.219116, mean_q: 68.588806\n",
      " 572424/700000: episode: 1672, duration: 1.520s, episode steps: 297, steps per second: 195, episode reward: 224.410, mean reward: 0.756 [-17.496, 100.000], mean action: 1.057 [0.000, 3.000], mean observation: 0.118 [-0.947, 1.000], loss: 11.398607, mean_absolute_error: 51.731426, mean_q: 69.242874\n",
      " 572650/700000: episode: 1673, duration: 1.160s, episode steps: 226, steps per second: 195, episode reward: 210.269, mean reward: 0.930 [-17.830, 100.000], mean action: 1.473 [0.000, 3.000], mean observation: 0.055 [-0.832, 1.000], loss: 8.576423, mean_absolute_error: 51.865364, mean_q: 69.550529\n",
      " 573130/700000: episode: 1674, duration: 2.471s, episode steps: 480, steps per second: 194, episode reward: 199.865, mean reward: 0.416 [-18.691, 100.000], mean action: 0.452 [0.000, 3.000], mean observation: 0.243 [-1.250, 1.000], loss: 8.595315, mean_absolute_error: 51.736938, mean_q: 69.293556\n",
      " 573369/700000: episode: 1675, duration: 1.220s, episode steps: 239, steps per second: 196, episode reward: -12.821, mean reward: -0.054 [-100.000, 12.827], mean action: 1.787 [0.000, 3.000], mean observation: 0.079 [-0.954, 1.000], loss: 7.706232, mean_absolute_error: 51.884983, mean_q: 69.510231\n",
      " 573667/700000: episode: 1676, duration: 1.516s, episode steps: 298, steps per second: 197, episode reward: 164.230, mean reward: 0.551 [-18.692, 100.000], mean action: 0.973 [0.000, 3.000], mean observation: 0.167 [-0.923, 1.000], loss: 7.366757, mean_absolute_error: 51.321648, mean_q: 68.777374\n",
      " 574469/700000: episode: 1677, duration: 4.469s, episode steps: 802, steps per second: 179, episode reward: 183.341, mean reward: 0.229 [-20.221, 100.000], mean action: 0.833 [0.000, 3.000], mean observation: 0.221 [-1.079, 1.000], loss: 9.030252, mean_absolute_error: 51.607403, mean_q: 69.097336\n",
      " 574610/700000: episode: 1678, duration: 0.726s, episode steps: 141, steps per second: 194, episode reward: 204.353, mean reward: 1.449 [-3.130, 100.000], mean action: 1.149 [0.000, 3.000], mean observation: 0.123 [-1.017, 1.000], loss: 6.638459, mean_absolute_error: 51.281498, mean_q: 68.771774\n",
      " 574838/700000: episode: 1679, duration: 1.193s, episode steps: 228, steps per second: 191, episode reward: 233.514, mean reward: 1.024 [-8.688, 100.000], mean action: 1.325 [0.000, 3.000], mean observation: 0.066 [-0.821, 1.000], loss: 5.435445, mean_absolute_error: 51.491383, mean_q: 68.976448\n",
      " 575192/700000: episode: 1680, duration: 2.036s, episode steps: 354, steps per second: 174, episode reward: 164.403, mean reward: 0.464 [-14.544, 100.000], mean action: 2.271 [0.000, 3.000], mean observation: 0.223 [-1.018, 1.000], loss: 7.522839, mean_absolute_error: 51.700916, mean_q: 69.461761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 575295/700000: episode: 1681, duration: 0.554s, episode steps: 103, steps per second: 186, episode reward: -42.333, mean reward: -0.411 [-100.000, 10.996], mean action: 1.932 [0.000, 3.000], mean observation: 0.120 [-0.971, 1.000], loss: 9.269746, mean_absolute_error: 52.033535, mean_q: 69.738403\n",
      " 576295/700000: episode: 1682, duration: 5.775s, episode steps: 1000, steps per second: 173, episode reward: -14.959, mean reward: -0.015 [-18.833, 20.431], mean action: 1.036 [0.000, 3.000], mean observation: 0.174 [-1.010, 1.000], loss: 8.913031, mean_absolute_error: 51.478661, mean_q: 68.998779\n",
      " 576840/700000: episode: 1683, duration: 2.891s, episode steps: 545, steps per second: 189, episode reward: 214.004, mean reward: 0.393 [-19.656, 100.000], mean action: 0.954 [0.000, 3.000], mean observation: 0.165 [-1.041, 1.000], loss: 10.511380, mean_absolute_error: 51.808327, mean_q: 69.459763\n",
      " 576958/700000: episode: 1684, duration: 0.593s, episode steps: 118, steps per second: 199, episode reward: -19.262, mean reward: -0.163 [-100.000, 22.071], mean action: 1.373 [0.000, 3.000], mean observation: 0.018 [-1.288, 1.000], loss: 12.210041, mean_absolute_error: 51.691193, mean_q: 69.375580\n",
      " 577159/700000: episode: 1685, duration: 1.013s, episode steps: 201, steps per second: 198, episode reward: 215.545, mean reward: 1.072 [-18.645, 100.000], mean action: 1.378 [0.000, 3.000], mean observation: 0.092 [-1.046, 1.000], loss: 13.050966, mean_absolute_error: 52.124523, mean_q: 69.920807\n",
      " 577268/700000: episode: 1686, duration: 0.545s, episode steps: 109, steps per second: 200, episode reward: -48.823, mean reward: -0.448 [-100.000, 24.486], mean action: 1.587 [0.000, 3.000], mean observation: 0.011 [-1.067, 1.000], loss: 13.062761, mean_absolute_error: 51.439674, mean_q: 69.108612\n",
      " 577672/700000: episode: 1687, duration: 2.088s, episode steps: 404, steps per second: 193, episode reward: 258.435, mean reward: 0.640 [-19.607, 100.000], mean action: 1.094 [0.000, 3.000], mean observation: 0.169 [-0.765, 1.000], loss: 9.216596, mean_absolute_error: 51.653126, mean_q: 69.291100\n",
      " 577767/700000: episode: 1688, duration: 0.483s, episode steps: 95, steps per second: 197, episode reward: -59.564, mean reward: -0.627 [-100.000, 18.957], mean action: 1.947 [0.000, 3.000], mean observation: -0.009 [-1.066, 1.000], loss: 10.327061, mean_absolute_error: 52.243286, mean_q: 70.198563\n",
      " 578185/700000: episode: 1689, duration: 2.115s, episode steps: 418, steps per second: 198, episode reward: 234.524, mean reward: 0.561 [-17.633, 100.000], mean action: 0.914 [0.000, 3.000], mean observation: 0.166 [-0.883, 1.000], loss: 7.558060, mean_absolute_error: 51.790108, mean_q: 69.543129\n",
      " 578621/700000: episode: 1690, duration: 2.378s, episode steps: 436, steps per second: 183, episode reward: 184.772, mean reward: 0.424 [-19.295, 100.000], mean action: 2.138 [0.000, 3.000], mean observation: 0.138 [-0.791, 1.000], loss: 7.939653, mean_absolute_error: 52.239159, mean_q: 70.034935\n",
      " 578847/700000: episode: 1691, duration: 1.161s, episode steps: 226, steps per second: 195, episode reward: 220.231, mean reward: 0.974 [-13.602, 100.000], mean action: 1.133 [0.000, 3.000], mean observation: 0.059 [-0.863, 1.000], loss: 11.392679, mean_absolute_error: 52.901642, mean_q: 71.128716\n",
      " 579194/700000: episode: 1692, duration: 1.819s, episode steps: 347, steps per second: 191, episode reward: 193.708, mean reward: 0.558 [-11.809, 100.000], mean action: 1.248 [0.000, 3.000], mean observation: 0.016 [-0.918, 1.052], loss: 8.312551, mean_absolute_error: 51.895477, mean_q: 69.762009\n",
      " 580194/700000: episode: 1693, duration: 5.547s, episode steps: 1000, steps per second: 180, episode reward: -15.538, mean reward: -0.016 [-20.718, 18.968], mean action: 1.419 [0.000, 3.000], mean observation: 0.119 [-0.898, 1.163], loss: 8.011375, mean_absolute_error: 52.361301, mean_q: 70.407738\n",
      " 580530/700000: episode: 1694, duration: 1.709s, episode steps: 336, steps per second: 197, episode reward: 229.966, mean reward: 0.684 [-14.346, 100.000], mean action: 0.792 [0.000, 3.000], mean observation: 0.156 [-1.273, 1.000], loss: 10.082759, mean_absolute_error: 52.728283, mean_q: 70.765068\n",
      " 580743/700000: episode: 1695, duration: 1.070s, episode steps: 213, steps per second: 199, episode reward: 218.944, mean reward: 1.028 [-17.941, 100.000], mean action: 1.225 [0.000, 3.000], mean observation: 0.138 [-0.982, 1.074], loss: 10.509525, mean_absolute_error: 52.381386, mean_q: 70.218567\n",
      " 580987/700000: episode: 1696, duration: 1.247s, episode steps: 244, steps per second: 196, episode reward: 217.853, mean reward: 0.893 [-12.571, 100.000], mean action: 1.328 [0.000, 3.000], mean observation: 0.122 [-0.956, 1.000], loss: 7.866529, mean_absolute_error: 52.236092, mean_q: 70.043327\n",
      " 581205/700000: episode: 1697, duration: 1.093s, episode steps: 218, steps per second: 199, episode reward: 236.584, mean reward: 1.085 [-8.356, 100.000], mean action: 1.083 [0.000, 3.000], mean observation: 0.113 [-0.971, 1.000], loss: 4.404747, mean_absolute_error: 52.014568, mean_q: 69.938461\n",
      " 581400/700000: episode: 1698, duration: 0.976s, episode steps: 195, steps per second: 200, episode reward: 246.381, mean reward: 1.263 [-4.811, 100.000], mean action: 1.467 [0.000, 3.000], mean observation: 0.114 [-1.021, 1.019], loss: 7.723289, mean_absolute_error: 52.560165, mean_q: 70.600983\n",
      " 581569/700000: episode: 1699, duration: 0.842s, episode steps: 169, steps per second: 201, episode reward: 234.451, mean reward: 1.387 [-2.543, 100.000], mean action: 0.994 [0.000, 3.000], mean observation: 0.113 [-0.982, 1.000], loss: 9.801657, mean_absolute_error: 52.818302, mean_q: 70.978531\n",
      " 582381/700000: episode: 1700, duration: 4.490s, episode steps: 812, steps per second: 181, episode reward: 151.907, mean reward: 0.187 [-20.033, 100.000], mean action: 0.687 [0.000, 3.000], mean observation: 0.188 [-1.048, 1.000], loss: 7.826497, mean_absolute_error: 52.596737, mean_q: 70.333115\n",
      " 582538/700000: episode: 1701, duration: 0.789s, episode steps: 157, steps per second: 199, episode reward: 246.417, mean reward: 1.570 [-9.701, 100.000], mean action: 1.255 [0.000, 3.000], mean observation: 0.072 [-0.946, 1.000], loss: 7.442189, mean_absolute_error: 53.248291, mean_q: 71.374954\n",
      " 582896/700000: episode: 1702, duration: 1.854s, episode steps: 358, steps per second: 193, episode reward: 245.090, mean reward: 0.685 [-11.484, 100.000], mean action: 1.411 [0.000, 3.000], mean observation: 0.031 [-1.391, 1.000], loss: 10.144253, mean_absolute_error: 53.175072, mean_q: 71.296288\n",
      " 583140/700000: episode: 1703, duration: 1.239s, episode steps: 244, steps per second: 197, episode reward: -146.489, mean reward: -0.600 [-100.000, 11.221], mean action: 1.746 [0.000, 3.000], mean observation: 0.117 [-0.787, 1.043], loss: 7.125563, mean_absolute_error: 52.907310, mean_q: 70.968010\n",
      " 583358/700000: episode: 1704, duration: 1.094s, episode steps: 218, steps per second: 199, episode reward: 225.304, mean reward: 1.034 [-3.613, 100.000], mean action: 1.206 [0.000, 3.000], mean observation: 0.090 [-1.029, 1.000], loss: 9.771891, mean_absolute_error: 52.835972, mean_q: 70.792450\n",
      " 583876/700000: episode: 1705, duration: 2.666s, episode steps: 518, steps per second: 194, episode reward: 221.139, mean reward: 0.427 [-19.935, 100.000], mean action: 0.855 [0.000, 3.000], mean observation: 0.210 [-0.883, 1.219], loss: 9.363256, mean_absolute_error: 52.757595, mean_q: 70.710159\n",
      " 584172/700000: episode: 1706, duration: 1.537s, episode steps: 296, steps per second: 193, episode reward: 188.063, mean reward: 0.635 [-19.396, 100.000], mean action: 1.307 [0.000, 3.000], mean observation: 0.101 [-0.806, 1.000], loss: 8.727105, mean_absolute_error: 52.945133, mean_q: 70.812920\n",
      " 584563/700000: episode: 1707, duration: 2.039s, episode steps: 391, steps per second: 192, episode reward: 179.501, mean reward: 0.459 [-19.222, 100.000], mean action: 1.020 [0.000, 3.000], mean observation: 0.144 [-0.707, 1.000], loss: 10.958031, mean_absolute_error: 52.340645, mean_q: 70.084473\n",
      " 584744/700000: episode: 1708, duration: 0.917s, episode steps: 181, steps per second: 197, episode reward: 211.728, mean reward: 1.170 [-7.060, 100.000], mean action: 1.459 [0.000, 3.000], mean observation: 0.112 [-0.786, 1.000], loss: 6.830244, mean_absolute_error: 52.656132, mean_q: 70.771461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 584931/700000: episode: 1709, duration: 0.947s, episode steps: 187, steps per second: 198, episode reward: -24.210, mean reward: -0.129 [-100.000, 14.290], mean action: 1.642 [0.000, 3.000], mean observation: -0.079 [-0.795, 1.403], loss: 8.108272, mean_absolute_error: 52.415012, mean_q: 69.878616\n",
      " 585272/700000: episode: 1710, duration: 1.757s, episode steps: 341, steps per second: 194, episode reward: 172.960, mean reward: 0.507 [-20.264, 100.000], mean action: 0.845 [0.000, 3.000], mean observation: 0.149 [-1.012, 1.000], loss: 9.483685, mean_absolute_error: 52.402058, mean_q: 70.073586\n",
      " 585531/700000: episode: 1711, duration: 1.318s, episode steps: 259, steps per second: 196, episode reward: 248.169, mean reward: 0.958 [-9.950, 100.000], mean action: 1.189 [0.000, 3.000], mean observation: 0.015 [-0.740, 1.000], loss: 14.938274, mean_absolute_error: 52.464634, mean_q: 70.003868\n",
      " 585847/700000: episode: 1712, duration: 1.619s, episode steps: 316, steps per second: 195, episode reward: 222.576, mean reward: 0.704 [-17.443, 100.000], mean action: 1.386 [0.000, 3.000], mean observation: 0.162 [-0.589, 1.000], loss: 10.694882, mean_absolute_error: 52.098064, mean_q: 69.682213\n",
      " 586108/700000: episode: 1713, duration: 1.338s, episode steps: 261, steps per second: 195, episode reward: 214.031, mean reward: 0.820 [-9.623, 100.000], mean action: 1.395 [0.000, 3.000], mean observation: 0.079 [-0.746, 1.000], loss: 9.339731, mean_absolute_error: 52.362988, mean_q: 69.826622\n",
      " 586444/700000: episode: 1714, duration: 1.732s, episode steps: 336, steps per second: 194, episode reward: 208.702, mean reward: 0.621 [-17.342, 100.000], mean action: 1.146 [0.000, 3.000], mean observation: 0.076 [-0.691, 1.000], loss: 8.095619, mean_absolute_error: 52.233829, mean_q: 69.775658\n",
      " 586726/700000: episode: 1715, duration: 1.431s, episode steps: 282, steps per second: 197, episode reward: -196.779, mean reward: -0.698 [-100.000, 12.365], mean action: 1.681 [0.000, 3.000], mean observation: 0.121 [-0.785, 1.641], loss: 16.362677, mean_absolute_error: 52.319893, mean_q: 69.834869\n",
      " 586929/700000: episode: 1716, duration: 1.030s, episode steps: 203, steps per second: 197, episode reward: 237.540, mean reward: 1.170 [-17.757, 100.000], mean action: 1.143 [0.000, 3.000], mean observation: 0.075 [-0.923, 1.000], loss: 4.765992, mean_absolute_error: 52.106026, mean_q: 69.753181\n",
      " 587151/700000: episode: 1717, duration: 1.126s, episode steps: 222, steps per second: 197, episode reward: 234.522, mean reward: 1.056 [-17.380, 100.000], mean action: 1.279 [0.000, 3.000], mean observation: 0.069 [-0.768, 1.000], loss: 4.147733, mean_absolute_error: 52.296707, mean_q: 70.105431\n",
      " 587360/700000: episode: 1718, duration: 1.043s, episode steps: 209, steps per second: 200, episode reward: 219.450, mean reward: 1.050 [-4.260, 100.000], mean action: 1.086 [0.000, 3.000], mean observation: 0.089 [-0.934, 1.000], loss: 10.296490, mean_absolute_error: 51.941639, mean_q: 69.543854\n",
      " 587558/700000: episode: 1719, duration: 1.000s, episode steps: 198, steps per second: 198, episode reward: 245.123, mean reward: 1.238 [-3.262, 100.000], mean action: 1.237 [0.000, 3.000], mean observation: 0.094 [-0.984, 1.000], loss: 12.408388, mean_absolute_error: 52.456814, mean_q: 70.094765\n",
      " 587817/700000: episode: 1720, duration: 1.345s, episode steps: 259, steps per second: 193, episode reward: 229.982, mean reward: 0.888 [-18.420, 100.000], mean action: 1.224 [0.000, 3.000], mean observation: 0.124 [-0.767, 1.076], loss: 10.664918, mean_absolute_error: 51.889896, mean_q: 69.507774\n",
      " 588379/700000: episode: 1721, duration: 3.093s, episode steps: 562, steps per second: 182, episode reward: 183.409, mean reward: 0.326 [-11.331, 100.000], mean action: 1.466 [0.000, 3.000], mean observation: 0.135 [-0.878, 1.000], loss: 9.083540, mean_absolute_error: 52.107124, mean_q: 69.710510\n",
      " 588693/700000: episode: 1722, duration: 1.637s, episode steps: 314, steps per second: 192, episode reward: 217.887, mean reward: 0.694 [-23.517, 100.000], mean action: 0.844 [0.000, 3.000], mean observation: 0.135 [-0.980, 1.000], loss: 8.132909, mean_absolute_error: 52.211163, mean_q: 69.843964\n",
      " 588963/700000: episode: 1723, duration: 1.373s, episode steps: 270, steps per second: 197, episode reward: 204.409, mean reward: 0.757 [-10.903, 100.000], mean action: 0.978 [0.000, 3.000], mean observation: 0.108 [-0.712, 1.000], loss: 12.431385, mean_absolute_error: 52.167751, mean_q: 69.836456\n",
      " 589211/700000: episode: 1724, duration: 1.261s, episode steps: 248, steps per second: 197, episode reward: 198.561, mean reward: 0.801 [-19.557, 100.000], mean action: 1.105 [0.000, 3.000], mean observation: 0.103 [-1.021, 1.000], loss: 6.115025, mean_absolute_error: 52.323608, mean_q: 70.016487\n",
      " 589648/700000: episode: 1725, duration: 2.311s, episode steps: 437, steps per second: 189, episode reward: 180.726, mean reward: 0.414 [-18.057, 100.000], mean action: 0.886 [0.000, 3.000], mean observation: 0.156 [-0.971, 1.000], loss: 8.430764, mean_absolute_error: 52.529381, mean_q: 70.500931\n",
      " 589931/700000: episode: 1726, duration: 1.490s, episode steps: 283, steps per second: 190, episode reward: -263.093, mean reward: -0.930 [-100.000, 24.385], mean action: 1.481 [0.000, 3.000], mean observation: 0.122 [-2.229, 1.348], loss: 8.556736, mean_absolute_error: 51.938400, mean_q: 69.728661\n",
      " 590085/700000: episode: 1727, duration: 0.763s, episode steps: 154, steps per second: 202, episode reward: 234.266, mean reward: 1.521 [-2.748, 100.000], mean action: 1.156 [0.000, 3.000], mean observation: 0.057 [-1.004, 1.000], loss: 6.767426, mean_absolute_error: 52.372257, mean_q: 70.246361\n",
      " 590366/700000: episode: 1728, duration: 1.424s, episode steps: 281, steps per second: 197, episode reward: 203.747, mean reward: 0.725 [-18.176, 100.000], mean action: 0.961 [0.000, 3.000], mean observation: 0.140 [-0.909, 1.000], loss: 10.893694, mean_absolute_error: 52.464916, mean_q: 70.404266\n",
      " 590604/700000: episode: 1729, duration: 1.196s, episode steps: 238, steps per second: 199, episode reward: 222.694, mean reward: 0.936 [-19.600, 100.000], mean action: 1.248 [0.000, 3.000], mean observation: 0.087 [-0.903, 1.000], loss: 5.427593, mean_absolute_error: 52.290211, mean_q: 70.021179\n",
      " 590866/700000: episode: 1730, duration: 1.349s, episode steps: 262, steps per second: 194, episode reward: 195.720, mean reward: 0.747 [-17.948, 100.000], mean action: 1.531 [0.000, 3.000], mean observation: 0.083 [-0.907, 1.000], loss: 10.883865, mean_absolute_error: 52.149925, mean_q: 69.862404\n",
      " 591064/700000: episode: 1731, duration: 0.993s, episode steps: 198, steps per second: 199, episode reward: 234.715, mean reward: 1.185 [-4.126, 100.000], mean action: 1.116 [0.000, 3.000], mean observation: 0.077 [-0.968, 1.000], loss: 8.316557, mean_absolute_error: 52.232964, mean_q: 69.904648\n",
      " 591308/700000: episode: 1732, duration: 1.243s, episode steps: 244, steps per second: 196, episode reward: 224.893, mean reward: 0.922 [-3.904, 100.000], mean action: 1.320 [0.000, 3.000], mean observation: 0.112 [-0.761, 1.000], loss: 8.736822, mean_absolute_error: 52.686066, mean_q: 70.528542\n",
      " 592216/700000: episode: 1733, duration: 5.213s, episode steps: 908, steps per second: 174, episode reward: 165.435, mean reward: 0.182 [-18.853, 100.000], mean action: 2.339 [0.000, 3.000], mean observation: 0.195 [-0.808, 1.000], loss: 8.560575, mean_absolute_error: 52.320080, mean_q: 70.131264\n",
      " 592814/700000: episode: 1734, duration: 3.246s, episode steps: 598, steps per second: 184, episode reward: 161.794, mean reward: 0.271 [-17.599, 100.000], mean action: 1.089 [0.000, 3.000], mean observation: 0.170 [-1.208, 1.000], loss: 8.648452, mean_absolute_error: 51.999294, mean_q: 69.766655\n",
      " 592953/700000: episode: 1735, duration: 0.708s, episode steps: 139, steps per second: 196, episode reward: -3.208, mean reward: -0.023 [-100.000, 13.242], mean action: 1.978 [0.000, 3.000], mean observation: -0.038 [-0.863, 1.000], loss: 5.631599, mean_absolute_error: 51.320923, mean_q: 68.742844\n",
      " 593290/700000: episode: 1736, duration: 1.730s, episode steps: 337, steps per second: 195, episode reward: 195.729, mean reward: 0.581 [-17.937, 100.000], mean action: 1.009 [0.000, 3.000], mean observation: 0.112 [-0.966, 1.000], loss: 7.354170, mean_absolute_error: 51.727802, mean_q: 69.468887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 593513/700000: episode: 1737, duration: 1.256s, episode steps: 223, steps per second: 178, episode reward: 224.345, mean reward: 1.006 [-9.623, 100.000], mean action: 1.507 [0.000, 3.000], mean observation: 0.042 [-0.946, 1.000], loss: 10.911820, mean_absolute_error: 51.824001, mean_q: 69.335587\n",
      " 593841/700000: episode: 1738, duration: 1.710s, episode steps: 328, steps per second: 192, episode reward: 218.788, mean reward: 0.667 [-3.264, 100.000], mean action: 1.482 [0.000, 3.000], mean observation: 0.082 [-0.744, 1.000], loss: 5.607190, mean_absolute_error: 52.035740, mean_q: 69.725548\n",
      " 594083/700000: episode: 1739, duration: 1.251s, episode steps: 242, steps per second: 193, episode reward: 224.108, mean reward: 0.926 [-8.809, 100.000], mean action: 1.434 [0.000, 3.000], mean observation: 0.090 [-1.002, 1.000], loss: 8.003345, mean_absolute_error: 52.377926, mean_q: 70.167633\n",
      " 594284/700000: episode: 1740, duration: 1.000s, episode steps: 201, steps per second: 201, episode reward: 233.475, mean reward: 1.162 [-9.920, 100.000], mean action: 1.100 [0.000, 3.000], mean observation: 0.103 [-1.015, 1.019], loss: 9.129664, mean_absolute_error: 52.481373, mean_q: 69.880219\n",
      " 594638/700000: episode: 1741, duration: 1.847s, episode steps: 354, steps per second: 192, episode reward: 247.384, mean reward: 0.699 [-11.497, 100.000], mean action: 0.811 [0.000, 3.000], mean observation: 0.171 [-1.013, 1.001], loss: 8.658606, mean_absolute_error: 52.243954, mean_q: 70.070419\n",
      " 594890/700000: episode: 1742, duration: 1.289s, episode steps: 252, steps per second: 195, episode reward: 241.795, mean reward: 0.960 [-19.255, 100.000], mean action: 1.091 [0.000, 3.000], mean observation: 0.068 [-0.752, 1.000], loss: 9.248552, mean_absolute_error: 52.066971, mean_q: 69.665970\n",
      " 595304/700000: episode: 1743, duration: 2.202s, episode steps: 414, steps per second: 188, episode reward: 225.034, mean reward: 0.544 [-19.140, 100.000], mean action: 1.628 [0.000, 3.000], mean observation: 0.139 [-0.876, 1.000], loss: 10.125148, mean_absolute_error: 52.192917, mean_q: 69.846573\n",
      " 595429/700000: episode: 1744, duration: 0.839s, episode steps: 125, steps per second: 149, episode reward: 13.872, mean reward: 0.111 [-100.000, 13.197], mean action: 1.800 [0.000, 3.000], mean observation: -0.065 [-0.776, 1.109], loss: 8.199317, mean_absolute_error: 52.171589, mean_q: 70.159714\n",
      " 595702/700000: episode: 1745, duration: 1.464s, episode steps: 273, steps per second: 187, episode reward: 196.324, mean reward: 0.719 [-12.797, 100.000], mean action: 0.897 [0.000, 3.000], mean observation: 0.079 [-1.016, 1.000], loss: 9.519261, mean_absolute_error: 52.721798, mean_q: 70.762550\n",
      " 596088/700000: episode: 1746, duration: 2.009s, episode steps: 386, steps per second: 192, episode reward: 225.638, mean reward: 0.585 [-24.178, 100.000], mean action: 1.386 [0.000, 3.000], mean observation: 0.210 [-0.788, 1.000], loss: 9.103919, mean_absolute_error: 52.341301, mean_q: 69.886940\n",
      " 596345/700000: episode: 1747, duration: 1.318s, episode steps: 257, steps per second: 195, episode reward: 192.107, mean reward: 0.747 [-8.323, 100.000], mean action: 1.117 [0.000, 3.000], mean observation: 0.065 [-0.814, 1.000], loss: 9.485120, mean_absolute_error: 52.468563, mean_q: 70.140327\n",
      " 596561/700000: episode: 1748, duration: 1.085s, episode steps: 216, steps per second: 199, episode reward: 230.384, mean reward: 1.067 [-17.341, 100.000], mean action: 1.278 [0.000, 3.000], mean observation: 0.058 [-0.925, 1.000], loss: 7.381853, mean_absolute_error: 52.521893, mean_q: 70.118942\n",
      " 596850/700000: episode: 1749, duration: 1.519s, episode steps: 289, steps per second: 190, episode reward: 260.637, mean reward: 0.902 [-19.500, 100.000], mean action: 1.010 [0.000, 3.000], mean observation: 0.139 [-1.064, 1.000], loss: 9.577265, mean_absolute_error: 52.519714, mean_q: 70.131500\n",
      " 597240/700000: episode: 1750, duration: 2.242s, episode steps: 390, steps per second: 174, episode reward: 223.558, mean reward: 0.573 [-17.725, 100.000], mean action: 1.356 [0.000, 3.000], mean observation: 0.110 [-0.859, 1.000], loss: 9.246913, mean_absolute_error: 52.074982, mean_q: 69.462196\n",
      " 597416/700000: episode: 1751, duration: 0.915s, episode steps: 176, steps per second: 192, episode reward: 253.005, mean reward: 1.438 [-3.242, 100.000], mean action: 1.290 [0.000, 3.000], mean observation: 0.135 [-1.080, 1.000], loss: 7.557811, mean_absolute_error: 52.038696, mean_q: 69.732521\n",
      " 597876/700000: episode: 1752, duration: 2.725s, episode steps: 460, steps per second: 169, episode reward: 227.837, mean reward: 0.495 [-10.322, 100.000], mean action: 1.200 [0.000, 3.000], mean observation: 0.184 [-0.685, 1.111], loss: 9.112808, mean_absolute_error: 52.495880, mean_q: 70.203461\n",
      " 598012/700000: episode: 1753, duration: 0.847s, episode steps: 136, steps per second: 161, episode reward: -6.891, mean reward: -0.051 [-100.000, 13.170], mean action: 1.816 [0.000, 3.000], mean observation: 0.092 [-0.900, 1.000], loss: 7.833519, mean_absolute_error: 52.281940, mean_q: 70.130882\n",
      " 598266/700000: episode: 1754, duration: 1.378s, episode steps: 254, steps per second: 184, episode reward: 238.060, mean reward: 0.937 [-8.241, 100.000], mean action: 1.445 [0.000, 3.000], mean observation: 0.112 [-0.879, 1.000], loss: 10.375308, mean_absolute_error: 51.777409, mean_q: 69.247604\n",
      " 598518/700000: episode: 1755, duration: 1.437s, episode steps: 252, steps per second: 175, episode reward: 226.404, mean reward: 0.898 [-3.201, 100.000], mean action: 0.944 [0.000, 3.000], mean observation: 0.137 [-0.901, 1.000], loss: 12.685081, mean_absolute_error: 51.891586, mean_q: 69.399109\n",
      " 598713/700000: episode: 1756, duration: 0.990s, episode steps: 195, steps per second: 197, episode reward: 228.507, mean reward: 1.172 [-10.450, 100.000], mean action: 1.467 [0.000, 3.000], mean observation: 0.038 [-0.853, 1.000], loss: 6.398394, mean_absolute_error: 52.487030, mean_q: 70.133240\n",
      " 598982/700000: episode: 1757, duration: 1.408s, episode steps: 269, steps per second: 191, episode reward: 238.917, mean reward: 0.888 [-9.029, 100.000], mean action: 1.416 [0.000, 3.000], mean observation: 0.044 [-0.703, 1.000], loss: 10.366919, mean_absolute_error: 52.115288, mean_q: 69.434593\n",
      " 599233/700000: episode: 1758, duration: 1.279s, episode steps: 251, steps per second: 196, episode reward: 222.599, mean reward: 0.887 [-19.628, 100.000], mean action: 0.992 [0.000, 3.000], mean observation: 0.078 [-1.069, 1.000], loss: 8.563439, mean_absolute_error: 52.193256, mean_q: 69.814163\n",
      " 599374/700000: episode: 1759, duration: 0.719s, episode steps: 141, steps per second: 196, episode reward: 18.371, mean reward: 0.130 [-100.000, 17.766], mean action: 1.957 [0.000, 3.000], mean observation: -0.026 [-1.626, 1.000], loss: 10.285927, mean_absolute_error: 52.088890, mean_q: 69.688065\n",
      " 599640/700000: episode: 1760, duration: 1.361s, episode steps: 266, steps per second: 195, episode reward: 205.378, mean reward: 0.772 [-9.594, 100.000], mean action: 1.165 [0.000, 3.000], mean observation: 0.128 [-0.986, 1.000], loss: 6.817459, mean_absolute_error: 51.917027, mean_q: 69.445656\n",
      " 600093/700000: episode: 1761, duration: 2.375s, episode steps: 453, steps per second: 191, episode reward: 221.510, mean reward: 0.489 [-17.756, 100.000], mean action: 1.082 [0.000, 3.000], mean observation: 0.128 [-0.900, 1.000], loss: 10.492437, mean_absolute_error: 52.137314, mean_q: 69.698196\n",
      " 600263/700000: episode: 1762, duration: 0.853s, episode steps: 170, steps per second: 199, episode reward: -16.005, mean reward: -0.094 [-100.000, 15.835], mean action: 1.359 [0.000, 3.000], mean observation: 0.050 [-0.970, 1.000], loss: 8.332548, mean_absolute_error: 51.518360, mean_q: 69.187950\n",
      " 600524/700000: episode: 1763, duration: 1.331s, episode steps: 261, steps per second: 196, episode reward: 215.116, mean reward: 0.824 [-9.353, 100.000], mean action: 1.425 [0.000, 3.000], mean observation: 0.074 [-0.995, 1.000], loss: 7.462281, mean_absolute_error: 51.974545, mean_q: 69.413376\n",
      " 600991/700000: episode: 1764, duration: 2.538s, episode steps: 467, steps per second: 184, episode reward: 217.279, mean reward: 0.465 [-14.588, 100.000], mean action: 0.743 [0.000, 3.000], mean observation: 0.168 [-0.955, 1.000], loss: 8.166523, mean_absolute_error: 51.820187, mean_q: 69.223595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 601369/700000: episode: 1765, duration: 1.942s, episode steps: 378, steps per second: 195, episode reward: 174.316, mean reward: 0.461 [-13.605, 100.000], mean action: 1.593 [0.000, 3.000], mean observation: 0.153 [-0.873, 1.000], loss: 6.613391, mean_absolute_error: 52.103329, mean_q: 69.581017\n",
      " 601633/700000: episode: 1766, duration: 1.344s, episode steps: 264, steps per second: 196, episode reward: 247.007, mean reward: 0.936 [-10.957, 100.000], mean action: 1.144 [0.000, 3.000], mean observation: 0.127 [-0.752, 1.000], loss: 6.861195, mean_absolute_error: 51.592369, mean_q: 68.825745\n",
      " 601907/700000: episode: 1767, duration: 1.373s, episode steps: 274, steps per second: 200, episode reward: 217.822, mean reward: 0.795 [-3.146, 100.000], mean action: 0.920 [0.000, 3.000], mean observation: 0.126 [-0.937, 1.000], loss: 13.971601, mean_absolute_error: 51.371922, mean_q: 68.737839\n",
      " 602292/700000: episode: 1768, duration: 2.024s, episode steps: 385, steps per second: 190, episode reward: 191.951, mean reward: 0.499 [-17.465, 100.000], mean action: 0.906 [0.000, 3.000], mean observation: 0.188 [-0.972, 1.000], loss: 11.068542, mean_absolute_error: 51.736416, mean_q: 69.119301\n",
      " 602523/700000: episode: 1769, duration: 1.153s, episode steps: 231, steps per second: 200, episode reward: 213.355, mean reward: 0.924 [-8.682, 100.000], mean action: 0.870 [0.000, 3.000], mean observation: 0.106 [-1.224, 1.000], loss: 6.823571, mean_absolute_error: 51.699074, mean_q: 69.096863\n",
      " 602649/700000: episode: 1770, duration: 0.630s, episode steps: 126, steps per second: 200, episode reward: -13.122, mean reward: -0.104 [-100.000, 9.470], mean action: 1.595 [0.000, 3.000], mean observation: 0.115 [-0.901, 1.000], loss: 3.912308, mean_absolute_error: 52.641689, mean_q: 70.437607\n",
      " 602950/700000: episode: 1771, duration: 1.535s, episode steps: 301, steps per second: 196, episode reward: 246.087, mean reward: 0.818 [-20.427, 100.000], mean action: 1.096 [0.000, 3.000], mean observation: 0.152 [-0.852, 1.000], loss: 7.902349, mean_absolute_error: 51.737129, mean_q: 69.234497\n",
      " 603553/700000: episode: 1772, duration: 3.134s, episode steps: 603, steps per second: 192, episode reward: 190.568, mean reward: 0.316 [-23.312, 100.000], mean action: 0.718 [0.000, 3.000], mean observation: 0.159 [-0.739, 1.000], loss: 13.575385, mean_absolute_error: 51.788345, mean_q: 69.155869\n",
      " 603697/700000: episode: 1773, duration: 0.730s, episode steps: 144, steps per second: 197, episode reward: -24.005, mean reward: -0.167 [-100.000, 16.469], mean action: 1.931 [0.000, 3.000], mean observation: 0.006 [-1.528, 1.000], loss: 4.733654, mean_absolute_error: 51.309586, mean_q: 68.809402\n",
      " 604089/700000: episode: 1774, duration: 2.134s, episode steps: 392, steps per second: 184, episode reward: 248.063, mean reward: 0.633 [-19.816, 100.000], mean action: 1.148 [0.000, 3.000], mean observation: 0.112 [-0.782, 1.179], loss: 6.504284, mean_absolute_error: 51.603077, mean_q: 68.909973\n",
      " 604193/700000: episode: 1775, duration: 0.533s, episode steps: 104, steps per second: 195, episode reward: -7.080, mean reward: -0.068 [-100.000, 12.598], mean action: 1.529 [0.000, 3.000], mean observation: -0.041 [-0.832, 1.000], loss: 12.837915, mean_absolute_error: 51.501217, mean_q: 68.845650\n",
      " 604364/700000: episode: 1776, duration: 0.881s, episode steps: 171, steps per second: 194, episode reward: -26.090, mean reward: -0.153 [-100.000, 14.227], mean action: 1.930 [0.000, 3.000], mean observation: -0.057 [-1.203, 1.000], loss: 6.885875, mean_absolute_error: 51.935650, mean_q: 69.291710\n",
      " 604716/700000: episode: 1777, duration: 1.876s, episode steps: 352, steps per second: 188, episode reward: 264.518, mean reward: 0.751 [-19.443, 100.000], mean action: 1.153 [0.000, 3.000], mean observation: 0.128 [-0.904, 1.000], loss: 5.575293, mean_absolute_error: 51.970669, mean_q: 69.656662\n",
      " 605043/700000: episode: 1778, duration: 1.676s, episode steps: 327, steps per second: 195, episode reward: 221.058, mean reward: 0.676 [-10.950, 100.000], mean action: 0.731 [0.000, 3.000], mean observation: 0.140 [-0.871, 1.000], loss: 8.076079, mean_absolute_error: 52.113991, mean_q: 69.791451\n",
      " 605268/700000: episode: 1779, duration: 1.119s, episode steps: 225, steps per second: 201, episode reward: 235.977, mean reward: 1.049 [-20.672, 100.000], mean action: 1.013 [0.000, 3.000], mean observation: 0.106 [-0.871, 1.000], loss: 6.830257, mean_absolute_error: 52.358196, mean_q: 70.115372\n",
      " 605638/700000: episode: 1780, duration: 1.873s, episode steps: 370, steps per second: 198, episode reward: 236.643, mean reward: 0.640 [-18.180, 100.000], mean action: 0.989 [0.000, 3.000], mean observation: 0.155 [-0.994, 1.333], loss: 8.985860, mean_absolute_error: 52.637650, mean_q: 70.405640\n",
      " 605748/700000: episode: 1781, duration: 0.557s, episode steps: 110, steps per second: 197, episode reward: 2.025, mean reward: 0.018 [-100.000, 15.071], mean action: 1.655 [0.000, 3.000], mean observation: 0.040 [-0.990, 1.000], loss: 9.389973, mean_absolute_error: 52.623936, mean_q: 70.645294\n",
      " 605962/700000: episode: 1782, duration: 1.066s, episode steps: 214, steps per second: 201, episode reward: 234.045, mean reward: 1.094 [-4.527, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.131 [-1.034, 1.000], loss: 14.251200, mean_absolute_error: 52.199505, mean_q: 69.630531\n",
      " 606406/700000: episode: 1783, duration: 2.384s, episode steps: 444, steps per second: 186, episode reward: 235.136, mean reward: 0.530 [-18.837, 100.000], mean action: 1.367 [0.000, 3.000], mean observation: 0.093 [-0.870, 1.000], loss: 8.519111, mean_absolute_error: 52.428562, mean_q: 69.918388\n",
      " 606744/700000: episode: 1784, duration: 1.735s, episode steps: 338, steps per second: 195, episode reward: 256.131, mean reward: 0.758 [-12.713, 100.000], mean action: 1.355 [0.000, 3.000], mean observation: 0.014 [-0.790, 1.000], loss: 6.872317, mean_absolute_error: 52.164986, mean_q: 69.896088\n",
      " 606970/700000: episode: 1785, duration: 1.156s, episode steps: 226, steps per second: 195, episode reward: 240.824, mean reward: 1.066 [-9.168, 100.000], mean action: 1.199 [0.000, 3.000], mean observation: 0.031 [-0.911, 1.000], loss: 7.049457, mean_absolute_error: 52.302189, mean_q: 69.883255\n",
      " 607899/700000: episode: 1786, duration: 5.270s, episode steps: 929, steps per second: 176, episode reward: 192.548, mean reward: 0.207 [-19.683, 100.000], mean action: 1.882 [0.000, 3.000], mean observation: 0.103 [-1.249, 1.000], loss: 8.908583, mean_absolute_error: 52.304413, mean_q: 70.014053\n",
      " 608142/700000: episode: 1787, duration: 1.244s, episode steps: 243, steps per second: 195, episode reward: 226.615, mean reward: 0.933 [-9.288, 100.000], mean action: 1.395 [0.000, 3.000], mean observation: 0.130 [-0.950, 1.000], loss: 7.035080, mean_absolute_error: 52.649105, mean_q: 70.434204\n",
      " 608351/700000: episode: 1788, duration: 1.039s, episode steps: 209, steps per second: 201, episode reward: 262.314, mean reward: 1.255 [-9.302, 100.000], mean action: 1.134 [0.000, 3.000], mean observation: 0.118 [-0.668, 1.244], loss: 10.931880, mean_absolute_error: 52.524433, mean_q: 70.456512\n",
      " 608593/700000: episode: 1789, duration: 1.232s, episode steps: 242, steps per second: 196, episode reward: 256.649, mean reward: 1.061 [-8.271, 100.000], mean action: 1.153 [0.000, 3.000], mean observation: 0.149 [-0.745, 1.560], loss: 6.997566, mean_absolute_error: 52.578430, mean_q: 70.627914\n",
      " 608765/700000: episode: 1790, duration: 0.854s, episode steps: 172, steps per second: 201, episode reward: 253.193, mean reward: 1.472 [-10.820, 100.000], mean action: 0.924 [0.000, 3.000], mean observation: 0.125 [-1.465, 1.000], loss: 4.958118, mean_absolute_error: 52.744038, mean_q: 70.954361\n",
      " 608973/700000: episode: 1791, duration: 1.056s, episode steps: 208, steps per second: 197, episode reward: 256.597, mean reward: 1.234 [-22.448, 100.000], mean action: 1.370 [0.000, 3.000], mean observation: 0.117 [-0.765, 1.033], loss: 10.716757, mean_absolute_error: 52.583382, mean_q: 70.629478\n",
      " 609263/700000: episode: 1792, duration: 1.480s, episode steps: 290, steps per second: 196, episode reward: 241.188, mean reward: 0.832 [-15.114, 100.000], mean action: 1.252 [0.000, 3.000], mean observation: 0.050 [-0.980, 1.002], loss: 8.096710, mean_absolute_error: 53.076710, mean_q: 71.118057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 609663/700000: episode: 1793, duration: 2.049s, episode steps: 400, steps per second: 195, episode reward: 183.211, mean reward: 0.458 [-10.865, 100.000], mean action: 1.093 [0.000, 3.000], mean observation: 0.180 [-0.971, 1.000], loss: 6.431015, mean_absolute_error: 53.035713, mean_q: 71.010773\n",
      " 609891/700000: episode: 1794, duration: 1.167s, episode steps: 228, steps per second: 195, episode reward: 248.350, mean reward: 1.089 [-12.392, 100.000], mean action: 1.263 [0.000, 3.000], mean observation: 0.118 [-0.727, 1.185], loss: 5.310159, mean_absolute_error: 52.703323, mean_q: 70.680923\n",
      " 610102/700000: episode: 1795, duration: 1.065s, episode steps: 211, steps per second: 198, episode reward: 234.032, mean reward: 1.109 [-9.523, 100.000], mean action: 1.370 [0.000, 3.000], mean observation: 0.084 [-0.754, 1.000], loss: 7.689984, mean_absolute_error: 52.675346, mean_q: 70.471359\n",
      " 610297/700000: episode: 1796, duration: 0.984s, episode steps: 195, steps per second: 198, episode reward: 241.059, mean reward: 1.236 [-11.886, 100.000], mean action: 1.179 [0.000, 3.000], mean observation: 0.110 [-0.908, 1.080], loss: 7.262436, mean_absolute_error: 53.047771, mean_q: 70.989662\n",
      " 610615/700000: episode: 1797, duration: 1.695s, episode steps: 318, steps per second: 188, episode reward: -198.758, mean reward: -0.625 [-100.000, 24.614], mean action: 1.921 [0.000, 3.000], mean observation: 0.089 [-0.851, 1.410], loss: 11.373199, mean_absolute_error: 52.850002, mean_q: 70.859924\n",
      " 611003/700000: episode: 1798, duration: 1.978s, episode steps: 388, steps per second: 196, episode reward: 208.853, mean reward: 0.538 [-18.268, 100.000], mean action: 0.786 [0.000, 3.000], mean observation: 0.147 [-1.116, 1.000], loss: 10.326683, mean_absolute_error: 52.476250, mean_q: 70.162933\n",
      " 611265/700000: episode: 1799, duration: 1.322s, episode steps: 262, steps per second: 198, episode reward: 224.313, mean reward: 0.856 [-17.466, 100.000], mean action: 1.015 [0.000, 3.000], mean observation: 0.093 [-0.934, 1.000], loss: 10.256875, mean_absolute_error: 52.731159, mean_q: 70.770302\n",
      " 611503/700000: episode: 1800, duration: 1.189s, episode steps: 238, steps per second: 200, episode reward: 272.629, mean reward: 1.146 [-4.591, 100.000], mean action: 1.143 [0.000, 3.000], mean observation: 0.147 [-0.880, 1.012], loss: 7.439840, mean_absolute_error: 52.943031, mean_q: 70.858864\n",
      " 611715/700000: episode: 1801, duration: 1.067s, episode steps: 212, steps per second: 199, episode reward: 220.618, mean reward: 1.041 [-6.301, 100.000], mean action: 1.127 [0.000, 3.000], mean observation: 0.112 [-0.867, 1.000], loss: 8.915192, mean_absolute_error: 52.625786, mean_q: 70.132462\n",
      " 611983/700000: episode: 1802, duration: 1.343s, episode steps: 268, steps per second: 200, episode reward: 201.928, mean reward: 0.753 [-3.835, 100.000], mean action: 0.802 [0.000, 3.000], mean observation: 0.140 [-1.027, 1.000], loss: 6.345150, mean_absolute_error: 52.460152, mean_q: 70.240318\n",
      " 612299/700000: episode: 1803, duration: 1.598s, episode steps: 316, steps per second: 198, episode reward: 216.576, mean reward: 0.685 [-18.788, 100.000], mean action: 0.880 [0.000, 3.000], mean observation: 0.169 [-1.252, 1.000], loss: 9.208890, mean_absolute_error: 52.678173, mean_q: 70.095390\n",
      " 612625/700000: episode: 1804, duration: 1.706s, episode steps: 326, steps per second: 191, episode reward: 256.854, mean reward: 0.788 [-19.374, 100.000], mean action: 0.991 [0.000, 3.000], mean observation: 0.084 [-0.833, 1.000], loss: 13.137204, mean_absolute_error: 52.337986, mean_q: 70.006287\n",
      " 612840/700000: episode: 1805, duration: 1.074s, episode steps: 215, steps per second: 200, episode reward: 204.885, mean reward: 0.953 [-9.874, 100.000], mean action: 0.916 [0.000, 3.000], mean observation: 0.053 [-0.869, 1.000], loss: 10.750175, mean_absolute_error: 52.777634, mean_q: 70.697311\n",
      " 613135/700000: episode: 1806, duration: 1.490s, episode steps: 295, steps per second: 198, episode reward: 238.915, mean reward: 0.810 [-10.607, 100.000], mean action: 1.295 [0.000, 3.000], mean observation: 0.121 [-0.769, 1.105], loss: 11.431225, mean_absolute_error: 53.094776, mean_q: 71.093353\n",
      " 613361/700000: episode: 1807, duration: 1.168s, episode steps: 226, steps per second: 194, episode reward: 180.434, mean reward: 0.798 [-8.674, 100.000], mean action: 1.412 [0.000, 3.000], mean observation: 0.104 [-0.886, 1.317], loss: 8.195060, mean_absolute_error: 51.964474, mean_q: 69.518692\n",
      " 613573/700000: episode: 1808, duration: 1.056s, episode steps: 212, steps per second: 201, episode reward: 224.929, mean reward: 1.061 [-8.954, 100.000], mean action: 1.127 [0.000, 3.000], mean observation: 0.126 [-1.014, 1.000], loss: 13.916785, mean_absolute_error: 52.223686, mean_q: 70.042793\n",
      " 613752/700000: episode: 1809, duration: 0.893s, episode steps: 179, steps per second: 200, episode reward: 231.744, mean reward: 1.295 [-6.029, 100.000], mean action: 1.156 [0.000, 3.000], mean observation: 0.107 [-0.912, 1.000], loss: 8.851186, mean_absolute_error: 52.757923, mean_q: 70.794952\n",
      " 613992/700000: episode: 1810, duration: 1.224s, episode steps: 240, steps per second: 196, episode reward: 240.865, mean reward: 1.004 [-19.420, 100.000], mean action: 1.100 [0.000, 3.000], mean observation: 0.077 [-0.800, 1.000], loss: 8.735045, mean_absolute_error: 52.383144, mean_q: 70.169518\n",
      " 614239/700000: episode: 1811, duration: 1.257s, episode steps: 247, steps per second: 197, episode reward: 247.952, mean reward: 1.004 [-8.711, 100.000], mean action: 1.138 [0.000, 3.000], mean observation: 0.066 [-1.077, 1.000], loss: 10.755639, mean_absolute_error: 52.315998, mean_q: 70.127502\n",
      " 614502/700000: episode: 1812, duration: 1.340s, episode steps: 263, steps per second: 196, episode reward: 249.367, mean reward: 0.948 [-19.484, 100.000], mean action: 1.703 [0.000, 3.000], mean observation: 0.058 [-0.713, 1.000], loss: 8.303107, mean_absolute_error: 52.573483, mean_q: 70.437927\n",
      " 615008/700000: episode: 1813, duration: 2.632s, episode steps: 506, steps per second: 192, episode reward: 248.351, mean reward: 0.491 [-20.455, 100.000], mean action: 1.134 [0.000, 3.000], mean observation: 0.168 [-0.743, 1.141], loss: 7.057824, mean_absolute_error: 52.546429, mean_q: 70.249924\n",
      " 615925/700000: episode: 1814, duration: 5.050s, episode steps: 917, steps per second: 182, episode reward: 152.750, mean reward: 0.167 [-20.424, 100.000], mean action: 1.086 [0.000, 3.000], mean observation: 0.208 [-1.069, 1.013], loss: 7.793258, mean_absolute_error: 52.589516, mean_q: 70.516876\n",
      " 616900/700000: episode: 1815, duration: 5.569s, episode steps: 975, steps per second: 175, episode reward: 141.993, mean reward: 0.146 [-22.813, 100.000], mean action: 1.192 [0.000, 3.000], mean observation: 0.189 [-0.990, 1.034], loss: 12.723654, mean_absolute_error: 52.947517, mean_q: 70.604744\n",
      " 617034/700000: episode: 1816, duration: 0.667s, episode steps: 134, steps per second: 201, episode reward: -35.812, mean reward: -0.267 [-100.000, 18.567], mean action: 1.373 [0.000, 3.000], mean observation: 0.030 [-0.971, 1.000], loss: 11.484103, mean_absolute_error: 52.557114, mean_q: 70.103752\n",
      " 617236/700000: episode: 1817, duration: 1.028s, episode steps: 202, steps per second: 197, episode reward: 232.968, mean reward: 1.153 [-17.375, 100.000], mean action: 1.441 [0.000, 3.000], mean observation: 0.142 [-0.952, 1.000], loss: 9.841490, mean_absolute_error: 53.077244, mean_q: 70.882446\n",
      " 617348/700000: episode: 1818, duration: 0.556s, episode steps: 112, steps per second: 201, episode reward: -45.138, mean reward: -0.403 [-100.000, 18.007], mean action: 1.571 [0.000, 3.000], mean observation: -0.005 [-1.030, 1.539], loss: 5.735451, mean_absolute_error: 52.721241, mean_q: 70.522621\n",
      " 617612/700000: episode: 1819, duration: 1.341s, episode steps: 264, steps per second: 197, episode reward: 228.929, mean reward: 0.867 [-20.006, 100.000], mean action: 0.894 [0.000, 3.000], mean observation: 0.117 [-0.921, 1.000], loss: 9.611981, mean_absolute_error: 52.946033, mean_q: 70.848976\n",
      " 617904/700000: episode: 1820, duration: 1.466s, episode steps: 292, steps per second: 199, episode reward: 254.497, mean reward: 0.872 [-4.036, 100.000], mean action: 0.894 [0.000, 3.000], mean observation: 0.125 [-1.158, 1.000], loss: 7.934279, mean_absolute_error: 52.960434, mean_q: 70.900017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 618030/700000: episode: 1821, duration: 0.635s, episode steps: 126, steps per second: 198, episode reward: 10.099, mean reward: 0.080 [-100.000, 11.038], mean action: 1.897 [0.000, 3.000], mean observation: 0.055 [-0.788, 1.353], loss: 4.661095, mean_absolute_error: 53.050598, mean_q: 70.900673\n",
      " 618292/700000: episode: 1822, duration: 1.346s, episode steps: 262, steps per second: 195, episode reward: 192.312, mean reward: 0.734 [-3.399, 100.000], mean action: 1.069 [0.000, 3.000], mean observation: 0.163 [-1.177, 1.000], loss: 11.065434, mean_absolute_error: 53.196468, mean_q: 70.923340\n",
      " 618447/700000: episode: 1823, duration: 0.774s, episode steps: 155, steps per second: 200, episode reward: 218.051, mean reward: 1.407 [-10.966, 100.000], mean action: 1.271 [0.000, 3.000], mean observation: 0.079 [-1.071, 1.000], loss: 10.546539, mean_absolute_error: 53.023159, mean_q: 70.926437\n",
      " 618858/700000: episode: 1824, duration: 2.138s, episode steps: 411, steps per second: 192, episode reward: 258.085, mean reward: 0.628 [-20.233, 100.000], mean action: 1.080 [0.000, 3.000], mean observation: 0.150 [-0.954, 1.000], loss: 9.540145, mean_absolute_error: 53.038006, mean_q: 70.843719\n",
      " 619032/700000: episode: 1825, duration: 0.875s, episode steps: 174, steps per second: 199, episode reward: 263.244, mean reward: 1.513 [-4.988, 100.000], mean action: 1.523 [0.000, 3.000], mean observation: 0.102 [-0.881, 1.000], loss: 12.983174, mean_absolute_error: 53.413822, mean_q: 71.266724\n",
      " 619255/700000: episode: 1826, duration: 1.122s, episode steps: 223, steps per second: 199, episode reward: 198.591, mean reward: 0.891 [-19.141, 100.000], mean action: 1.170 [0.000, 3.000], mean observation: 0.075 [-0.898, 1.000], loss: 9.368599, mean_absolute_error: 53.114113, mean_q: 71.082054\n",
      " 619540/700000: episode: 1827, duration: 1.456s, episode steps: 285, steps per second: 196, episode reward: 205.719, mean reward: 0.722 [-17.524, 100.000], mean action: 0.874 [0.000, 3.000], mean observation: 0.174 [-0.968, 1.000], loss: 6.010893, mean_absolute_error: 52.852955, mean_q: 70.757332\n",
      " 619754/700000: episode: 1828, duration: 1.149s, episode steps: 214, steps per second: 186, episode reward: 198.404, mean reward: 0.927 [-10.416, 100.000], mean action: 1.234 [0.000, 3.000], mean observation: 0.112 [-0.920, 1.000], loss: 12.805286, mean_absolute_error: 52.846489, mean_q: 70.707642\n",
      " 620275/700000: episode: 1829, duration: 2.945s, episode steps: 521, steps per second: 177, episode reward: 224.044, mean reward: 0.430 [-20.397, 100.000], mean action: 0.678 [0.000, 3.000], mean observation: 0.196 [-0.731, 1.000], loss: 10.298830, mean_absolute_error: 52.734577, mean_q: 70.325302\n",
      " 620587/700000: episode: 1830, duration: 1.653s, episode steps: 312, steps per second: 189, episode reward: 215.348, mean reward: 0.690 [-16.963, 100.000], mean action: 1.388 [0.000, 3.000], mean observation: 0.140 [-0.785, 1.000], loss: 10.406085, mean_absolute_error: 52.610916, mean_q: 70.175941\n",
      " 620783/700000: episode: 1831, duration: 1.120s, episode steps: 196, steps per second: 175, episode reward: 235.580, mean reward: 1.202 [-3.685, 100.000], mean action: 1.255 [0.000, 3.000], mean observation: 0.093 [-1.114, 1.015], loss: 4.419763, mean_absolute_error: 52.478741, mean_q: 70.568054\n",
      " 621026/700000: episode: 1832, duration: 1.352s, episode steps: 243, steps per second: 180, episode reward: 220.854, mean reward: 0.909 [-2.420, 100.000], mean action: 0.807 [0.000, 3.000], mean observation: 0.121 [-0.927, 1.000], loss: 7.471732, mean_absolute_error: 53.167950, mean_q: 71.172050\n",
      " 621405/700000: episode: 1833, duration: 2.173s, episode steps: 379, steps per second: 174, episode reward: 188.930, mean reward: 0.498 [-19.450, 100.000], mean action: 1.467 [0.000, 3.000], mean observation: 0.130 [-0.971, 1.000], loss: 10.649347, mean_absolute_error: 53.212784, mean_q: 71.251404\n",
      " 621739/700000: episode: 1834, duration: 1.733s, episode steps: 334, steps per second: 193, episode reward: 222.017, mean reward: 0.665 [-10.980, 100.000], mean action: 1.162 [0.000, 3.000], mean observation: 0.124 [-1.005, 1.000], loss: 8.136901, mean_absolute_error: 53.064686, mean_q: 71.129395\n",
      " 621976/700000: episode: 1835, duration: 1.193s, episode steps: 237, steps per second: 199, episode reward: 181.050, mean reward: 0.764 [-19.177, 100.000], mean action: 0.882 [0.000, 3.000], mean observation: 0.068 [-0.969, 1.250], loss: 7.933817, mean_absolute_error: 53.083424, mean_q: 70.941376\n",
      " 622844/700000: episode: 1836, duration: 4.985s, episode steps: 868, steps per second: 174, episode reward: 129.982, mean reward: 0.150 [-20.488, 100.000], mean action: 1.203 [0.000, 3.000], mean observation: 0.166 [-0.865, 1.000], loss: 9.565122, mean_absolute_error: 52.651985, mean_q: 70.295479\n",
      " 623111/700000: episode: 1837, duration: 1.406s, episode steps: 267, steps per second: 190, episode reward: 242.191, mean reward: 0.907 [-17.623, 100.000], mean action: 1.109 [0.000, 3.000], mean observation: 0.122 [-0.852, 1.000], loss: 13.423083, mean_absolute_error: 52.758602, mean_q: 70.392090\n",
      " 623413/700000: episode: 1838, duration: 1.541s, episode steps: 302, steps per second: 196, episode reward: 262.491, mean reward: 0.869 [-17.889, 100.000], mean action: 1.136 [0.000, 3.000], mean observation: 0.093 [-1.273, 1.000], loss: 9.574892, mean_absolute_error: 52.381866, mean_q: 70.026382\n",
      " 623815/700000: episode: 1839, duration: 2.097s, episode steps: 402, steps per second: 192, episode reward: 199.672, mean reward: 0.497 [-17.632, 100.000], mean action: 0.868 [0.000, 3.000], mean observation: 0.180 [-1.033, 1.000], loss: 8.707401, mean_absolute_error: 52.689793, mean_q: 70.330482\n",
      " 624815/700000: episode: 1840, duration: 5.395s, episode steps: 1000, steps per second: 185, episode reward: 113.388, mean reward: 0.113 [-19.028, 22.615], mean action: 1.771 [0.000, 3.000], mean observation: 0.186 [-1.007, 1.000], loss: 8.402652, mean_absolute_error: 52.558258, mean_q: 70.066719\n",
      " 625017/700000: episode: 1841, duration: 1.028s, episode steps: 202, steps per second: 196, episode reward: 257.852, mean reward: 1.276 [-10.522, 100.000], mean action: 1.411 [0.000, 3.000], mean observation: 0.022 [-0.768, 1.000], loss: 11.692849, mean_absolute_error: 52.638577, mean_q: 69.985832\n",
      " 625355/700000: episode: 1842, duration: 1.775s, episode steps: 338, steps per second: 190, episode reward: 226.477, mean reward: 0.670 [-14.457, 100.000], mean action: 1.491 [0.000, 3.000], mean observation: 0.070 [-0.646, 1.000], loss: 5.534065, mean_absolute_error: 52.187332, mean_q: 69.727989\n",
      " 625671/700000: episode: 1843, duration: 1.637s, episode steps: 316, steps per second: 193, episode reward: 244.049, mean reward: 0.772 [-18.490, 100.000], mean action: 0.927 [0.000, 3.000], mean observation: 0.141 [-0.730, 1.000], loss: 8.907610, mean_absolute_error: 52.464485, mean_q: 70.012482\n",
      " 626114/700000: episode: 1844, duration: 2.316s, episode steps: 443, steps per second: 191, episode reward: 199.184, mean reward: 0.450 [-17.630, 100.000], mean action: 0.767 [0.000, 3.000], mean observation: 0.152 [-0.933, 1.000], loss: 10.243343, mean_absolute_error: 52.189812, mean_q: 69.797760\n",
      " 626437/700000: episode: 1845, duration: 1.651s, episode steps: 323, steps per second: 196, episode reward: 243.684, mean reward: 0.754 [-9.669, 100.000], mean action: 1.223 [0.000, 3.000], mean observation: 0.067 [-0.967, 1.008], loss: 10.613228, mean_absolute_error: 52.732876, mean_q: 70.459465\n",
      " 626886/700000: episode: 1846, duration: 2.339s, episode steps: 449, steps per second: 192, episode reward: 239.183, mean reward: 0.533 [-17.735, 100.000], mean action: 1.118 [0.000, 3.000], mean observation: 0.152 [-0.871, 1.611], loss: 9.334738, mean_absolute_error: 52.717411, mean_q: 70.480881\n",
      " 627376/700000: episode: 1847, duration: 2.578s, episode steps: 490, steps per second: 190, episode reward: 214.485, mean reward: 0.438 [-18.083, 100.000], mean action: 0.741 [0.000, 3.000], mean observation: 0.170 [-0.917, 1.000], loss: 10.053093, mean_absolute_error: 52.286655, mean_q: 69.842392\n",
      " 627618/700000: episode: 1848, duration: 1.230s, episode steps: 242, steps per second: 197, episode reward: 242.661, mean reward: 1.003 [-18.115, 100.000], mean action: 1.066 [0.000, 3.000], mean observation: 0.141 [-0.757, 1.000], loss: 8.143090, mean_absolute_error: 52.622799, mean_q: 70.267548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 628003/700000: episode: 1849, duration: 1.978s, episode steps: 385, steps per second: 195, episode reward: 211.781, mean reward: 0.550 [-19.702, 100.000], mean action: 1.636 [0.000, 3.000], mean observation: 0.095 [-0.723, 1.123], loss: 8.246916, mean_absolute_error: 52.691593, mean_q: 70.586174\n",
      " 628110/700000: episode: 1850, duration: 0.539s, episode steps: 107, steps per second: 199, episode reward: -26.804, mean reward: -0.251 [-100.000, 14.988], mean action: 1.514 [0.000, 3.000], mean observation: -0.011 [-1.619, 1.000], loss: 19.540464, mean_absolute_error: 52.820564, mean_q: 70.702263\n",
      " 628267/700000: episode: 1851, duration: 0.796s, episode steps: 157, steps per second: 197, episode reward: 13.430, mean reward: 0.086 [-100.000, 32.689], mean action: 1.854 [0.000, 3.000], mean observation: 0.053 [-0.862, 1.229], loss: 6.778589, mean_absolute_error: 52.801849, mean_q: 70.853371\n",
      " 628539/700000: episode: 1852, duration: 1.373s, episode steps: 272, steps per second: 198, episode reward: 216.639, mean reward: 0.796 [-11.895, 100.000], mean action: 1.438 [0.000, 3.000], mean observation: 0.035 [-0.840, 1.000], loss: 12.156303, mean_absolute_error: 52.430664, mean_q: 70.423294\n",
      " 629230/700000: episode: 1853, duration: 3.743s, episode steps: 691, steps per second: 185, episode reward: 206.897, mean reward: 0.299 [-18.046, 100.000], mean action: 1.210 [0.000, 3.000], mean observation: 0.150 [-0.931, 1.000], loss: 8.063893, mean_absolute_error: 52.473747, mean_q: 70.413506\n",
      " 629719/700000: episode: 1854, duration: 2.545s, episode steps: 489, steps per second: 192, episode reward: 218.616, mean reward: 0.447 [-11.009, 100.000], mean action: 1.053 [0.000, 3.000], mean observation: 0.145 [-1.023, 1.252], loss: 7.218514, mean_absolute_error: 52.130543, mean_q: 69.884003\n",
      " 629950/700000: episode: 1855, duration: 1.164s, episode steps: 231, steps per second: 198, episode reward: 234.877, mean reward: 1.017 [-17.991, 100.000], mean action: 0.944 [0.000, 3.000], mean observation: 0.112 [-0.881, 1.045], loss: 8.396356, mean_absolute_error: 52.209152, mean_q: 70.057999\n",
      " 630397/700000: episode: 1856, duration: 2.441s, episode steps: 447, steps per second: 183, episode reward: 135.605, mean reward: 0.303 [-21.821, 100.000], mean action: 1.324 [0.000, 3.000], mean observation: 0.088 [-0.936, 1.562], loss: 9.850023, mean_absolute_error: 52.519970, mean_q: 70.412056\n",
      " 630838/700000: episode: 1857, duration: 2.363s, episode steps: 441, steps per second: 187, episode reward: 174.521, mean reward: 0.396 [-18.958, 100.000], mean action: 1.197 [0.000, 3.000], mean observation: 0.154 [-1.017, 1.000], loss: 8.645205, mean_absolute_error: 52.852200, mean_q: 70.708046\n",
      " 631150/700000: episode: 1858, duration: 1.616s, episode steps: 312, steps per second: 193, episode reward: 190.496, mean reward: 0.611 [-11.186, 100.000], mean action: 1.119 [0.000, 3.000], mean observation: 0.118 [-1.034, 1.000], loss: 7.130112, mean_absolute_error: 53.042244, mean_q: 71.119087\n",
      " 631672/700000: episode: 1859, duration: 2.708s, episode steps: 522, steps per second: 193, episode reward: -155.172, mean reward: -0.297 [-100.000, 13.613], mean action: 1.241 [0.000, 3.000], mean observation: 0.159 [-1.042, 1.114], loss: 8.187008, mean_absolute_error: 52.830433, mean_q: 70.582047\n",
      " 631925/700000: episode: 1860, duration: 1.303s, episode steps: 253, steps per second: 194, episode reward: 229.848, mean reward: 0.908 [-17.700, 100.000], mean action: 1.036 [0.000, 3.000], mean observation: 0.096 [-0.864, 1.000], loss: 9.512700, mean_absolute_error: 53.037643, mean_q: 71.168694\n",
      " 632298/700000: episode: 1861, duration: 1.904s, episode steps: 373, steps per second: 196, episode reward: 234.858, mean reward: 0.630 [-21.535, 100.000], mean action: 0.710 [0.000, 3.000], mean observation: 0.171 [-1.261, 1.000], loss: 6.741241, mean_absolute_error: 52.510094, mean_q: 70.304115\n",
      " 632885/700000: episode: 1862, duration: 3.031s, episode steps: 587, steps per second: 194, episode reward: 208.721, mean reward: 0.356 [-17.741, 100.000], mean action: 0.537 [0.000, 3.000], mean observation: 0.203 [-1.121, 1.136], loss: 9.691294, mean_absolute_error: 52.633823, mean_q: 70.454346\n",
      " 633162/700000: episode: 1863, duration: 1.411s, episode steps: 277, steps per second: 196, episode reward: 217.388, mean reward: 0.785 [-17.682, 100.000], mean action: 0.975 [0.000, 3.000], mean observation: 0.130 [-0.752, 1.114], loss: 5.595501, mean_absolute_error: 52.442192, mean_q: 70.490479\n",
      " 633401/700000: episode: 1864, duration: 1.208s, episode steps: 239, steps per second: 198, episode reward: 214.009, mean reward: 0.895 [-18.991, 100.000], mean action: 1.285 [0.000, 3.000], mean observation: 0.106 [-0.992, 1.000], loss: 14.381836, mean_absolute_error: 52.202072, mean_q: 69.910896\n",
      " 633591/700000: episode: 1865, duration: 0.946s, episode steps: 190, steps per second: 201, episode reward: 229.073, mean reward: 1.206 [-3.306, 100.000], mean action: 1.005 [0.000, 3.000], mean observation: 0.070 [-1.027, 1.000], loss: 6.636337, mean_absolute_error: 52.759899, mean_q: 70.650505\n",
      " 633697/700000: episode: 1866, duration: 0.542s, episode steps: 106, steps per second: 196, episode reward: 13.318, mean reward: 0.126 [-100.000, 16.846], mean action: 1.906 [0.000, 3.000], mean observation: 0.070 [-0.939, 1.000], loss: 10.846210, mean_absolute_error: 52.837120, mean_q: 70.555038\n",
      " 634144/700000: episode: 1867, duration: 2.268s, episode steps: 447, steps per second: 197, episode reward: 235.148, mean reward: 0.526 [-20.601, 100.000], mean action: 0.805 [0.000, 3.000], mean observation: 0.172 [-0.709, 1.368], loss: 9.496861, mean_absolute_error: 52.682682, mean_q: 70.486320\n",
      " 634381/700000: episode: 1868, duration: 1.204s, episode steps: 237, steps per second: 197, episode reward: 238.916, mean reward: 1.008 [-12.147, 100.000], mean action: 1.207 [0.000, 3.000], mean observation: 0.096 [-1.105, 1.000], loss: 6.885010, mean_absolute_error: 52.599476, mean_q: 70.361076\n",
      " 634699/700000: episode: 1869, duration: 1.616s, episode steps: 318, steps per second: 197, episode reward: 224.631, mean reward: 0.706 [-3.952, 100.000], mean action: 0.811 [0.000, 3.000], mean observation: 0.153 [-1.076, 1.156], loss: 9.856823, mean_absolute_error: 52.473541, mean_q: 70.097618\n",
      " 634956/700000: episode: 1870, duration: 1.307s, episode steps: 257, steps per second: 197, episode reward: 195.302, mean reward: 0.760 [-6.054, 100.000], mean action: 1.401 [0.000, 3.000], mean observation: 0.063 [-0.714, 1.000], loss: 8.199640, mean_absolute_error: 52.313061, mean_q: 70.079758\n",
      " 635248/700000: episode: 1871, duration: 1.483s, episode steps: 292, steps per second: 197, episode reward: 208.588, mean reward: 0.714 [-19.423, 100.000], mean action: 0.781 [0.000, 3.000], mean observation: 0.170 [-0.999, 1.264], loss: 8.793761, mean_absolute_error: 52.059460, mean_q: 69.622154\n",
      " 635459/700000: episode: 1872, duration: 1.062s, episode steps: 211, steps per second: 199, episode reward: 223.585, mean reward: 1.060 [-8.069, 100.000], mean action: 1.507 [0.000, 3.000], mean observation: 0.070 [-0.987, 1.000], loss: 7.474332, mean_absolute_error: 52.105072, mean_q: 69.853401\n",
      " 635799/700000: episode: 1873, duration: 1.774s, episode steps: 340, steps per second: 192, episode reward: 226.476, mean reward: 0.666 [-17.340, 100.000], mean action: 0.888 [0.000, 3.000], mean observation: 0.161 [-1.249, 1.000], loss: 12.120742, mean_absolute_error: 52.242928, mean_q: 69.771194\n",
      " 636039/700000: episode: 1874, duration: 1.199s, episode steps: 240, steps per second: 200, episode reward: 229.634, mean reward: 0.957 [-17.521, 100.000], mean action: 0.662 [0.000, 3.000], mean observation: 0.143 [-1.041, 1.000], loss: 7.356198, mean_absolute_error: 52.176685, mean_q: 69.912102\n",
      " 636254/700000: episode: 1875, duration: 1.083s, episode steps: 215, steps per second: 198, episode reward: 234.606, mean reward: 1.091 [-7.905, 100.000], mean action: 1.465 [0.000, 3.000], mean observation: 0.027 [-0.879, 1.000], loss: 8.388513, mean_absolute_error: 52.046822, mean_q: 69.829269\n",
      " 637254/700000: episode: 1876, duration: 5.446s, episode steps: 1000, steps per second: 184, episode reward: 110.892, mean reward: 0.111 [-20.352, 20.540], mean action: 0.811 [0.000, 3.000], mean observation: 0.224 [-0.865, 1.000], loss: 9.747188, mean_absolute_error: 52.372864, mean_q: 70.257706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 637432/700000: episode: 1877, duration: 0.892s, episode steps: 178, steps per second: 199, episode reward: 224.235, mean reward: 1.260 [-11.779, 100.000], mean action: 1.067 [0.000, 3.000], mean observation: 0.075 [-1.026, 1.209], loss: 7.325198, mean_absolute_error: 52.279057, mean_q: 70.097847\n",
      " 637646/700000: episode: 1878, duration: 1.089s, episode steps: 214, steps per second: 196, episode reward: 204.412, mean reward: 0.955 [-9.805, 100.000], mean action: 1.579 [0.000, 3.000], mean observation: 0.020 [-0.784, 1.000], loss: 11.639611, mean_absolute_error: 52.392578, mean_q: 70.042496\n",
      " 637870/700000: episode: 1879, duration: 1.149s, episode steps: 224, steps per second: 195, episode reward: 234.044, mean reward: 1.045 [-17.381, 100.000], mean action: 1.156 [0.000, 3.000], mean observation: 0.111 [-0.990, 1.098], loss: 11.046592, mean_absolute_error: 52.117664, mean_q: 69.799431\n",
      " 638040/700000: episode: 1880, duration: 0.852s, episode steps: 170, steps per second: 199, episode reward: 233.158, mean reward: 1.372 [-20.418, 100.000], mean action: 1.171 [0.000, 3.000], mean observation: 0.115 [-0.996, 1.000], loss: 7.729666, mean_absolute_error: 52.435089, mean_q: 70.240669\n",
      " 638366/700000: episode: 1881, duration: 1.662s, episode steps: 326, steps per second: 196, episode reward: 205.223, mean reward: 0.630 [-3.466, 100.000], mean action: 0.871 [0.000, 3.000], mean observation: 0.153 [-0.882, 1.000], loss: 6.807629, mean_absolute_error: 52.282204, mean_q: 69.967400\n",
      " 638455/700000: episode: 1882, duration: 0.453s, episode steps: 89, steps per second: 196, episode reward: -35.676, mean reward: -0.401 [-100.000, 18.456], mean action: 1.787 [0.000, 3.000], mean observation: -0.089 [-0.856, 1.782], loss: 8.128706, mean_absolute_error: 52.163383, mean_q: 69.967003\n",
      " 638828/700000: episode: 1883, duration: 1.902s, episode steps: 373, steps per second: 196, episode reward: 245.952, mean reward: 0.659 [-17.787, 100.000], mean action: 0.743 [0.000, 3.000], mean observation: 0.180 [-1.083, 1.021], loss: 5.857970, mean_absolute_error: 52.163967, mean_q: 69.994698\n",
      " 639066/700000: episode: 1884, duration: 1.216s, episode steps: 238, steps per second: 196, episode reward: 217.259, mean reward: 0.913 [-9.911, 100.000], mean action: 1.550 [0.000, 3.000], mean observation: 0.103 [-1.300, 1.000], loss: 7.576417, mean_absolute_error: 52.827965, mean_q: 70.941338\n",
      " 639196/700000: episode: 1885, duration: 0.647s, episode steps: 130, steps per second: 201, episode reward: -63.200, mean reward: -0.486 [-100.000, 15.372], mean action: 1.831 [0.000, 3.000], mean observation: -0.026 [-1.335, 1.000], loss: 12.840779, mean_absolute_error: 52.404041, mean_q: 70.340172\n",
      " 639345/700000: episode: 1886, duration: 0.746s, episode steps: 149, steps per second: 200, episode reward: -6.104, mean reward: -0.041 [-100.000, 14.901], mean action: 1.705 [0.000, 3.000], mean observation: 0.091 [-1.138, 1.000], loss: 15.431077, mean_absolute_error: 52.379105, mean_q: 70.205559\n",
      " 639647/700000: episode: 1887, duration: 1.559s, episode steps: 302, steps per second: 194, episode reward: 232.846, mean reward: 0.771 [-18.118, 100.000], mean action: 0.868 [0.000, 3.000], mean observation: 0.140 [-0.982, 1.000], loss: 11.574771, mean_absolute_error: 51.882683, mean_q: 69.347679\n",
      " 639815/700000: episode: 1888, duration: 0.843s, episode steps: 168, steps per second: 199, episode reward: 211.876, mean reward: 1.261 [-3.193, 100.000], mean action: 1.476 [0.000, 3.000], mean observation: 0.049 [-1.014, 1.000], loss: 7.541311, mean_absolute_error: 52.053211, mean_q: 69.927353\n",
      " 640048/700000: episode: 1889, duration: 1.188s, episode steps: 233, steps per second: 196, episode reward: 207.955, mean reward: 0.893 [-3.763, 100.000], mean action: 1.433 [0.000, 3.000], mean observation: 0.076 [-0.746, 1.000], loss: 11.511431, mean_absolute_error: 51.820789, mean_q: 69.204262\n",
      " 640261/700000: episode: 1890, duration: 1.053s, episode steps: 213, steps per second: 202, episode reward: 204.244, mean reward: 0.959 [-3.177, 100.000], mean action: 0.873 [0.000, 3.000], mean observation: 0.106 [-0.930, 1.000], loss: 5.720525, mean_absolute_error: 52.146915, mean_q: 70.076477\n",
      " 640454/700000: episode: 1891, duration: 0.959s, episode steps: 193, steps per second: 201, episode reward: 194.767, mean reward: 1.009 [-11.179, 100.000], mean action: 1.337 [0.000, 3.000], mean observation: 0.031 [-0.867, 1.000], loss: 7.745804, mean_absolute_error: 52.128391, mean_q: 69.900429\n",
      " 640696/700000: episode: 1892, duration: 1.217s, episode steps: 242, steps per second: 199, episode reward: 200.748, mean reward: 0.830 [-3.010, 100.000], mean action: 1.289 [0.000, 3.000], mean observation: 0.075 [-0.742, 1.000], loss: 14.605375, mean_absolute_error: 51.924026, mean_q: 69.314163\n",
      " 640942/700000: episode: 1893, duration: 1.232s, episode steps: 246, steps per second: 200, episode reward: 210.930, mean reward: 0.857 [-9.745, 100.000], mean action: 1.008 [0.000, 3.000], mean observation: 0.095 [-0.712, 1.000], loss: 8.844208, mean_absolute_error: 52.119743, mean_q: 69.814102\n",
      " 641040/700000: episode: 1894, duration: 0.493s, episode steps: 98, steps per second: 199, episode reward: -168.912, mean reward: -1.724 [-100.000, 15.398], mean action: 1.245 [0.000, 3.000], mean observation: 0.147 [-5.296, 1.000], loss: 12.671040, mean_absolute_error: 52.080143, mean_q: 70.082512\n",
      " 641409/700000: episode: 1895, duration: 1.913s, episode steps: 369, steps per second: 193, episode reward: 221.320, mean reward: 0.600 [-17.754, 100.000], mean action: 0.802 [0.000, 3.000], mean observation: 0.126 [-1.260, 1.000], loss: 6.258309, mean_absolute_error: 51.616646, mean_q: 69.185402\n",
      " 641623/700000: episode: 1896, duration: 1.083s, episode steps: 214, steps per second: 198, episode reward: 207.974, mean reward: 0.972 [-10.291, 100.000], mean action: 1.178 [0.000, 3.000], mean observation: 0.074 [-0.730, 1.000], loss: 8.550426, mean_absolute_error: 52.160713, mean_q: 69.843803\n",
      " 642152/700000: episode: 1897, duration: 2.741s, episode steps: 529, steps per second: 193, episode reward: 198.550, mean reward: 0.375 [-19.338, 100.000], mean action: 0.682 [0.000, 3.000], mean observation: 0.166 [-0.856, 1.000], loss: 8.368526, mean_absolute_error: 52.328033, mean_q: 70.184425\n",
      " 642546/700000: episode: 1898, duration: 2.079s, episode steps: 394, steps per second: 190, episode reward: 222.984, mean reward: 0.566 [-18.318, 100.000], mean action: 0.939 [0.000, 3.000], mean observation: 0.122 [-1.768, 1.000], loss: 8.914306, mean_absolute_error: 51.899693, mean_q: 69.531647\n",
      " 642788/700000: episode: 1899, duration: 1.380s, episode steps: 242, steps per second: 175, episode reward: 237.523, mean reward: 0.981 [-17.506, 100.000], mean action: 1.302 [0.000, 3.000], mean observation: 0.092 [-0.868, 1.014], loss: 8.007128, mean_absolute_error: 52.140778, mean_q: 69.885605\n",
      " 643006/700000: episode: 1900, duration: 1.186s, episode steps: 218, steps per second: 184, episode reward: 223.941, mean reward: 1.027 [-7.444, 100.000], mean action: 1.138 [0.000, 3.000], mean observation: 0.085 [-1.007, 1.129], loss: 10.487831, mean_absolute_error: 52.539429, mean_q: 70.423996\n",
      " 643815/700000: episode: 1901, duration: 4.715s, episode steps: 809, steps per second: 172, episode reward: 140.249, mean reward: 0.173 [-20.872, 100.000], mean action: 1.508 [0.000, 3.000], mean observation: 0.140 [-0.866, 1.539], loss: 11.246675, mean_absolute_error: 52.494755, mean_q: 70.358871\n",
      " 643903/700000: episode: 1902, duration: 0.470s, episode steps: 88, steps per second: 187, episode reward: -176.802, mean reward: -2.009 [-100.000, 10.653], mean action: 1.841 [0.000, 3.000], mean observation: -0.183 [-3.672, 1.000], loss: 11.306183, mean_absolute_error: 52.184036, mean_q: 69.952087\n",
      " 644200/700000: episode: 1903, duration: 1.559s, episode steps: 297, steps per second: 190, episode reward: 226.785, mean reward: 0.764 [-17.565, 100.000], mean action: 0.980 [0.000, 3.000], mean observation: 0.124 [-0.866, 1.000], loss: 5.794861, mean_absolute_error: 52.211926, mean_q: 69.731339\n",
      " 644473/700000: episode: 1904, duration: 1.376s, episode steps: 273, steps per second: 198, episode reward: 203.827, mean reward: 0.747 [-4.301, 100.000], mean action: 0.905 [0.000, 3.000], mean observation: 0.126 [-0.724, 1.000], loss: 9.636558, mean_absolute_error: 52.553806, mean_q: 70.170914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 644666/700000: episode: 1905, duration: 0.981s, episode steps: 193, steps per second: 197, episode reward: 184.159, mean reward: 0.954 [-9.542, 100.000], mean action: 1.280 [0.000, 3.000], mean observation: 0.053 [-0.867, 1.000], loss: 11.254116, mean_absolute_error: 52.677807, mean_q: 70.625618\n",
      " 644913/700000: episode: 1906, duration: 1.251s, episode steps: 247, steps per second: 197, episode reward: 212.966, mean reward: 0.862 [-17.333, 100.000], mean action: 1.283 [0.000, 3.000], mean observation: 0.111 [-1.065, 1.000], loss: 7.600597, mean_absolute_error: 52.630241, mean_q: 70.609993\n",
      " 645314/700000: episode: 1907, duration: 2.098s, episode steps: 401, steps per second: 191, episode reward: 211.065, mean reward: 0.526 [-19.467, 100.000], mean action: 0.820 [0.000, 3.000], mean observation: 0.169 [-0.906, 1.000], loss: 13.655714, mean_absolute_error: 52.425171, mean_q: 70.186035\n",
      " 645510/700000: episode: 1908, duration: 0.983s, episode steps: 196, steps per second: 199, episode reward: 241.238, mean reward: 1.231 [-6.730, 100.000], mean action: 1.224 [0.000, 3.000], mean observation: 0.109 [-1.120, 1.000], loss: 8.352345, mean_absolute_error: 52.073792, mean_q: 69.888306\n",
      " 645614/700000: episode: 1909, duration: 0.527s, episode steps: 104, steps per second: 197, episode reward: -34.724, mean reward: -0.334 [-100.000, 12.588], mean action: 1.865 [0.000, 3.000], mean observation: -0.021 [-1.047, 1.640], loss: 8.242989, mean_absolute_error: 51.944218, mean_q: 69.726089\n",
      " 645924/700000: episode: 1910, duration: 1.584s, episode steps: 310, steps per second: 196, episode reward: 261.821, mean reward: 0.845 [-11.125, 100.000], mean action: 1.087 [0.000, 3.000], mean observation: 0.127 [-0.851, 1.000], loss: 10.312213, mean_absolute_error: 52.256683, mean_q: 70.068825\n",
      " 646137/700000: episode: 1911, duration: 1.077s, episode steps: 213, steps per second: 198, episode reward: 241.713, mean reward: 1.135 [-17.519, 100.000], mean action: 0.878 [0.000, 3.000], mean observation: 0.110 [-0.869, 1.150], loss: 10.934779, mean_absolute_error: 52.322128, mean_q: 70.091057\n",
      " 646291/700000: episode: 1912, duration: 0.768s, episode steps: 154, steps per second: 200, episode reward: 237.342, mean reward: 1.541 [-3.324, 100.000], mean action: 1.377 [0.000, 3.000], mean observation: 0.064 [-0.841, 1.000], loss: 7.007052, mean_absolute_error: 52.385208, mean_q: 70.274834\n",
      " 646583/700000: episode: 1913, duration: 1.492s, episode steps: 292, steps per second: 196, episode reward: 202.885, mean reward: 0.695 [-15.850, 100.000], mean action: 1.387 [0.000, 3.000], mean observation: 0.110 [-1.121, 1.000], loss: 10.186989, mean_absolute_error: 52.665779, mean_q: 70.588638\n",
      " 646680/700000: episode: 1914, duration: 0.486s, episode steps: 97, steps per second: 200, episode reward: -51.317, mean reward: -0.529 [-100.000, 9.728], mean action: 1.371 [0.000, 3.000], mean observation: -0.071 [-1.074, 1.830], loss: 21.352297, mean_absolute_error: 51.874744, mean_q: 69.522499\n",
      " 646804/700000: episode: 1915, duration: 0.633s, episode steps: 124, steps per second: 196, episode reward: -43.641, mean reward: -0.352 [-100.000, 17.774], mean action: 1.677 [0.000, 3.000], mean observation: -0.031 [-0.845, 1.507], loss: 7.054930, mean_absolute_error: 52.625603, mean_q: 70.531738\n",
      " 647085/700000: episode: 1916, duration: 1.446s, episode steps: 281, steps per second: 194, episode reward: 218.442, mean reward: 0.777 [-8.466, 100.000], mean action: 1.306 [0.000, 3.000], mean observation: 0.115 [-0.738, 1.000], loss: 7.729127, mean_absolute_error: 52.093029, mean_q: 69.745216\n",
      " 647306/700000: episode: 1917, duration: 1.109s, episode steps: 221, steps per second: 199, episode reward: 206.720, mean reward: 0.935 [-20.229, 100.000], mean action: 1.163 [0.000, 3.000], mean observation: 0.086 [-0.773, 1.000], loss: 13.925447, mean_absolute_error: 52.796265, mean_q: 70.548347\n",
      " 647469/700000: episode: 1918, duration: 0.818s, episode steps: 163, steps per second: 199, episode reward: -18.591, mean reward: -0.114 [-100.000, 21.391], mean action: 1.957 [0.000, 3.000], mean observation: 0.085 [-0.697, 1.817], loss: 12.387969, mean_absolute_error: 52.560116, mean_q: 70.367760\n",
      " 647806/700000: episode: 1919, duration: 1.793s, episode steps: 337, steps per second: 188, episode reward: 234.329, mean reward: 0.695 [-19.238, 100.000], mean action: 1.864 [0.000, 3.000], mean observation: 0.069 [-0.863, 1.014], loss: 9.210662, mean_absolute_error: 52.309174, mean_q: 70.112968\n",
      " 648175/700000: episode: 1920, duration: 1.888s, episode steps: 369, steps per second: 195, episode reward: 226.507, mean reward: 0.614 [-18.622, 100.000], mean action: 1.190 [0.000, 3.000], mean observation: 0.150 [-0.798, 1.000], loss: 9.121076, mean_absolute_error: 52.091515, mean_q: 69.694138\n",
      " 648362/700000: episode: 1921, duration: 0.944s, episode steps: 187, steps per second: 198, episode reward: 242.765, mean reward: 1.298 [-9.376, 100.000], mean action: 1.342 [0.000, 3.000], mean observation: 0.092 [-0.985, 1.000], loss: 6.886706, mean_absolute_error: 52.349598, mean_q: 70.200172\n",
      " 648651/700000: episode: 1922, duration: 1.473s, episode steps: 289, steps per second: 196, episode reward: 222.012, mean reward: 0.768 [-11.992, 100.000], mean action: 0.976 [0.000, 3.000], mean observation: 0.162 [-0.751, 1.133], loss: 12.154160, mean_absolute_error: 52.262455, mean_q: 70.151382\n",
      " 648837/700000: episode: 1923, duration: 0.931s, episode steps: 186, steps per second: 200, episode reward: 234.085, mean reward: 1.259 [-3.355, 100.000], mean action: 1.134 [0.000, 3.000], mean observation: 0.082 [-0.834, 1.000], loss: 7.432076, mean_absolute_error: 52.727360, mean_q: 70.770081\n",
      " 649395/700000: episode: 1924, duration: 2.975s, episode steps: 558, steps per second: 188, episode reward: 138.539, mean reward: 0.248 [-20.360, 100.000], mean action: 1.405 [0.000, 3.000], mean observation: 0.232 [-0.788, 1.000], loss: 8.671453, mean_absolute_error: 52.454041, mean_q: 70.412834\n",
      " 649614/700000: episode: 1925, duration: 1.109s, episode steps: 219, steps per second: 197, episode reward: 221.181, mean reward: 1.010 [-10.239, 100.000], mean action: 1.388 [0.000, 3.000], mean observation: 0.082 [-1.022, 1.000], loss: 10.359778, mean_absolute_error: 52.904591, mean_q: 71.018059\n",
      " 649968/700000: episode: 1926, duration: 1.828s, episode steps: 354, steps per second: 194, episode reward: 228.618, mean reward: 0.646 [-5.587, 100.000], mean action: 1.356 [0.000, 3.000], mean observation: 0.096 [-0.657, 1.018], loss: 9.302808, mean_absolute_error: 52.591541, mean_q: 70.628540\n",
      " 650968/700000: episode: 1927, duration: 5.725s, episode steps: 1000, steps per second: 175, episode reward: 78.398, mean reward: 0.078 [-17.645, 22.290], mean action: 2.581 [0.000, 3.000], mean observation: 0.173 [-0.697, 1.000], loss: 11.092975, mean_absolute_error: 52.547180, mean_q: 70.583763\n",
      " 651139/700000: episode: 1928, duration: 0.850s, episode steps: 171, steps per second: 201, episode reward: 193.438, mean reward: 1.131 [-6.921, 100.000], mean action: 1.158 [0.000, 3.000], mean observation: 0.072 [-0.752, 1.000], loss: 7.044024, mean_absolute_error: 53.022602, mean_q: 71.041870\n",
      " 651416/700000: episode: 1929, duration: 1.395s, episode steps: 277, steps per second: 199, episode reward: 244.858, mean reward: 0.884 [-9.261, 100.000], mean action: 1.014 [0.000, 3.000], mean observation: 0.129 [-0.922, 1.242], loss: 7.823532, mean_absolute_error: 53.353516, mean_q: 71.616692\n",
      " 651709/700000: episode: 1930, duration: 1.475s, episode steps: 293, steps per second: 199, episode reward: 237.444, mean reward: 0.810 [-11.964, 100.000], mean action: 0.863 [0.000, 3.000], mean observation: 0.116 [-0.809, 1.000], loss: 7.656903, mean_absolute_error: 52.804321, mean_q: 70.887337\n",
      " 651995/700000: episode: 1931, duration: 1.484s, episode steps: 286, steps per second: 193, episode reward: 249.583, mean reward: 0.873 [-19.344, 100.000], mean action: 1.150 [0.000, 3.000], mean observation: 0.090 [-0.917, 1.043], loss: 7.296479, mean_absolute_error: 52.986130, mean_q: 71.310753\n",
      " 652167/700000: episode: 1932, duration: 0.868s, episode steps: 172, steps per second: 198, episode reward: 215.717, mean reward: 1.254 [-9.181, 100.000], mean action: 1.413 [0.000, 3.000], mean observation: 0.057 [-0.902, 1.000], loss: 5.561063, mean_absolute_error: 53.283855, mean_q: 71.430367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 652386/700000: episode: 1933, duration: 1.102s, episode steps: 219, steps per second: 199, episode reward: 207.249, mean reward: 0.946 [-18.454, 100.000], mean action: 1.205 [0.000, 3.000], mean observation: 0.107 [-0.815, 1.000], loss: 8.541140, mean_absolute_error: 53.160049, mean_q: 71.187187\n",
      " 652627/700000: episode: 1934, duration: 1.414s, episode steps: 241, steps per second: 170, episode reward: 201.033, mean reward: 0.834 [-9.039, 100.000], mean action: 1.178 [0.000, 3.000], mean observation: 0.086 [-0.827, 1.000], loss: 6.795444, mean_absolute_error: 53.693745, mean_q: 72.208473\n",
      " 652912/700000: episode: 1935, duration: 1.477s, episode steps: 285, steps per second: 193, episode reward: 224.560, mean reward: 0.788 [-17.352, 100.000], mean action: 1.502 [0.000, 3.000], mean observation: 0.102 [-0.671, 1.000], loss: 9.466476, mean_absolute_error: 53.104847, mean_q: 71.344833\n",
      " 653082/700000: episode: 1936, duration: 0.854s, episode steps: 170, steps per second: 199, episode reward: 239.081, mean reward: 1.406 [-3.084, 100.000], mean action: 1.435 [0.000, 3.000], mean observation: 0.055 [-0.745, 1.000], loss: 7.025774, mean_absolute_error: 53.305656, mean_q: 71.819839\n",
      " 653251/700000: episode: 1937, duration: 0.843s, episode steps: 169, steps per second: 201, episode reward: 197.453, mean reward: 1.168 [-2.625, 100.000], mean action: 1.349 [0.000, 3.000], mean observation: 0.046 [-0.780, 1.000], loss: 8.958547, mean_absolute_error: 53.298214, mean_q: 71.641319\n",
      " 653504/700000: episode: 1938, duration: 1.277s, episode steps: 253, steps per second: 198, episode reward: 200.482, mean reward: 0.792 [-21.178, 100.000], mean action: 1.059 [0.000, 3.000], mean observation: 0.073 [-0.884, 1.000], loss: 6.981754, mean_absolute_error: 53.122051, mean_q: 71.279121\n",
      " 653784/700000: episode: 1939, duration: 1.414s, episode steps: 280, steps per second: 198, episode reward: 242.080, mean reward: 0.865 [-3.893, 100.000], mean action: 1.275 [0.000, 3.000], mean observation: 0.092 [-0.819, 1.024], loss: 14.290540, mean_absolute_error: 53.269005, mean_q: 71.716103\n",
      " 654199/700000: episode: 1940, duration: 2.126s, episode steps: 415, steps per second: 195, episode reward: 238.259, mean reward: 0.574 [-19.461, 100.000], mean action: 0.737 [0.000, 3.000], mean observation: 0.190 [-1.039, 1.000], loss: 7.873580, mean_absolute_error: 53.742367, mean_q: 72.365791\n",
      " 654433/700000: episode: 1941, duration: 1.174s, episode steps: 234, steps per second: 199, episode reward: 228.905, mean reward: 0.978 [-11.413, 100.000], mean action: 1.282 [0.000, 3.000], mean observation: 0.064 [-0.734, 1.016], loss: 8.843351, mean_absolute_error: 53.300396, mean_q: 71.452148\n",
      " 654581/700000: episode: 1942, duration: 0.744s, episode steps: 148, steps per second: 199, episode reward: -128.084, mean reward: -0.865 [-100.000, 18.755], mean action: 1.750 [0.000, 3.000], mean observation: -0.080 [-0.905, 1.844], loss: 8.206441, mean_absolute_error: 52.826851, mean_q: 70.915871\n",
      " 654864/700000: episode: 1943, duration: 1.435s, episode steps: 283, steps per second: 197, episode reward: 198.471, mean reward: 0.701 [-17.338, 100.000], mean action: 1.035 [0.000, 3.000], mean observation: 0.114 [-0.790, 1.000], loss: 7.681427, mean_absolute_error: 53.277496, mean_q: 71.555321\n",
      " 655210/700000: episode: 1944, duration: 1.800s, episode steps: 346, steps per second: 192, episode reward: 262.122, mean reward: 0.758 [-10.742, 100.000], mean action: 1.130 [0.000, 3.000], mean observation: 0.128 [-0.692, 1.003], loss: 8.149405, mean_absolute_error: 53.132202, mean_q: 71.419456\n",
      " 655450/700000: episode: 1945, duration: 1.218s, episode steps: 240, steps per second: 197, episode reward: 207.881, mean reward: 0.866 [-4.270, 100.000], mean action: 1.288 [0.000, 3.000], mean observation: 0.087 [-0.660, 1.000], loss: 7.542224, mean_absolute_error: 53.148216, mean_q: 71.305580\n",
      " 655647/700000: episode: 1946, duration: 0.994s, episode steps: 197, steps per second: 198, episode reward: 223.058, mean reward: 1.132 [-9.071, 100.000], mean action: 1.157 [0.000, 3.000], mean observation: 0.064 [-1.129, 1.000], loss: 7.481024, mean_absolute_error: 53.158962, mean_q: 71.226723\n",
      " 655906/700000: episode: 1947, duration: 1.317s, episode steps: 259, steps per second: 197, episode reward: 242.764, mean reward: 0.937 [-15.379, 100.000], mean action: 1.259 [0.000, 3.000], mean observation: 0.066 [-0.952, 1.000], loss: 8.057592, mean_absolute_error: 53.314583, mean_q: 71.661423\n",
      " 656304/700000: episode: 1948, duration: 2.056s, episode steps: 398, steps per second: 194, episode reward: 226.166, mean reward: 0.568 [-17.781, 100.000], mean action: 0.786 [0.000, 3.000], mean observation: 0.145 [-0.975, 1.000], loss: 9.053838, mean_absolute_error: 53.144268, mean_q: 71.403137\n",
      " 656459/700000: episode: 1949, duration: 0.788s, episode steps: 155, steps per second: 197, episode reward: -204.539, mean reward: -1.320 [-100.000, 51.800], mean action: 1.910 [0.000, 3.000], mean observation: -0.118 [-1.959, 1.000], loss: 8.273154, mean_absolute_error: 53.263935, mean_q: 71.537239\n",
      " 656732/700000: episode: 1950, duration: 1.395s, episode steps: 273, steps per second: 196, episode reward: 215.992, mean reward: 0.791 [-18.036, 100.000], mean action: 0.905 [0.000, 3.000], mean observation: 0.093 [-0.825, 1.000], loss: 7.611545, mean_absolute_error: 53.040237, mean_q: 71.335739\n",
      " 657038/700000: episode: 1951, duration: 1.567s, episode steps: 306, steps per second: 195, episode reward: 205.435, mean reward: 0.671 [-18.553, 100.000], mean action: 0.712 [0.000, 3.000], mean observation: 0.135 [-0.919, 1.000], loss: 7.687218, mean_absolute_error: 53.459507, mean_q: 71.923470\n",
      " 657178/700000: episode: 1952, duration: 0.706s, episode steps: 140, steps per second: 198, episode reward: -252.462, mean reward: -1.803 [-100.000, 27.967], mean action: 1.643 [0.000, 3.000], mean observation: -0.183 [-2.018, 1.000], loss: 8.323745, mean_absolute_error: 53.503014, mean_q: 72.045334\n",
      " 657355/700000: episode: 1953, duration: 0.907s, episode steps: 177, steps per second: 195, episode reward: 241.506, mean reward: 1.364 [-11.861, 100.000], mean action: 2.181 [0.000, 3.000], mean observation: 0.018 [-0.809, 1.000], loss: 4.877346, mean_absolute_error: 53.832912, mean_q: 72.479622\n",
      " 657476/700000: episode: 1954, duration: 0.605s, episode steps: 121, steps per second: 200, episode reward: -151.698, mean reward: -1.254 [-100.000, 21.431], mean action: 1.702 [0.000, 3.000], mean observation: -0.134 [-3.942, 1.000], loss: 5.631757, mean_absolute_error: 54.129662, mean_q: 72.756699\n",
      " 657668/700000: episode: 1955, duration: 0.964s, episode steps: 192, steps per second: 199, episode reward: 203.232, mean reward: 1.059 [-19.401, 100.000], mean action: 1.167 [0.000, 3.000], mean observation: 0.076 [-0.879, 1.039], loss: 10.231671, mean_absolute_error: 53.618378, mean_q: 71.857704\n",
      " 657878/700000: episode: 1956, duration: 1.064s, episode steps: 210, steps per second: 197, episode reward: 216.693, mean reward: 1.032 [-10.336, 100.000], mean action: 1.262 [0.000, 3.000], mean observation: 0.086 [-0.883, 1.000], loss: 4.165800, mean_absolute_error: 53.376873, mean_q: 71.707237\n",
      " 658878/700000: episode: 1957, duration: 5.500s, episode steps: 1000, steps per second: 182, episode reward: 133.540, mean reward: 0.134 [-18.985, 22.135], mean action: 1.009 [0.000, 3.000], mean observation: 0.188 [-0.839, 1.000], loss: 8.116624, mean_absolute_error: 53.924095, mean_q: 72.425262\n",
      " 659266/700000: episode: 1958, duration: 2.024s, episode steps: 388, steps per second: 192, episode reward: 259.810, mean reward: 0.670 [-19.076, 100.000], mean action: 1.186 [0.000, 3.000], mean observation: 0.154 [-0.801, 1.000], loss: 8.887739, mean_absolute_error: 54.334610, mean_q: 72.923569\n",
      " 659525/700000: episode: 1959, duration: 1.314s, episode steps: 259, steps per second: 197, episode reward: 233.297, mean reward: 0.901 [-12.599, 100.000], mean action: 1.012 [0.000, 3.000], mean observation: 0.083 [-0.878, 1.000], loss: 9.535243, mean_absolute_error: 53.829453, mean_q: 71.969666\n",
      " 659718/700000: episode: 1960, duration: 0.967s, episode steps: 193, steps per second: 200, episode reward: 215.363, mean reward: 1.116 [-8.006, 100.000], mean action: 1.181 [0.000, 3.000], mean observation: 0.084 [-0.952, 1.000], loss: 8.196461, mean_absolute_error: 54.406223, mean_q: 73.050110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 660004/700000: episode: 1961, duration: 1.442s, episode steps: 286, steps per second: 198, episode reward: 247.865, mean reward: 0.867 [-17.553, 100.000], mean action: 1.318 [0.000, 3.000], mean observation: 0.114 [-0.718, 1.012], loss: 8.406827, mean_absolute_error: 54.147575, mean_q: 72.527893\n",
      " 660203/700000: episode: 1962, duration: 1.002s, episode steps: 199, steps per second: 199, episode reward: 244.520, mean reward: 1.229 [-11.447, 100.000], mean action: 1.482 [0.000, 3.000], mean observation: 0.085 [-0.681, 1.000], loss: 12.867167, mean_absolute_error: 54.154610, mean_q: 72.535881\n",
      " 660390/700000: episode: 1963, duration: 0.936s, episode steps: 187, steps per second: 200, episode reward: 224.952, mean reward: 1.203 [-2.758, 100.000], mean action: 1.332 [0.000, 3.000], mean observation: 0.076 [-0.687, 1.000], loss: 7.044367, mean_absolute_error: 54.725246, mean_q: 73.115944\n",
      " 660588/700000: episode: 1964, duration: 1.001s, episode steps: 198, steps per second: 198, episode reward: 228.308, mean reward: 1.153 [-10.412, 100.000], mean action: 1.227 [0.000, 3.000], mean observation: -0.009 [-1.133, 1.000], loss: 4.367590, mean_absolute_error: 55.009014, mean_q: 73.945755\n",
      " 660949/700000: episode: 1965, duration: 1.895s, episode steps: 361, steps per second: 191, episode reward: 232.253, mean reward: 0.643 [-18.138, 100.000], mean action: 1.025 [0.000, 3.000], mean observation: 0.116 [-0.589, 1.000], loss: 15.570239, mean_absolute_error: 54.602055, mean_q: 73.145821\n",
      " 661236/700000: episode: 1966, duration: 1.459s, episode steps: 287, steps per second: 197, episode reward: 261.520, mean reward: 0.911 [-24.631, 100.000], mean action: 0.923 [0.000, 3.000], mean observation: 0.150 [-0.935, 1.037], loss: 8.213007, mean_absolute_error: 54.282242, mean_q: 72.869087\n",
      " 661495/700000: episode: 1967, duration: 1.311s, episode steps: 259, steps per second: 198, episode reward: 257.501, mean reward: 0.994 [-6.841, 100.000], mean action: 1.116 [0.000, 3.000], mean observation: 0.083 [-0.861, 1.000], loss: 10.272747, mean_absolute_error: 54.550056, mean_q: 72.999092\n",
      " 661578/700000: episode: 1968, duration: 0.422s, episode steps: 83, steps per second: 197, episode reward: 17.630, mean reward: 0.212 [-100.000, 20.389], mean action: 1.831 [0.000, 3.000], mean observation: 0.040 [-1.608, 1.000], loss: 6.301656, mean_absolute_error: 55.198101, mean_q: 74.445053\n",
      " 662008/700000: episode: 1969, duration: 2.234s, episode steps: 430, steps per second: 192, episode reward: 245.240, mean reward: 0.570 [-11.568, 100.000], mean action: 1.256 [0.000, 3.000], mean observation: 0.122 [-1.148, 1.000], loss: 9.468906, mean_absolute_error: 55.295113, mean_q: 74.036331\n",
      " 662214/700000: episode: 1970, duration: 1.038s, episode steps: 206, steps per second: 198, episode reward: 225.510, mean reward: 1.095 [-9.482, 100.000], mean action: 1.359 [0.000, 3.000], mean observation: 0.087 [-0.926, 1.000], loss: 7.443925, mean_absolute_error: 54.468426, mean_q: 73.212959\n",
      " 662465/700000: episode: 1971, duration: 1.311s, episode steps: 251, steps per second: 192, episode reward: 204.131, mean reward: 0.813 [-10.569, 100.000], mean action: 1.614 [0.000, 3.000], mean observation: 0.070 [-1.039, 1.000], loss: 8.305995, mean_absolute_error: 54.562965, mean_q: 73.159752\n",
      " 662670/700000: episode: 1972, duration: 1.051s, episode steps: 205, steps per second: 195, episode reward: 249.632, mean reward: 1.218 [-3.920, 100.000], mean action: 1.263 [0.000, 3.000], mean observation: 0.048 [-1.091, 1.000], loss: 7.838423, mean_absolute_error: 54.881126, mean_q: 73.812279\n",
      " 662943/700000: episode: 1973, duration: 1.397s, episode steps: 273, steps per second: 195, episode reward: 233.533, mean reward: 0.855 [-9.833, 100.000], mean action: 1.147 [0.000, 3.000], mean observation: 0.114 [-0.922, 1.026], loss: 6.359516, mean_absolute_error: 54.837814, mean_q: 73.890495\n",
      " 663058/700000: episode: 1974, duration: 0.581s, episode steps: 115, steps per second: 198, episode reward: -6.073, mean reward: -0.053 [-100.000, 11.649], mean action: 1.896 [0.000, 3.000], mean observation: 0.055 [-0.811, 1.000], loss: 13.361665, mean_absolute_error: 54.548878, mean_q: 73.074532\n",
      " 663287/700000: episode: 1975, duration: 1.147s, episode steps: 229, steps per second: 200, episode reward: 202.899, mean reward: 0.886 [-3.214, 100.000], mean action: 1.175 [0.000, 3.000], mean observation: 0.108 [-0.897, 1.000], loss: 6.877117, mean_absolute_error: 54.788406, mean_q: 73.625595\n",
      " 663587/700000: episode: 1976, duration: 1.511s, episode steps: 300, steps per second: 199, episode reward: 261.503, mean reward: 0.872 [-10.229, 100.000], mean action: 0.970 [0.000, 3.000], mean observation: 0.144 [-0.838, 1.000], loss: 8.278638, mean_absolute_error: 55.042866, mean_q: 73.838898\n",
      " 663866/700000: episode: 1977, duration: 1.427s, episode steps: 279, steps per second: 196, episode reward: 231.480, mean reward: 0.830 [-8.877, 100.000], mean action: 1.348 [0.000, 3.000], mean observation: 0.072 [-0.681, 1.000], loss: 8.438575, mean_absolute_error: 55.241257, mean_q: 74.320076\n",
      " 664324/700000: episode: 1978, duration: 2.355s, episode steps: 458, steps per second: 194, episode reward: 193.336, mean reward: 0.422 [-18.430, 100.000], mean action: 0.539 [0.000, 3.000], mean observation: 0.179 [-0.954, 1.000], loss: 5.569487, mean_absolute_error: 54.876484, mean_q: 73.755417\n",
      " 664480/700000: episode: 1979, duration: 0.778s, episode steps: 156, steps per second: 200, episode reward: 5.122, mean reward: 0.033 [-100.000, 10.862], mean action: 1.705 [0.000, 3.000], mean observation: -0.031 [-0.845, 1.023], loss: 7.428103, mean_absolute_error: 55.345127, mean_q: 74.201469\n",
      " 665358/700000: episode: 1980, duration: 4.698s, episode steps: 878, steps per second: 187, episode reward: 215.256, mean reward: 0.245 [-21.515, 100.000], mean action: 1.200 [0.000, 3.000], mean observation: 0.222 [-0.984, 1.001], loss: 8.551719, mean_absolute_error: 54.589931, mean_q: 73.378151\n",
      " 665449/700000: episode: 1981, duration: 0.462s, episode steps: 91, steps per second: 197, episode reward: -42.614, mean reward: -0.468 [-100.000, 17.382], mean action: 1.615 [0.000, 3.000], mean observation: -0.001 [-1.157, 1.000], loss: 15.453316, mean_absolute_error: 55.102379, mean_q: 73.805405\n",
      " 665590/700000: episode: 1982, duration: 0.810s, episode steps: 141, steps per second: 174, episode reward: -36.413, mean reward: -0.258 [-100.000, 18.869], mean action: 1.546 [0.000, 3.000], mean observation: 0.100 [-1.452, 1.000], loss: 8.265283, mean_absolute_error: 54.878532, mean_q: 73.423027\n",
      " 665777/700000: episode: 1983, duration: 0.981s, episode steps: 187, steps per second: 191, episode reward: 251.461, mean reward: 1.345 [-10.925, 100.000], mean action: 1.219 [0.000, 3.000], mean observation: 0.112 [-0.800, 1.314], loss: 9.724236, mean_absolute_error: 54.768585, mean_q: 73.042480\n",
      " 666027/700000: episode: 1984, duration: 1.404s, episode steps: 250, steps per second: 178, episode reward: 235.570, mean reward: 0.942 [-4.600, 100.000], mean action: 1.460 [0.000, 3.000], mean observation: 0.085 [-0.747, 1.173], loss: 9.286643, mean_absolute_error: 54.719685, mean_q: 73.218346\n",
      " 666319/700000: episode: 1985, duration: 1.528s, episode steps: 292, steps per second: 191, episode reward: 241.959, mean reward: 0.829 [-17.530, 100.000], mean action: 1.240 [0.000, 3.000], mean observation: 0.109 [-0.879, 1.000], loss: 12.433088, mean_absolute_error: 54.490826, mean_q: 73.019279\n",
      " 666550/700000: episode: 1986, duration: 1.212s, episode steps: 231, steps per second: 191, episode reward: 245.298, mean reward: 1.062 [-19.834, 100.000], mean action: 1.009 [0.000, 3.000], mean observation: 0.114 [-0.962, 1.000], loss: 10.243567, mean_absolute_error: 54.516937, mean_q: 73.104233\n",
      " 666817/700000: episode: 1987, duration: 1.607s, episode steps: 267, steps per second: 166, episode reward: 187.769, mean reward: 0.703 [-18.393, 100.000], mean action: 0.925 [0.000, 3.000], mean observation: 0.137 [-0.955, 1.000], loss: 11.018153, mean_absolute_error: 54.571182, mean_q: 73.382927\n",
      " 667184/700000: episode: 1988, duration: 2.102s, episode steps: 367, steps per second: 175, episode reward: 242.956, mean reward: 0.662 [-10.692, 100.000], mean action: 0.995 [0.000, 3.000], mean observation: 0.148 [-0.622, 1.000], loss: 8.382094, mean_absolute_error: 54.592026, mean_q: 73.358376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 667282/700000: episode: 1989, duration: 0.498s, episode steps: 98, steps per second: 197, episode reward: -386.587, mean reward: -3.945 [-100.000, 3.861], mean action: 1.847 [0.000, 3.000], mean observation: 0.265 [-1.250, 2.041], loss: 14.901793, mean_absolute_error: 54.011082, mean_q: 72.691727\n",
      " 667642/700000: episode: 1990, duration: 1.889s, episode steps: 360, steps per second: 191, episode reward: 127.680, mean reward: 0.355 [-19.198, 100.000], mean action: 1.969 [0.000, 3.000], mean observation: 0.107 [-0.953, 1.000], loss: 8.343477, mean_absolute_error: 54.625210, mean_q: 73.458961\n",
      " 667924/700000: episode: 1991, duration: 1.425s, episode steps: 282, steps per second: 198, episode reward: 228.470, mean reward: 0.810 [-17.553, 100.000], mean action: 0.908 [0.000, 3.000], mean observation: 0.132 [-0.890, 1.000], loss: 9.638046, mean_absolute_error: 54.674210, mean_q: 73.673439\n",
      " 668632/700000: episode: 1992, duration: 3.765s, episode steps: 708, steps per second: 188, episode reward: 232.752, mean reward: 0.329 [-19.942, 100.000], mean action: 0.662 [0.000, 3.000], mean observation: 0.190 [-1.167, 1.000], loss: 8.660953, mean_absolute_error: 54.624668, mean_q: 73.496468\n",
      " 669089/700000: episode: 1993, duration: 2.456s, episode steps: 457, steps per second: 186, episode reward: 229.707, mean reward: 0.503 [-18.086, 100.000], mean action: 1.098 [0.000, 3.000], mean observation: 0.172 [-0.899, 1.000], loss: 8.978333, mean_absolute_error: 54.512016, mean_q: 73.228424\n",
      " 669382/700000: episode: 1994, duration: 1.515s, episode steps: 293, steps per second: 193, episode reward: 221.174, mean reward: 0.755 [-19.613, 100.000], mean action: 1.263 [0.000, 3.000], mean observation: 0.149 [-1.075, 1.335], loss: 10.141797, mean_absolute_error: 54.493462, mean_q: 72.961037\n",
      " 670094/700000: episode: 1995, duration: 3.711s, episode steps: 712, steps per second: 192, episode reward: 177.232, mean reward: 0.249 [-19.897, 100.000], mean action: 0.980 [0.000, 3.000], mean observation: 0.157 [-0.716, 1.123], loss: 8.197084, mean_absolute_error: 54.283894, mean_q: 72.786842\n",
      " 670256/700000: episode: 1996, duration: 0.810s, episode steps: 162, steps per second: 200, episode reward: 211.195, mean reward: 1.304 [-9.257, 100.000], mean action: 1.247 [0.000, 3.000], mean observation: 0.078 [-1.078, 1.000], loss: 9.936500, mean_absolute_error: 53.545074, mean_q: 71.961708\n",
      " 670557/700000: episode: 1997, duration: 1.869s, episode steps: 301, steps per second: 161, episode reward: 246.237, mean reward: 0.818 [-18.526, 100.000], mean action: 1.432 [0.000, 3.000], mean observation: 0.107 [-1.012, 1.000], loss: 7.198883, mean_absolute_error: 53.648865, mean_q: 71.763481\n",
      " 670638/700000: episode: 1998, duration: 0.570s, episode steps: 81, steps per second: 142, episode reward: 3.735, mean reward: 0.046 [-100.000, 14.424], mean action: 1.852 [0.000, 3.000], mean observation: -0.035 [-1.132, 1.000], loss: 6.111008, mean_absolute_error: 53.366920, mean_q: 71.495300\n",
      " 671022/700000: episode: 1999, duration: 2.699s, episode steps: 384, steps per second: 142, episode reward: 255.939, mean reward: 0.667 [-19.581, 100.000], mean action: 0.773 [0.000, 3.000], mean observation: 0.144 [-0.736, 1.000], loss: 7.860079, mean_absolute_error: 53.757458, mean_q: 72.123779\n",
      " 671278/700000: episode: 2000, duration: 1.737s, episode steps: 256, steps per second: 147, episode reward: 222.203, mean reward: 0.868 [-9.329, 100.000], mean action: 1.551 [0.000, 3.000], mean observation: 0.098 [-1.058, 1.000], loss: 7.734313, mean_absolute_error: 53.902390, mean_q: 72.264191\n",
      " 671557/700000: episode: 2001, duration: 2.001s, episode steps: 279, steps per second: 139, episode reward: 236.853, mean reward: 0.849 [-7.834, 100.000], mean action: 1.305 [0.000, 3.000], mean observation: 0.034 [-0.690, 1.000], loss: 8.780555, mean_absolute_error: 53.932579, mean_q: 72.200447\n",
      " 671851/700000: episode: 2002, duration: 1.526s, episode steps: 294, steps per second: 193, episode reward: 227.696, mean reward: 0.774 [-18.896, 100.000], mean action: 1.156 [0.000, 3.000], mean observation: 0.147 [-0.998, 1.001], loss: 6.641806, mean_absolute_error: 54.119194, mean_q: 72.493134\n",
      " 672128/700000: episode: 2003, duration: 1.423s, episode steps: 277, steps per second: 195, episode reward: 257.655, mean reward: 0.930 [-18.028, 100.000], mean action: 1.202 [0.000, 3.000], mean observation: 0.082 [-0.688, 1.000], loss: 10.682214, mean_absolute_error: 53.382141, mean_q: 71.494125\n",
      " 672397/700000: episode: 2004, duration: 1.372s, episode steps: 269, steps per second: 196, episode reward: 209.769, mean reward: 0.780 [-9.270, 100.000], mean action: 1.145 [0.000, 3.000], mean observation: 0.131 [-1.088, 1.000], loss: 6.808019, mean_absolute_error: 53.429691, mean_q: 71.833199\n",
      " 672749/700000: episode: 2005, duration: 1.856s, episode steps: 352, steps per second: 190, episode reward: 203.900, mean reward: 0.579 [-17.499, 100.000], mean action: 1.199 [0.000, 3.000], mean observation: 0.100 [-0.853, 1.000], loss: 7.534665, mean_absolute_error: 53.590965, mean_q: 71.984322\n",
      " 673137/700000: episode: 2006, duration: 2.253s, episode steps: 388, steps per second: 172, episode reward: 129.962, mean reward: 0.335 [-22.514, 100.000], mean action: 1.799 [0.000, 3.000], mean observation: 0.089 [-0.690, 1.000], loss: 7.278332, mean_absolute_error: 53.881805, mean_q: 72.296280\n",
      " 673488/700000: episode: 2007, duration: 1.901s, episode steps: 351, steps per second: 185, episode reward: 222.968, mean reward: 0.635 [-19.175, 100.000], mean action: 0.849 [0.000, 3.000], mean observation: 0.156 [-1.042, 1.000], loss: 9.582540, mean_absolute_error: 53.578114, mean_q: 72.054428\n",
      " 673873/700000: episode: 2008, duration: 2.106s, episode steps: 385, steps per second: 183, episode reward: 220.366, mean reward: 0.572 [-21.897, 100.000], mean action: 1.153 [0.000, 3.000], mean observation: 0.126 [-1.014, 1.000], loss: 6.876571, mean_absolute_error: 53.715195, mean_q: 72.209618\n",
      " 674234/700000: episode: 2009, duration: 1.845s, episode steps: 361, steps per second: 196, episode reward: 227.107, mean reward: 0.629 [-10.943, 100.000], mean action: 0.789 [0.000, 3.000], mean observation: 0.157 [-0.922, 1.000], loss: 6.489843, mean_absolute_error: 53.650841, mean_q: 72.152908\n",
      " 674450/700000: episode: 2010, duration: 1.105s, episode steps: 216, steps per second: 195, episode reward: 220.457, mean reward: 1.021 [-12.012, 100.000], mean action: 1.153 [0.000, 3.000], mean observation: 0.082 [-0.898, 1.000], loss: 8.383121, mean_absolute_error: 53.110622, mean_q: 71.467400\n",
      " 674954/700000: episode: 2011, duration: 2.628s, episode steps: 504, steps per second: 192, episode reward: 209.527, mean reward: 0.416 [-19.276, 100.000], mean action: 0.498 [0.000, 3.000], mean observation: 0.180 [-0.903, 1.000], loss: 11.828547, mean_absolute_error: 53.079185, mean_q: 71.463463\n",
      " 675200/700000: episode: 2012, duration: 1.268s, episode steps: 246, steps per second: 194, episode reward: 254.515, mean reward: 1.035 [-9.345, 100.000], mean action: 1.154 [0.000, 3.000], mean observation: 0.072 [-0.778, 1.170], loss: 9.243168, mean_absolute_error: 53.125153, mean_q: 71.638130\n",
      " 675412/700000: episode: 2013, duration: 1.061s, episode steps: 212, steps per second: 200, episode reward: 258.051, mean reward: 1.217 [-9.683, 100.000], mean action: 1.297 [0.000, 3.000], mean observation: 0.104 [-0.668, 1.000], loss: 6.766269, mean_absolute_error: 53.126068, mean_q: 71.406357\n",
      " 675527/700000: episode: 2014, duration: 0.582s, episode steps: 115, steps per second: 198, episode reward: 12.579, mean reward: 0.109 [-100.000, 16.389], mean action: 1.817 [0.000, 3.000], mean observation: 0.079 [-0.806, 1.000], loss: 5.749316, mean_absolute_error: 52.812366, mean_q: 71.243851\n",
      " 675897/700000: episode: 2015, duration: 1.915s, episode steps: 370, steps per second: 193, episode reward: 186.219, mean reward: 0.503 [-17.949, 100.000], mean action: 0.776 [0.000, 3.000], mean observation: 0.122 [-0.936, 1.000], loss: 5.621606, mean_absolute_error: 53.519093, mean_q: 72.022415\n",
      " 676244/700000: episode: 2016, duration: 1.779s, episode steps: 347, steps per second: 195, episode reward: 203.937, mean reward: 0.588 [-18.161, 100.000], mean action: 0.620 [0.000, 3.000], mean observation: 0.176 [-1.054, 1.000], loss: 9.923576, mean_absolute_error: 53.423725, mean_q: 71.944550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 676582/700000: episode: 2017, duration: 1.736s, episode steps: 338, steps per second: 195, episode reward: 261.582, mean reward: 0.774 [-19.309, 100.000], mean action: 0.953 [0.000, 3.000], mean observation: 0.197 [-0.806, 1.403], loss: 9.192776, mean_absolute_error: 53.371613, mean_q: 71.859665\n",
      " 676803/700000: episode: 2018, duration: 1.118s, episode steps: 221, steps per second: 198, episode reward: 245.270, mean reward: 1.110 [-9.631, 100.000], mean action: 1.235 [0.000, 3.000], mean observation: 0.037 [-0.693, 1.000], loss: 4.828215, mean_absolute_error: 52.881248, mean_q: 71.157234\n",
      " 677287/700000: episode: 2019, duration: 2.578s, episode steps: 484, steps per second: 188, episode reward: 226.408, mean reward: 0.468 [-18.591, 100.000], mean action: 0.632 [0.000, 3.000], mean observation: 0.205 [-0.798, 1.402], loss: 8.735344, mean_absolute_error: 53.278934, mean_q: 71.571373\n",
      " 677672/700000: episode: 2020, duration: 2.579s, episode steps: 385, steps per second: 149, episode reward: 266.261, mean reward: 0.692 [-18.595, 100.000], mean action: 0.860 [0.000, 3.000], mean observation: 0.133 [-1.083, 1.000], loss: 8.267647, mean_absolute_error: 53.111294, mean_q: 71.311752\n",
      " 677993/700000: episode: 2021, duration: 2.150s, episode steps: 321, steps per second: 149, episode reward: 262.334, mean reward: 0.817 [-11.879, 100.000], mean action: 1.125 [0.000, 3.000], mean observation: 0.156 [-0.926, 1.000], loss: 7.510816, mean_absolute_error: 52.865108, mean_q: 70.790871\n",
      " 678213/700000: episode: 2022, duration: 1.445s, episode steps: 220, steps per second: 152, episode reward: 204.929, mean reward: 0.931 [-5.424, 100.000], mean action: 0.864 [0.000, 3.000], mean observation: 0.115 [-1.116, 1.000], loss: 8.018768, mean_absolute_error: 53.271030, mean_q: 71.502014\n",
      " 678563/700000: episode: 2023, duration: 1.924s, episode steps: 350, steps per second: 182, episode reward: 229.441, mean reward: 0.656 [-18.590, 100.000], mean action: 1.069 [0.000, 3.000], mean observation: 0.052 [-0.905, 1.000], loss: 9.980228, mean_absolute_error: 52.661026, mean_q: 70.686157\n",
      " 678689/700000: episode: 2024, duration: 0.640s, episode steps: 126, steps per second: 197, episode reward: -58.526, mean reward: -0.464 [-100.000, 14.295], mean action: 1.730 [0.000, 3.000], mean observation: 0.039 [-1.076, 1.249], loss: 7.328499, mean_absolute_error: 52.344509, mean_q: 70.362518\n",
      " 678897/700000: episode: 2025, duration: 1.051s, episode steps: 208, steps per second: 198, episode reward: 246.567, mean reward: 1.185 [-6.467, 100.000], mean action: 1.312 [0.000, 3.000], mean observation: 0.123 [-0.841, 1.464], loss: 11.178300, mean_absolute_error: 52.931454, mean_q: 71.176872\n",
      " 679107/700000: episode: 2026, duration: 1.082s, episode steps: 210, steps per second: 194, episode reward: 203.467, mean reward: 0.969 [-24.546, 100.000], mean action: 1.105 [0.000, 3.000], mean observation: 0.091 [-1.064, 1.000], loss: 12.104097, mean_absolute_error: 52.619877, mean_q: 70.529091\n",
      " 679331/700000: episode: 2027, duration: 1.158s, episode steps: 224, steps per second: 193, episode reward: 245.615, mean reward: 1.096 [-10.226, 100.000], mean action: 1.094 [0.000, 3.000], mean observation: 0.082 [-1.091, 1.000], loss: 7.807386, mean_absolute_error: 52.645958, mean_q: 70.705742\n",
      " 679513/700000: episode: 2028, duration: 0.922s, episode steps: 182, steps per second: 197, episode reward: 208.462, mean reward: 1.145 [-4.980, 100.000], mean action: 1.308 [0.000, 3.000], mean observation: 0.077 [-1.017, 1.000], loss: 8.507932, mean_absolute_error: 52.851593, mean_q: 71.004837\n",
      " 679694/700000: episode: 2029, duration: 0.936s, episode steps: 181, steps per second: 193, episode reward: 270.884, mean reward: 1.497 [-15.061, 100.000], mean action: 1.564 [0.000, 3.000], mean observation: 0.123 [-0.918, 1.000], loss: 8.139159, mean_absolute_error: 52.517212, mean_q: 70.432373\n",
      " 679938/700000: episode: 2030, duration: 1.274s, episode steps: 244, steps per second: 191, episode reward: 199.158, mean reward: 0.816 [-17.346, 100.000], mean action: 1.148 [0.000, 3.000], mean observation: 0.135 [-1.119, 1.187], loss: 11.058051, mean_absolute_error: 52.363525, mean_q: 70.374023\n",
      " 680352/700000: episode: 2031, duration: 2.162s, episode steps: 414, steps per second: 191, episode reward: 254.169, mean reward: 0.614 [-20.630, 100.000], mean action: 0.923 [0.000, 3.000], mean observation: 0.183 [-0.778, 1.247], loss: 7.243810, mean_absolute_error: 52.609138, mean_q: 70.738853\n",
      " 680447/700000: episode: 2032, duration: 0.507s, episode steps: 95, steps per second: 187, episode reward: -30.419, mean reward: -0.320 [-100.000, 8.509], mean action: 1.768 [0.000, 3.000], mean observation: -0.088 [-0.957, 1.604], loss: 8.897753, mean_absolute_error: 52.881763, mean_q: 70.954636\n",
      " 680530/700000: episode: 2033, duration: 0.432s, episode steps: 83, steps per second: 192, episode reward: 12.984, mean reward: 0.156 [-100.000, 17.535], mean action: 1.916 [0.000, 3.000], mean observation: 0.019 [-1.143, 1.000], loss: 5.965702, mean_absolute_error: 53.194023, mean_q: 71.581894\n",
      " 680874/700000: episode: 2034, duration: 1.805s, episode steps: 344, steps per second: 191, episode reward: 196.935, mean reward: 0.572 [-19.774, 100.000], mean action: 1.058 [0.000, 3.000], mean observation: 0.081 [-0.833, 1.000], loss: 7.861306, mean_absolute_error: 52.733971, mean_q: 70.870750\n",
      " 681085/700000: episode: 2035, duration: 1.070s, episode steps: 211, steps per second: 197, episode reward: 214.077, mean reward: 1.015 [-10.392, 100.000], mean action: 1.242 [0.000, 3.000], mean observation: 0.055 [-0.977, 1.000], loss: 8.541315, mean_absolute_error: 52.971043, mean_q: 71.187439\n",
      " 681713/700000: episode: 2036, duration: 3.479s, episode steps: 628, steps per second: 181, episode reward: 76.090, mean reward: 0.121 [-17.262, 100.000], mean action: 2.021 [0.000, 3.000], mean observation: 0.028 [-1.020, 1.109], loss: 10.386537, mean_absolute_error: 52.496540, mean_q: 70.477234\n",
      " 682210/700000: episode: 2037, duration: 2.610s, episode steps: 497, steps per second: 190, episode reward: 247.098, mean reward: 0.497 [-17.887, 100.000], mean action: 0.742 [0.000, 3.000], mean observation: 0.193 [-0.810, 1.000], loss: 7.327098, mean_absolute_error: 52.982315, mean_q: 71.107529\n",
      " 682579/700000: episode: 2038, duration: 1.909s, episode steps: 369, steps per second: 193, episode reward: 213.859, mean reward: 0.580 [-21.236, 100.000], mean action: 0.875 [0.000, 3.000], mean observation: 0.142 [-0.978, 1.000], loss: 6.384080, mean_absolute_error: 52.911224, mean_q: 71.171577\n",
      " 682745/700000: episode: 2039, duration: 0.832s, episode steps: 166, steps per second: 200, episode reward: -113.228, mean reward: -0.682 [-100.000, 11.709], mean action: 1.536 [0.000, 3.000], mean observation: 0.147 [-0.849, 1.925], loss: 7.909855, mean_absolute_error: 52.887444, mean_q: 71.198891\n",
      " 683060/700000: episode: 2040, duration: 1.593s, episode steps: 315, steps per second: 198, episode reward: 241.265, mean reward: 0.766 [-13.022, 100.000], mean action: 0.952 [0.000, 3.000], mean observation: 0.147 [-0.811, 1.000], loss: 8.278212, mean_absolute_error: 52.775360, mean_q: 71.049339\n",
      " 683231/700000: episode: 2041, duration: 0.862s, episode steps: 171, steps per second: 198, episode reward: 202.913, mean reward: 1.187 [-4.129, 100.000], mean action: 1.626 [0.000, 3.000], mean observation: 0.085 [-1.020, 1.000], loss: 9.671252, mean_absolute_error: 52.525436, mean_q: 70.604874\n",
      " 684121/700000: episode: 2042, duration: 4.861s, episode steps: 890, steps per second: 183, episode reward: 183.008, mean reward: 0.206 [-19.908, 100.000], mean action: 1.704 [0.000, 3.000], mean observation: 0.187 [-1.015, 1.000], loss: 8.101394, mean_absolute_error: 52.870899, mean_q: 71.068062\n",
      " 684344/700000: episode: 2043, duration: 1.118s, episode steps: 223, steps per second: 199, episode reward: 235.882, mean reward: 1.058 [-2.607, 100.000], mean action: 1.152 [0.000, 3.000], mean observation: 0.076 [-1.020, 1.000], loss: 5.563128, mean_absolute_error: 52.905479, mean_q: 71.316727\n",
      " 684560/700000: episode: 2044, duration: 1.094s, episode steps: 216, steps per second: 197, episode reward: 231.727, mean reward: 1.073 [-10.044, 100.000], mean action: 1.241 [0.000, 3.000], mean observation: 0.120 [-0.851, 1.000], loss: 7.061178, mean_absolute_error: 52.875858, mean_q: 71.170891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 684918/700000: episode: 2045, duration: 1.838s, episode steps: 358, steps per second: 195, episode reward: 239.112, mean reward: 0.668 [-19.902, 100.000], mean action: 0.804 [0.000, 3.000], mean observation: 0.112 [-1.141, 1.016], loss: 9.238003, mean_absolute_error: 53.025730, mean_q: 71.176796\n",
      " 685097/700000: episode: 2046, duration: 0.897s, episode steps: 179, steps per second: 200, episode reward: 196.349, mean reward: 1.097 [-2.799, 100.000], mean action: 0.939 [0.000, 3.000], mean observation: 0.091 [-1.097, 1.000], loss: 4.562304, mean_absolute_error: 52.828571, mean_q: 71.061874\n",
      " 685350/700000: episode: 2047, duration: 1.294s, episode steps: 253, steps per second: 196, episode reward: 237.142, mean reward: 0.937 [-8.667, 100.000], mean action: 1.237 [0.000, 3.000], mean observation: -0.017 [-1.138, 1.000], loss: 13.281186, mean_absolute_error: 52.938900, mean_q: 71.149811\n",
      " 685832/700000: episode: 2048, duration: 2.514s, episode steps: 482, steps per second: 192, episode reward: 223.150, mean reward: 0.463 [-19.030, 100.000], mean action: 0.815 [0.000, 3.000], mean observation: 0.143 [-0.898, 1.000], loss: 7.225396, mean_absolute_error: 53.052647, mean_q: 71.319504\n",
      " 686329/700000: episode: 2049, duration: 2.595s, episode steps: 497, steps per second: 191, episode reward: 205.740, mean reward: 0.414 [-20.752, 100.000], mean action: 0.537 [0.000, 3.000], mean observation: 0.155 [-0.871, 1.077], loss: 8.980758, mean_absolute_error: 52.918362, mean_q: 71.260887\n",
      " 686622/700000: episode: 2050, duration: 1.512s, episode steps: 293, steps per second: 194, episode reward: 250.324, mean reward: 0.854 [-11.005, 100.000], mean action: 1.253 [0.000, 3.000], mean observation: 0.145 [-0.805, 1.156], loss: 5.261632, mean_absolute_error: 53.265224, mean_q: 71.617203\n",
      " 686840/700000: episode: 2051, duration: 1.107s, episode steps: 218, steps per second: 197, episode reward: 229.567, mean reward: 1.053 [-10.614, 100.000], mean action: 1.532 [0.000, 3.000], mean observation: 0.088 [-0.584, 1.000], loss: 7.391319, mean_absolute_error: 53.271851, mean_q: 71.663467\n",
      " 686941/700000: episode: 2052, duration: 0.506s, episode steps: 101, steps per second: 200, episode reward: -26.685, mean reward: -0.264 [-100.000, 20.184], mean action: 1.614 [0.000, 3.000], mean observation: 0.018 [-1.175, 1.000], loss: 10.245910, mean_absolute_error: 54.687683, mean_q: 73.396057\n",
      " 687113/700000: episode: 2053, duration: 0.876s, episode steps: 172, steps per second: 196, episode reward: -537.254, mean reward: -3.124 [-100.000, 3.783], mean action: 1.831 [0.000, 3.000], mean observation: 0.238 [-2.213, 6.892], loss: 7.775320, mean_absolute_error: 53.412479, mean_q: 71.838867\n",
      " 687247/700000: episode: 2054, duration: 0.677s, episode steps: 134, steps per second: 198, episode reward: -23.039, mean reward: -0.172 [-100.000, 10.633], mean action: 1.672 [0.000, 3.000], mean observation: 0.110 [-0.896, 2.121], loss: 5.102597, mean_absolute_error: 53.860744, mean_q: 72.310753\n",
      " 687490/700000: episode: 2055, duration: 1.224s, episode steps: 243, steps per second: 199, episode reward: 213.712, mean reward: 0.879 [-17.506, 100.000], mean action: 1.082 [0.000, 3.000], mean observation: 0.096 [-0.826, 1.000], loss: 7.738187, mean_absolute_error: 53.252476, mean_q: 71.556290\n",
      " 687716/700000: episode: 2056, duration: 1.144s, episode steps: 226, steps per second: 198, episode reward: 210.911, mean reward: 0.933 [-3.131, 100.000], mean action: 1.323 [0.000, 3.000], mean observation: 0.089 [-0.890, 1.000], loss: 8.576427, mean_absolute_error: 53.299545, mean_q: 71.639282\n",
      " 688086/700000: episode: 2057, duration: 2.115s, episode steps: 370, steps per second: 175, episode reward: 220.727, mean reward: 0.597 [-10.475, 100.000], mean action: 0.930 [0.000, 3.000], mean observation: 0.193 [-1.146, 1.000], loss: 10.267221, mean_absolute_error: 53.353062, mean_q: 71.748764\n",
      " 688528/700000: episode: 2058, duration: 2.470s, episode steps: 442, steps per second: 179, episode reward: 217.828, mean reward: 0.493 [-17.374, 100.000], mean action: 0.674 [0.000, 3.000], mean observation: 0.140 [-0.737, 1.000], loss: 7.455612, mean_absolute_error: 53.815426, mean_q: 72.293488\n",
      " 688613/700000: episode: 2059, duration: 0.452s, episode steps: 85, steps per second: 188, episode reward: -107.945, mean reward: -1.270 [-100.000, 10.681], mean action: 1.553 [0.000, 3.000], mean observation: 0.070 [-1.034, 3.199], loss: 3.408626, mean_absolute_error: 53.568260, mean_q: 71.837761\n",
      " 688781/700000: episode: 2060, duration: 0.900s, episode steps: 168, steps per second: 187, episode reward: -121.722, mean reward: -0.725 [-100.000, 12.652], mean action: 1.839 [0.000, 3.000], mean observation: 0.186 [-1.846, 1.000], loss: 7.227768, mean_absolute_error: 53.761627, mean_q: 72.164307\n",
      " 688872/700000: episode: 2061, duration: 0.637s, episode steps: 91, steps per second: 143, episode reward: -319.770, mean reward: -3.514 [-100.000, 1.312], mean action: 2.011 [0.000, 3.000], mean observation: 0.217 [-1.036, 1.491], loss: 4.510535, mean_absolute_error: 52.958466, mean_q: 71.124367\n",
      " 689174/700000: episode: 2062, duration: 1.650s, episode steps: 302, steps per second: 183, episode reward: 234.283, mean reward: 0.776 [-19.904, 100.000], mean action: 1.099 [0.000, 3.000], mean observation: 0.106 [-0.894, 1.000], loss: 7.827633, mean_absolute_error: 53.756119, mean_q: 72.183701\n",
      " 689545/700000: episode: 2063, duration: 2.296s, episode steps: 371, steps per second: 162, episode reward: 220.749, mean reward: 0.595 [-5.090, 100.000], mean action: 1.367 [0.000, 3.000], mean observation: 0.169 [-0.701, 1.000], loss: 7.735929, mean_absolute_error: 53.872589, mean_q: 72.482651\n",
      " 689796/700000: episode: 2064, duration: 1.900s, episode steps: 251, steps per second: 132, episode reward: 233.969, mean reward: 0.932 [-2.086, 100.000], mean action: 0.849 [0.000, 3.000], mean observation: 0.120 [-0.959, 1.000], loss: 11.421377, mean_absolute_error: 53.871593, mean_q: 72.263062\n",
      " 690016/700000: episode: 2065, duration: 1.501s, episode steps: 220, steps per second: 147, episode reward: 218.115, mean reward: 0.991 [-2.724, 100.000], mean action: 1.227 [0.000, 3.000], mean observation: 0.084 [-1.010, 1.000], loss: 9.163457, mean_absolute_error: 53.650337, mean_q: 71.797028\n",
      " 690237/700000: episode: 2066, duration: 1.351s, episode steps: 221, steps per second: 164, episode reward: 198.978, mean reward: 0.900 [-18.053, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.122 [-0.997, 1.000], loss: 7.527971, mean_absolute_error: 53.716969, mean_q: 72.257935\n",
      " 690575/700000: episode: 2067, duration: 1.889s, episode steps: 338, steps per second: 179, episode reward: 229.460, mean reward: 0.679 [-20.756, 100.000], mean action: 1.411 [0.000, 3.000], mean observation: 0.132 [-1.036, 1.008], loss: 10.279404, mean_absolute_error: 53.350594, mean_q: 71.636276\n",
      " 690870/700000: episode: 2068, duration: 1.521s, episode steps: 295, steps per second: 194, episode reward: 227.505, mean reward: 0.771 [-19.038, 100.000], mean action: 1.342 [0.000, 3.000], mean observation: 0.188 [-1.231, 1.183], loss: 6.406470, mean_absolute_error: 53.954338, mean_q: 72.583839\n",
      " 691226/700000: episode: 2069, duration: 1.851s, episode steps: 356, steps per second: 192, episode reward: 237.766, mean reward: 0.668 [-18.144, 100.000], mean action: 1.444 [0.000, 3.000], mean observation: 0.104 [-0.990, 1.006], loss: 6.674384, mean_absolute_error: 53.356907, mean_q: 71.752533\n",
      " 691596/700000: episode: 2070, duration: 1.972s, episode steps: 370, steps per second: 188, episode reward: 219.170, mean reward: 0.592 [-18.681, 100.000], mean action: 1.173 [0.000, 3.000], mean observation: 0.096 [-1.152, 1.247], loss: 39.142921, mean_absolute_error: 53.302883, mean_q: 71.622330\n",
      " 691824/700000: episode: 2071, duration: 1.205s, episode steps: 228, steps per second: 189, episode reward: 207.283, mean reward: 0.909 [-10.810, 100.000], mean action: 1.171 [0.000, 3.000], mean observation: 0.090 [-0.968, 1.000], loss: 4.832883, mean_absolute_error: 53.800293, mean_q: 72.154297\n",
      " 691927/700000: episode: 2072, duration: 0.518s, episode steps: 103, steps per second: 199, episode reward: -59.359, mean reward: -0.576 [-100.000, 13.380], mean action: 1.427 [0.000, 3.000], mean observation: -0.002 [-1.403, 1.000], loss: 9.773447, mean_absolute_error: 52.826363, mean_q: 71.112183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 692445/700000: episode: 2073, duration: 2.661s, episode steps: 518, steps per second: 195, episode reward: 219.884, mean reward: 0.424 [-17.945, 100.000], mean action: 0.620 [0.000, 3.000], mean observation: 0.184 [-0.967, 1.019], loss: 6.426335, mean_absolute_error: 53.451675, mean_q: 71.844070\n",
      " 692670/700000: episode: 2074, duration: 1.137s, episode steps: 225, steps per second: 198, episode reward: 217.784, mean reward: 0.968 [-17.530, 100.000], mean action: 1.040 [0.000, 3.000], mean observation: 0.128 [-0.964, 1.000], loss: 6.040617, mean_absolute_error: 53.745808, mean_q: 72.366768\n",
      " 693022/700000: episode: 2075, duration: 1.837s, episode steps: 352, steps per second: 192, episode reward: 254.057, mean reward: 0.722 [-18.543, 100.000], mean action: 0.966 [0.000, 3.000], mean observation: 0.167 [-1.018, 1.000], loss: 7.500653, mean_absolute_error: 54.037308, mean_q: 72.461327\n",
      " 694022/700000: episode: 2076, duration: 5.582s, episode steps: 1000, steps per second: 179, episode reward: 36.600, mean reward: 0.037 [-19.867, 21.651], mean action: 2.489 [0.000, 3.000], mean observation: 0.190 [-1.183, 1.000], loss: 7.783719, mean_absolute_error: 53.849110, mean_q: 72.226486\n",
      " 694291/700000: episode: 2077, duration: 1.373s, episode steps: 269, steps per second: 196, episode reward: 212.282, mean reward: 0.789 [-6.871, 100.000], mean action: 0.881 [0.000, 3.000], mean observation: 0.135 [-0.984, 1.000], loss: 12.245348, mean_absolute_error: 53.447918, mean_q: 71.634872\n",
      " 694569/700000: episode: 2078, duration: 1.421s, episode steps: 278, steps per second: 196, episode reward: 239.628, mean reward: 0.862 [-5.156, 100.000], mean action: 1.263 [0.000, 3.000], mean observation: 0.140 [-1.008, 1.364], loss: 6.596478, mean_absolute_error: 53.727116, mean_q: 72.244148\n",
      " 694798/700000: episode: 2079, duration: 1.175s, episode steps: 229, steps per second: 195, episode reward: 199.802, mean reward: 0.872 [-10.162, 100.000], mean action: 1.384 [0.000, 3.000], mean observation: 0.044 [-0.831, 1.000], loss: 7.714816, mean_absolute_error: 53.712940, mean_q: 72.167145\n",
      " 695169/700000: episode: 2080, duration: 1.993s, episode steps: 371, steps per second: 186, episode reward: 191.220, mean reward: 0.515 [-18.951, 100.000], mean action: 1.776 [0.000, 3.000], mean observation: 0.083 [-0.692, 1.000], loss: 7.167873, mean_absolute_error: 53.600536, mean_q: 71.870934\n",
      " 695332/700000: episode: 2081, duration: 0.825s, episode steps: 163, steps per second: 198, episode reward: 4.627, mean reward: 0.028 [-100.000, 14.861], mean action: 1.982 [0.000, 3.000], mean observation: -0.026 [-0.792, 1.577], loss: 8.458965, mean_absolute_error: 53.987705, mean_q: 72.567963\n",
      " 695560/700000: episode: 2082, duration: 1.171s, episode steps: 228, steps per second: 195, episode reward: 235.433, mean reward: 1.033 [-6.823, 100.000], mean action: 1.338 [0.000, 3.000], mean observation: 0.041 [-0.789, 1.000], loss: 8.648477, mean_absolute_error: 54.328304, mean_q: 72.905922\n",
      " 695778/700000: episode: 2083, duration: 1.099s, episode steps: 218, steps per second: 198, episode reward: 228.688, mean reward: 1.049 [-9.551, 100.000], mean action: 1.390 [0.000, 3.000], mean observation: 0.030 [-0.729, 1.000], loss: 10.917414, mean_absolute_error: 53.991463, mean_q: 72.272369\n",
      " 696196/700000: episode: 2084, duration: 2.156s, episode steps: 418, steps per second: 194, episode reward: 223.034, mean reward: 0.534 [-20.604, 100.000], mean action: 0.780 [0.000, 3.000], mean observation: 0.169 [-0.752, 1.000], loss: 10.193825, mean_absolute_error: 53.847908, mean_q: 72.141586\n",
      " 696855/700000: episode: 2085, duration: 3.777s, episode steps: 659, steps per second: 174, episode reward: 135.822, mean reward: 0.206 [-19.778, 100.000], mean action: 1.601 [0.000, 3.000], mean observation: 0.041 [-0.919, 1.000], loss: 8.061541, mean_absolute_error: 53.834610, mean_q: 72.353340\n",
      " 697261/700000: episode: 2086, duration: 2.104s, episode steps: 406, steps per second: 193, episode reward: 233.843, mean reward: 0.576 [-22.231, 100.000], mean action: 1.042 [0.000, 3.000], mean observation: 0.122 [-0.631, 1.000], loss: 6.485437, mean_absolute_error: 53.886959, mean_q: 72.535568\n",
      " 697867/700000: episode: 2087, duration: 3.257s, episode steps: 606, steps per second: 186, episode reward: 203.621, mean reward: 0.336 [-17.745, 100.000], mean action: 1.076 [0.000, 3.000], mean observation: 0.083 [-0.823, 1.000], loss: 10.746065, mean_absolute_error: 53.771626, mean_q: 72.459595\n",
      " 698102/700000: episode: 2088, duration: 1.205s, episode steps: 235, steps per second: 195, episode reward: 198.349, mean reward: 0.844 [-20.478, 100.000], mean action: 1.391 [0.000, 3.000], mean observation: 0.102 [-1.060, 1.000], loss: 10.239404, mean_absolute_error: 53.905643, mean_q: 72.410492\n",
      " 698561/700000: episode: 2089, duration: 2.379s, episode steps: 459, steps per second: 193, episode reward: 226.333, mean reward: 0.493 [-9.686, 100.000], mean action: 0.865 [0.000, 3.000], mean observation: 0.101 [-0.735, 1.000], loss: 8.626026, mean_absolute_error: 53.716629, mean_q: 72.235092\n",
      " 698860/700000: episode: 2090, duration: 1.554s, episode steps: 299, steps per second: 192, episode reward: 259.243, mean reward: 0.867 [-17.496, 100.000], mean action: 1.418 [0.000, 3.000], mean observation: 0.137 [-0.833, 1.000], loss: 5.569047, mean_absolute_error: 54.418343, mean_q: 73.118866\n",
      " 699186/700000: episode: 2091, duration: 1.694s, episode steps: 326, steps per second: 192, episode reward: 195.960, mean reward: 0.601 [-9.881, 100.000], mean action: 1.482 [0.000, 3.000], mean observation: 0.090 [-1.036, 1.000], loss: 11.948446, mean_absolute_error: 53.917831, mean_q: 72.592461\n",
      " 699330/700000: episode: 2092, duration: 0.719s, episode steps: 144, steps per second: 200, episode reward: -7.534, mean reward: -0.052 [-100.000, 15.916], mean action: 1.667 [0.000, 3.000], mean observation: 0.003 [-0.758, 1.000], loss: 6.490203, mean_absolute_error: 54.060390, mean_q: 72.706856\n",
      " 699692/700000: episode: 2093, duration: 1.902s, episode steps: 362, steps per second: 190, episode reward: 262.812, mean reward: 0.726 [-10.155, 100.000], mean action: 1.558 [0.000, 3.000], mean observation: 0.026 [-0.722, 1.000], loss: 7.279962, mean_absolute_error: 53.993362, mean_q: 72.552025\n",
      "done, took 3793.081 seconds\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = 'LunarLander-v2'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=500000, window_length=1)\n",
    "policy = EpsGreedyQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "dqn.fit(env, nb_steps=700000, visualize=False, verbose=2)\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our model and saved the weights to file, we can perform our evaluation experiment. We believe a three part evaluation experiment would suitably evaluate how effective the trained model is:\n",
    "1. Visual evalution: Whilst more of a rough evaluation, a visual inspection of the model can still be useful to get an idea of its performance. We will visualize the model playing Lunar Lander 10 times and make observations on its performance.\n",
    "2. Reward evaluation: Reward is a good measure of the models performance. As specified on OpenAIs github page (https://github.com/openai/gym/wiki/Leaderboard#lunarlander-v2), reward for Lunar Lander is gained by moving towards the landing area and landing successfully, and reward is lost by moving away from the landing area and crashing. We will run our model over 200 episodes and note its mean reward. This should be a good indication of whether our model is performing well or not.\n",
    "3. Solved Episodes evaluation: OpenAI define the game as \"solved\" when the reward is 200. We will count how many solved episodes occur during the 200 episodes.\n",
    "\n",
    "\n",
    "Firstly, we load the weights saved previously (this is not needed if the previous cell has been run, however it means that we only need to run the previous cell once, and then can run the following cell without needing to recalculate the weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_6 (Flatten)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 4)                 516       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 34,692\n",
      "Trainable params: 34,692\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=500000, window_length=1)\n",
    "policy = EpsGreedyQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "dqn.load_weights('dqn_{}_weights.h5f'.format(ENV_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation 1 - Visual\n",
    "\n",
    "We simply run the model on 10 episodes and observe the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 226.454, steps: 204\n",
      "Episode 2: reward: 200.664, steps: 200\n",
      "Episode 3: reward: 229.733, steps: 307\n",
      "Episode 4: reward: 233.379, steps: 253\n",
      "Episode 5: reward: 179.293, steps: 427\n",
      "Episode 6: reward: 215.350, steps: 229\n",
      "Episode 7: reward: 243.887, steps: 222\n",
      "Episode 8: reward: 203.602, steps: 212\n",
      "Episode 9: reward: 228.328, steps: 191\n",
      "Episode 10: reward: 192.017, steps: 428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12bd52eb8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "* The model performed very well, as reflected in its excellent scores\n",
    "* The spacecraft never once crashed\n",
    "* Having played the game ourselves, we believe that the model performed better than we ever could, and had inhuman precision.\n",
    "\n",
    "### Evaluation 2 - Reward\n",
    "\n",
    "We run the model on 200 episodes and store the resulting rewards in a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>226.453966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200.664476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>229.732994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>233.379147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>179.292717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>215.349685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>243.886568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>203.602363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>228.328149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>192.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>244.549862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>174.999480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>83.230858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>222.177915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>230.376022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>177.867831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>228.563729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>225.585875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>232.495577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>229.406272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>251.866086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>190.082371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>217.604335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>207.333374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>236.642740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>171.814119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>236.137870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>182.622709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>240.493246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>198.140430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>198.180270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>257.564707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>225.161957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>233.959519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>251.219206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>222.022224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>214.726297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>219.712209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>221.815831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>220.244048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>190.703741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>192.974481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>238.022402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>236.936463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>218.046282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>228.962656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>204.050009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>236.442805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>226.342537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>233.003560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>228.744338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>70.015474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>202.374238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>184.882938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>222.739962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>225.182018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>73.409226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>228.692326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>197.712789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>192.384247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Reward\n",
       "0    226.453966\n",
       "1    200.664476\n",
       "2    229.732994\n",
       "3    233.379147\n",
       "4    179.292717\n",
       "5    215.349685\n",
       "6    243.886568\n",
       "7    203.602363\n",
       "8    228.328149\n",
       "9    192.017400\n",
       "10   244.549862\n",
       "11   174.999480\n",
       "12    83.230858\n",
       "13   222.177915\n",
       "14   230.376022\n",
       "15   177.867831\n",
       "16   228.563729\n",
       "17   225.585875\n",
       "18   232.495577\n",
       "19   229.406272\n",
       "20   251.866086\n",
       "21   190.082371\n",
       "22   217.604335\n",
       "23   207.333374\n",
       "24   236.642740\n",
       "25   171.814119\n",
       "26   236.137870\n",
       "27   182.622709\n",
       "28   240.493246\n",
       "29   198.140430\n",
       "..          ...\n",
       "170  198.180270\n",
       "171  257.564707\n",
       "172  225.161957\n",
       "173  233.959519\n",
       "174  251.219206\n",
       "175  222.022224\n",
       "176  214.726297\n",
       "177  219.712209\n",
       "178  221.815831\n",
       "179  220.244048\n",
       "180  190.703741\n",
       "181  192.974481\n",
       "182  238.022402\n",
       "183  236.936463\n",
       "184  218.046282\n",
       "185  228.962656\n",
       "186  204.050009\n",
       "187  236.442805\n",
       "188  226.342537\n",
       "189  233.003560\n",
       "190  228.744338\n",
       "191   70.015474\n",
       "192  202.374238\n",
       "193  184.882938\n",
       "194  222.739962\n",
       "195  225.182018\n",
       "196   73.409226\n",
       "197  228.692326\n",
       "198  197.712789\n",
       "199  192.384247\n",
       "\n",
       "[200 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = dqn.test(env, nb_episodes=200, visualize=False, verbose=0)\n",
    "results_df = pd.DataFrame({'Reward': history.history['episode_reward']})\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214.51801258860186"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['Reward'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the dataframe containing the episode rewards, as well as the mean of all rewards, the model performs very well on almost all episodes. There are, however, a few episodes in which it performs worse than expected (i.e. below 100)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation 3 - Solved Episodes\n",
    "\n",
    "* All episodes with a reward of over 200 count as solved episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reward    172\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.loc[results_df['Reward'] > 200].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reward    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.loc[results_df['Reward'] < 0].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that 172 out of the 200 episodes were \"solved\" (86% of all episodes), while not a single episode got a reward of below 0. OpenAI defines the game of LunarLander \"solved\" when an average reward of 200 or over is gotten over 100 episodes. We can therefore say that we \"solved\" LunarLander."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "1. From visually observing the trained model playing Lunar Lander, we believe that it appears to be better than a good human at playing the game. With more training, we believe it would appear to be better than any human expert at the game.\n",
    "2. The mean reward over the 200 episodes was very high (over 200)\n",
    "3. Our model \"solved\" the game 86% of the time. We believe that more training episodes would allow it to solve the game even more often."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
